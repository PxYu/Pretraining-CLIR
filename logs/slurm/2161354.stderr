INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 680539.98it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 609566.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 575413.49it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
 12%|█▏        | 21/176 [00:00<00:00, 169.68it/s]100%|██████████| 176/176 [00:00<00:00, 1325.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
100%|██████████| 176/176 [00:00<00:00, 16258.07it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 151.45it/s]100%|██████████| 176/176 [00:00<00:00, 1190.37it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16004.28it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
 12%|█▏        | 21/176 [00:00<00:01, 112.19it/s]100%|██████████| 176/176 [00:00<00:00, 876.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 10751.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 651329.90it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 668265.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 647548.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 679988.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 667946.62it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 661666.51it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 713997.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 171.84it/s]100%|██████████| 176/176 [00:00<00:00, 1342.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16205.60it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:00, 163.77it/s]100%|██████████| 176/176 [00:00<00:00, 1281.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16164.08it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:01, 150.83it/s]100%|██████████| 176/176 [00:00<00:00, 1185.87it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16323.86it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 155.46it/s]100%|██████████| 176/176 [00:00<00:00, 1223.03it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 146.83it/s]100%|██████████| 176/176 [00:00<00:00, 16350.25it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 1157.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16327.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
 12%|█▏        | 21/176 [00:00<00:00, 158.96it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1244.99it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15827.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:01, 154.85it/s]100%|██████████| 176/176 [00:00<00:00, 1216.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16048.82it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3818.9 words/s - loss: 0.3911
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3630.9 words/s - loss: 0.3639
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3470.8 words/s - loss: 0.3667
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3526.2 words/s - loss: 0.3483
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3439.0 words/s - loss: 0.3536
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3367.0 words/s - loss: 0.3481
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3294.8 words/s - loss: 0.3653
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3354.3 words/s - loss: 0.3511
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3272.7 words/s - loss: 0.3814
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3231.9 words/s - loss: 0.3649
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3709.4 words/s - loss: 0.1983
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3630.7 words/s - loss: 0.2171
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3643.5 words/s - loss: 0.1981
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3770.7 words/s - loss: 0.2294
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3144.3 words/s - loss: 0.2135
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3525.0 words/s - loss: 0.2324
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3306.2 words/s - loss: 0.2055
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3394.5 words/s - loss: 0.2546
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3187.7 words/s - loss: 0.2115
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3175.1 words/s - loss: 0.2292
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3986.5 words/s - loss: 0.1636
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3650.6 words/s - loss: 0.1840
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3557.1 words/s - loss: 0.2234
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3903.8 words/s - loss: 0.1972
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3431.0 words/s - loss: 0.2361
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3400.0 words/s - loss: 0.2102
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3314.8 words/s - loss: 0.2007
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3599.4 words/s - loss: 0.1754
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3551.1 words/s - loss: 0.1900
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3290.2 words/s - loss: 0.1686
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3393.0 words/s - loss: 0.1770
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3635.4 words/s - loss: 0.1883
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3646.7 words/s - loss: 0.1736
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3753.7 words/s - loss: 0.2003
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3296.7 words/s - loss: 0.2098
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3404.8 words/s - loss: 0.1698
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3575.2 words/s - loss: 0.1859
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3537.5 words/s - loss: 0.1637
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3205.3 words/s - loss: 0.1784
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3363.9 words/s - loss: 0.1750
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3506.1 words/s - loss: 0.1710
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3494.7 words/s - loss: 0.1695
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3648.7 words/s - loss: 0.1741
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3571.8 words/s - loss: 0.1667
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3531.9 words/s - loss: 0.1889
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3366.6 words/s - loss: 0.1729
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3787.9 words/s - loss: 0.1827
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3507.1 words/s - loss: 0.1748
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3120.2 words/s - loss: 0.1859
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2935.8 words/s - loss: 0.1464
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3508.5 words/s - loss: 0.1617
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3736.7 words/s - loss: 0.1630
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3619.1 words/s - loss: 0.1614
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3486.7 words/s - loss: 0.1685
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3820.2 words/s - loss: 0.1959
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3455.8 words/s - loss: 0.1648
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3415.8 words/s - loss: 0.1789
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3412.2 words/s - loss: 0.1581
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3516.7 words/s - loss: 0.1541
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3143.8 words/s - loss: 0.1515
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.310 w/ 88 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_0_5.txt
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3805.5 words/s - loss: 0.1463
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3754.5 words/s - loss: 0.1458
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3540.6 words/s - loss: 0.1440
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3464.2 words/s - loss: 0.1441
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3354.3 words/s - loss: 0.1567
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3403.6 words/s - loss: 0.1222
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3290.2 words/s - loss: 0.1754
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3215.5 words/s - loss: 0.1519
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3294.3 words/s - loss: 0.1842
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3194.6 words/s - loss: 0.1565
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fren_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3845.8 words/s - loss: 0.1454
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3617.5 words/s - loss: 0.1369
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3524.2 words/s - loss: 0.1520
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3418.4 words/s - loss: 0.1573
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3434.0 words/s - loss: 0.1477
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3427.0 words/s - loss: 0.1664
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3385.5 words/s - loss: 0.1533
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3382.1 words/s - loss: 0.1410
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3309.4 words/s - loss: 0.1485
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3123.7 words/s - loss: 0.1430
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.305 w/ 88 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fren_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3779.1 words/s - loss: 0.1422
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3732.2 words/s - loss: 0.1320
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3665.6 words/s - loss: 0.1471
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3600.5 words/s - loss: 0.1395
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3618.0 words/s - loss: 0.1636
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3544.0 words/s - loss: 0.1504
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3545.4 words/s - loss: 0.1341
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3309.8 words/s - loss: 0.1391
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3280.6 words/s - loss: 0.1296
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3249.5 words/s - loss: 0.1315
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fren_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3795.3 words/s - loss: 0.1450
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3767.2 words/s - loss: 0.1314
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3667.1 words/s - loss: 0.1607
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3607.9 words/s - loss: 0.1366
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3650.7 words/s - loss: 0.1645
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3407.8 words/s - loss: 0.1301
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3545.5 words/s - loss: 0.1299
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3538.7 words/s - loss: 0.1442
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3462.0 words/s - loss: 0.1465
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3175.1 words/s - loss: 0.1256
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.314 w/ 88 queries
INFO:__main__:removing file tmp/mbert_fren_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_9.txt
INFO:__main__:[0.34239189030742956, 0.34225210502110603, 0.34551160548330895, 0.35303366798482544, 0.3438444055242972]
INFO:__main__:[0.3101960298205842, 0.30935590168923777, 0.3050132411807745, 0.2991428923978716, 0.31353470959031027]
INFO:__main__:0.3438444055242972
INFO:__main__:0.2991428923978716
INFO:__main__:best MAP: 0.321
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 806379.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 720596.50it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 179.28it/s]100%|██████████| 176/176 [00:00<00:00, 1280.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15842.85it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 133.63it/s]100%|██████████| 176/176 [00:00<00:00, 970.09it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15379.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 667627.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 648209.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 660645.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 700474.97it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 498431.85it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 648530.17it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 655749.35it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 554259.59it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 177.75it/s]100%|██████████| 176/176 [00:00<00:00, 1272.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 9987.52it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 128.98it/s]100%|██████████| 176/176 [00:00<00:00, 938.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15670.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 150.31it/s] 13%|█▎        | 23/176 [00:00<00:00, 174.58it/s] 13%|█▎        | 23/176 [00:00<00:00, 174.09it/s]100%|██████████| 176/176 [00:00<00:00, 1084.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1252.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1245.80it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15432.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16203.82it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 15777.50it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 144.92it/s] 13%|█▎        | 23/176 [00:00<00:00, 176.86it/s]100%|██████████| 176/176 [00:00<00:00, 1046.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1267.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 14892.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16296.47it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 166.66it/s]100%|██████████| 176/176 [00:00<00:00, 1198.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15786.27it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3800.8 words/s - loss: 0.2615
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3720.2 words/s - loss: 0.2429
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3601.3 words/s - loss: 0.2678
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3669.9 words/s - loss: 0.2557
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3594.6 words/s - loss: 0.2542
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3577.3 words/s - loss: 0.2569
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3525.0 words/s - loss: 0.2532
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3497.4 words/s - loss: 0.2584
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3316.1 words/s - loss: 0.2302
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3302.3 words/s - loss: 0.2326
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3801.5 words/s - loss: 0.1788
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4094.2 words/s - loss: 0.1723
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3763.6 words/s - loss: 0.1824
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3679.4 words/s - loss: 0.1676
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3688.7 words/s - loss: 0.2071
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3484.2 words/s - loss: 0.1697
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3432.3 words/s - loss: 0.1836
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3640.2 words/s - loss: 0.1873
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3298.4 words/s - loss: 0.1640
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3174.4 words/s - loss: 0.1692
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3896.1 words/s - loss: 0.1761
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3295.3 words/s - loss: 0.1578
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3917.0 words/s - loss: 0.1833
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3387.6 words/s - loss: 0.1369
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3757.3 words/s - loss: 0.1741
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3393.8 words/s - loss: 0.1617
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3324.9 words/s - loss: 0.1728
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3437.0 words/s - loss: 0.1941
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3171.6 words/s - loss: 0.1750
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3423.1 words/s - loss: 0.1505
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3745.4 words/s - loss: 0.1478
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3712.1 words/s - loss: 0.1343
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 4018.5 words/s - loss: 0.1439
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3311.0 words/s - loss: 0.1763
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3300.4 words/s - loss: 0.1437
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3491.5 words/s - loss: 0.1294
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3314.5 words/s - loss: 0.1591
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3293.9 words/s - loss: 0.1437
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3279.1 words/s - loss: 0.1423
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3209.1 words/s - loss: 0.1667
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3382.1 words/s - loss: 0.1442
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3513.1 words/s - loss: 0.1401
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3612.3 words/s - loss: 0.1451
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3872.9 words/s - loss: 0.1616
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3643.1 words/s - loss: 0.1444
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3688.4 words/s - loss: 0.1481
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3308.9 words/s - loss: 0.1405
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3262.7 words/s - loss: 0.1480
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3029.8 words/s - loss: 0.1422
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3514.2 words/s - loss: 0.1555
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3799.5 words/s - loss: 0.1481
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4003.5 words/s - loss: 0.1404
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3371.9 words/s - loss: 0.1484
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3441.7 words/s - loss: 0.1389
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3558.3 words/s - loss: 0.1544
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3034.3 words/s - loss: 0.1392
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3782.6 words/s - loss: 0.1435
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3602.7 words/s - loss: 0.1401
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3413.3 words/s - loss: 0.1267
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3290.1 words/s - loss: 0.1249
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3877.5 words/s - loss: 0.1238
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3865.4 words/s - loss: 0.1649
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3744.5 words/s - loss: 0.1244
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3764.8 words/s - loss: 0.1426
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3601.8 words/s - loss: 0.1696
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3500.1 words/s - loss: 0.1390
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3461.3 words/s - loss: 0.1507
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3294.9 words/s - loss: 0.1378
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3181.0 words/s - loss: 0.1243
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3117.5 words/s - loss: 0.1386
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fren_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 4140.0 words/s - loss: 0.1163
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3820.7 words/s - loss: 0.1344
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3749.3 words/s - loss: 0.1493
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3827.2 words/s - loss: 0.1443
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3693.5 words/s - loss: 0.1360
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3595.2 words/s - loss: 0.1295
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3522.4 words/s - loss: 0.1537
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3495.1 words/s - loss: 0.1307
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3438.1 words/s - loss: 0.1484
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3326.2 words/s - loss: 0.1416
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fren_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3932.3 words/s - loss: 0.1260
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3897.2 words/s - loss: 0.1380
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3663.6 words/s - loss: 0.1055
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3584.9 words/s - loss: 0.1258
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3572.1 words/s - loss: 0.1429
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3537.0 words/s - loss: 0.1135
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3488.3 words/s - loss: 0.1511
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3489.3 words/s - loss: 0.1278
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3389.2 words/s - loss: 0.1264
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3430.0 words/s - loss: 0.1254
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.403 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fren_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3843.1 words/s - loss: 0.1454
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3752.3 words/s - loss: 0.1446
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3639.3 words/s - loss: 0.1392
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3601.5 words/s - loss: 0.1130
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3661.5 words/s - loss: 0.1192
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3490.5 words/s - loss: 0.1474
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3441.0 words/s - loss: 0.1407
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3342.2 words/s - loss: 0.1472
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3267.3 words/s - loss: 0.1256
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3260.5 words/s - loss: 0.1032
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.370 w/ 88 queries
INFO:__main__:removing file tmp/mbert_fren_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_9.txt
INFO:__main__:[0.40308721597970704, 0.39329474347040877, 0.41824686437936087, 0.4033347164814484, 0.40995344876863576]
INFO:__main__:[0.3586419431812774, 0.3526122772146401, 0.3781995172563563, 0.3531725361944647, 0.369657139665313]
INFO:__main__:0.41824686437936087
INFO:__main__:0.3781995172563563
INFO:__main__:best MAP: 0.398
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 712662.52it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 689988.81it/s]
100%|██████████| 5000/5000 [00:00<00:00, 689195.18it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 600026.32it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 643140.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 664244.27it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 677134.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 647949.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 585077.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 538795.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 152.61it/s] 13%|█▎        | 23/176 [00:00<00:01, 151.90it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1102.98it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1098.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16029.30it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 16362.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 144.88it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 153.25it/s]100%|██████████| 176/176 [00:00<00:00, 1051.06it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 151.83it/s]100%|██████████| 176/176 [00:00<00:00, 1106.78it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1099.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16487.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 15767.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 16608.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 146.79it/s]100%|██████████| 176/176 [00:00<00:00, 1063.58it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16301.87it/s]
 13%|█▎        | 23/176 [00:00<00:01, 127.28it/s]INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 928.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15988.68it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 166.78it/s]100%|██████████| 176/176 [00:00<00:00, 1197.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16095.71it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 172.41it/s]100%|██████████| 176/176 [00:00<00:00, 1237.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16089.04it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 177.04it/s]100%|██████████| 176/176 [00:00<00:00, 1267.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15413.15it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3856.8 words/s - loss: 0.2553
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3726.2 words/s - loss: 0.2441
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3551.0 words/s - loss: 0.2560
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3574.7 words/s - loss: 0.2632
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3541.3 words/s - loss: 0.2261
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3311.1 words/s - loss: 0.2538
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3366.5 words/s - loss: 0.2267
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3236.3 words/s - loss: 0.2506
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3195.9 words/s - loss: 0.2683
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3184.1 words/s - loss: 0.2176
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4157.3 words/s - loss: 0.1912
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3474.6 words/s - loss: 0.1601
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4222.6 words/s - loss: 0.1555
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3636.3 words/s - loss: 0.1872
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3458.4 words/s - loss: 0.1917
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3190.3 words/s - loss: 0.1659
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3441.9 words/s - loss: 0.1748
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3444.4 words/s - loss: 0.1744
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3307.2 words/s - loss: 0.1850
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3136.9 words/s - loss: 0.2064
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3637.2 words/s - loss: 0.1240
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3758.2 words/s - loss: 0.1571
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3468.1 words/s - loss: 0.1369
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3375.8 words/s - loss: 0.1801
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3536.7 words/s - loss: 0.1629
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3433.8 words/s - loss: 0.1577
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3507.2 words/s - loss: 0.1452
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3691.0 words/s - loss: 0.1705
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3345.9 words/s - loss: 0.1701
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3336.9 words/s - loss: 0.1748
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3639.7 words/s - loss: 0.1420
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3614.9 words/s - loss: 0.1490
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3712.2 words/s - loss: 0.1587
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3469.7 words/s - loss: 0.1473
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3425.6 words/s - loss: 0.1629
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3292.6 words/s - loss: 0.1501
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3426.4 words/s - loss: 0.1428
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3593.8 words/s - loss: 0.1332
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3260.1 words/s - loss: 0.1606
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3180.7 words/s - loss: 0.1316
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3777.5 words/s - loss: 0.1379
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 4154.6 words/s - loss: 0.1463
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3588.9 words/s - loss: 0.1714
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4160.4 words/s - loss: 0.1380
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3633.4 words/s - loss: 0.1512
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3416.5 words/s - loss: 0.1492
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3419.2 words/s - loss: 0.1195
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3800.7 words/s - loss: 0.1518
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3384.7 words/s - loss: 0.1429
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3223.6 words/s - loss: 0.1546
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3603.7 words/s - loss: 0.1291
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3460.4 words/s - loss: 0.1388
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3454.8 words/s - loss: 0.1481
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3918.0 words/s - loss: 0.1586
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 4130.6 words/s - loss: 0.1400
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3635.9 words/s - loss: 0.1334
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3919.6 words/s - loss: 0.1448
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3559.4 words/s - loss: 0.1337
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3292.8 words/s - loss: 0.1536
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3579.1 words/s - loss: 0.1480
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_0_5.txt
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3608.1 words/s - loss: 0.1630
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3743.9 words/s - loss: 0.1597
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3569.1 words/s - loss: 0.1323
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3581.5 words/s - loss: 0.1564
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3438.9 words/s - loss: 0.1174
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3473.1 words/s - loss: 0.1273
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3317.5 words/s - loss: 0.1422
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3267.4 words/s - loss: 0.1242
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3311.0 words/s - loss: 0.1710
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3159.9 words/s - loss: 0.1425
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.348 w/ 88 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fren_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4099.9 words/s - loss: 0.1373
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3795.2 words/s - loss: 0.1684
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3696.5 words/s - loss: 0.1395
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3595.8 words/s - loss: 0.1219
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3637.2 words/s - loss: 0.1122
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3394.5 words/s - loss: 0.1106
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3361.7 words/s - loss: 0.1260
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3315.5 words/s - loss: 0.1402
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3257.7 words/s - loss: 0.1210
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3244.7 words/s - loss: 0.1152
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.429 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fren_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3907.5 words/s - loss: 0.1198
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3835.7 words/s - loss: 0.1086
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3789.5 words/s - loss: 0.1295
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3690.6 words/s - loss: 0.1123
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3565.5 words/s - loss: 0.1147
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3520.4 words/s - loss: 0.1283
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3396.1 words/s - loss: 0.1345
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3227.4 words/s - loss: 0.1211
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3265.7 words/s - loss: 0.1136
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3175.1 words/s - loss: 0.1313
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fren_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4009.5 words/s - loss: 0.1166
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3908.0 words/s - loss: 0.1043
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3762.4 words/s - loss: 0.1386
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3552.4 words/s - loss: 0.0955
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3566.1 words/s - loss: 0.1559
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3574.0 words/s - loss: 0.1157
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3580.5 words/s - loss: 0.1327
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3468.6 words/s - loss: 0.1321
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3513.9 words/s - loss: 0.1344
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3372.0 words/s - loss: 0.1192
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.357 w/ 88 queries
INFO:__main__:removing file tmp/mbert_fren_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_9.txt
INFO:__main__:[0.4231573979179235, 0.41026554659815984, 0.42914279108504894, 0.41458745120253776, 0.4165047184346198]
INFO:__main__:[0.34637433745932233, 0.34839985408003327, 0.35811636573711325, 0.3588872121988652, 0.3569714527229388]
INFO:__main__:0.41458745120253776
INFO:__main__:0.35811636573711325
INFO:__main__:best MAP: 0.386

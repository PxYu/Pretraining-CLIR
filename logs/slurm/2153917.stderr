INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 519572.88it/s]
100%|██████████| 5000/5000 [00:00<00:00, 521809.41it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 514865.95it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 504317.05it/s]100%|██████████| 5000/5000 [00:00<00:00, 513403.84it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 501411.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 527784.57it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 517023.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 506460.59it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 512688.42it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 17718.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 16508.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
100%|██████████| 192/192 [00:00<00:00, 17668.75it/s]100%|██████████| 192/192 [00:00<00:00, 17514.28it/s]

INFO:root:Number of labelled query-document pairs in [f1] set: 34657
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 17388.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18588.85it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
100%|██████████| 192/192 [00:00<00:00, 18648.26it/s]
  0%|          | 0/192 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 34657
100%|██████████| 192/192 [00:00<00:00, 13662.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
100%|██████████| 192/192 [00:00<00:00, 17356.11it/s]
  0%|          | 0/192 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20700.37it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 15533.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20233.32it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 20240.95it/s]
INFO:__main__:f1 has 96 queries ...
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 18824.37it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 20560.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 20186.66it/s]
100%|██████████| 192/192 [00:00<00:00, 20735.55it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 15337.12it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 18920.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 18667.28it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 1239.3 words/s - loss: 0.6914
INFO:__main__:process[9] - epoch 0 - train iter 500 - 1243.8 words/s - loss: 0.6915
INFO:__main__:process[7] - epoch 0 - train iter 500 - 1252.8 words/s - loss: 0.6950
INFO:__main__:process[3] - epoch 0 - train iter 500 - 1279.2 words/s - loss: 0.6934
INFO:__main__:process[6] - epoch 0 - train iter 500 - 1279.0 words/s - loss: 0.6916
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 1259.0 words/s - loss: 0.6942
INFO:__main__:process[4] - epoch 0 - train iter 500 - 1271.2 words/s - loss: 0.6947
INFO:__main__:process[5] - epoch 0 - train iter 500 - 1268.3 words/s - loss: 0.6906
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 1280.5 words/s - loss: 0.6936
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 1257.4 words/s - loss: 0.6927
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 1527.9 words/s - loss: 0.6158
INFO:__main__:process[2] - epoch 1 - train iter 500 - 1536.3 words/s - loss: 0.6054
INFO:__main__:process[4] - epoch 1 - train iter 500 - 1560.8 words/s - loss: 0.6208
INFO:__main__:process[1] - epoch 1 - train iter 500 - 1542.4 words/s - loss: 0.6094
INFO:__main__:process[0] - epoch 1 - train iter 500 - 1569.0 words/s - loss: 0.6028
INFO:__main__:process[9] - epoch 1 - train iter 500 - 1513.3 words/s - loss: 0.6174
INFO:__main__:process[7] - epoch 1 - train iter 500 - 1528.2 words/s - loss: 0.6137
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 1526.3 words/s - loss: 0.6070
INFO:__main__:process[8] - epoch 1 - train iter 500 - 1507.7 words/s - loss: 0.6018
INFO:__main__:process[5] - epoch 1 - train iter 500 - 1538.6 words/s - loss: 0.6083
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 1364.2 words/s - loss: 0.4456
INFO:__main__:process[9] - epoch 2 - train iter 500 - 1391.7 words/s - loss: 0.4721
INFO:__main__:process[3] - epoch 2 - train iter 500 - 1370.1 words/s - loss: 0.4331
INFO:__main__:process[4] - epoch 2 - train iter 500 - 1397.3 words/s - loss: 0.4732
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 1415.4 words/s - loss: 0.4587
INFO:__main__:process[7] - epoch 2 - train iter 500 - 1389.9 words/s - loss: 0.4564
INFO:__main__:process[8] - epoch 2 - train iter 500 - 1366.9 words/s - loss: 0.4840
INFO:__main__:process[0] - epoch 2 - train iter 500 - 1384.5 words/s - loss: 0.4647
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 1379.2 words/s - loss: 0.4641
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 1385.5 words/s - loss: 0.4440
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 1639.9 words/s - loss: 0.3303
INFO:__main__:process[7] - epoch 3 - train iter 500 - 1642.6 words/s - loss: 0.3349
INFO:__main__:process[3] - epoch 3 - train iter 500 - 1642.2 words/s - loss: 0.3629
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 1624.7 words/s - loss: 0.3437
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 1649.2 words/s - loss: 0.3162
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 1637.8 words/s - loss: 0.3303
INFO:__main__:process[4] - epoch 3 - train iter 500 - 1652.4 words/s - loss: 0.3499
INFO:__main__:process[1] - epoch 3 - train iter 500 - 1686.0 words/s - loss: 0.3387
INFO:__main__:process[0] - epoch 3 - train iter 500 - 1686.4 words/s - loss: 0.3450
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 1674.9 words/s - loss: 0.3138
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 1296.8 words/s - loss: 0.2770
INFO:__main__:process[7] - epoch 4 - train iter 500 - 1301.1 words/s - loss: 0.2894
INFO:__main__:process[3] - epoch 4 - train iter 500 - 1306.5 words/s - loss: 0.2850
INFO:__main__:process[9] - epoch 4 - train iter 500 - 1297.4 words/s - loss: 0.2797
INFO:__main__:process[4] - epoch 4 - train iter 500 - 1306.8 words/s - loss: 0.2518
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 1330.2 words/s - loss: 0.2587
INFO:__main__:process[1] - epoch 4 - train iter 500 - 1316.5 words/s - loss: 0.2783
INFO:__main__:process[5] - epoch 4 - train iter 500 - 1318.5 words/s - loss: 0.2793
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 1313.1 words/s - loss: 0.2938
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 1325.4 words/s - loss: 0.2704
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 1398.6 words/s - loss: 0.2656
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 1393.0 words/s - loss: 0.2466
INFO:__main__:process[4] - epoch 5 - train iter 500 - 1386.6 words/s - loss: 0.2620
INFO:__main__:process[5] - epoch 5 - train iter 500 - 1425.2 words/s - loss: 0.2784
INFO:__main__:process[3] - epoch 5 - train iter 500 - 1399.4 words/s - loss: 0.2410
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 1389.7 words/s - loss: 0.2375
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 1417.5 words/s - loss: 0.2665
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 1397.9 words/s - loss: 0.2662
INFO:__main__:process[1] - epoch 5 - train iter 500 - 1418.3 words/s - loss: 0.2427
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 1400.9 words/s - loss: 0.2548
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 1585.1 words/s - loss: 0.2472
INFO:__main__:process[9] - epoch 6 - train iter 500 - 1569.2 words/s - loss: 0.2610
INFO:__main__:process[7] - epoch 6 - train iter 500 - 1561.7 words/s - loss: 0.2573
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 1570.8 words/s - loss: 0.2293
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 1617.9 words/s - loss: 0.2363
INFO:__main__:process[4] - epoch 6 - train iter 500 - 1548.2 words/s - loss: 0.2670
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 1574.9 words/s - loss: 0.2172
INFO:__main__:process[5] - epoch 6 - train iter 500 - 1595.4 words/s - loss: 0.2184
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 1567.8 words/s - loss: 0.2177
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 1589.2 words/s - loss: 0.2145
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 1507.0 words/s - loss: 0.2126
INFO:__main__:process[7] - epoch 7 - train iter 500 - 1489.4 words/s - loss: 0.2088
INFO:__main__:process[3] - epoch 7 - train iter 500 - 1525.0 words/s - loss: 0.2186
INFO:__main__:process[8] - epoch 7 - train iter 500 - 1528.3 words/s - loss: 0.2175
INFO:__main__:process[9] - epoch 7 - train iter 500 - 1491.0 words/s - loss: 0.2352
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 1516.9 words/s - loss: 0.2051
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 1491.4 words/s - loss: 0.2312
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 1534.4 words/s - loss: 0.2587
INFO:__main__:process[0] - epoch 7 - train iter 500 - 1521.8 words/s - loss: 0.2534
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 1495.2 words/s - loss: 0.2143
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 1418.5 words/s - loss: 0.1738
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 1436.4 words/s - loss: 0.1981
INFO:__main__:process[6] - epoch 8 - train iter 500 - 1414.1 words/s - loss: 0.2185
INFO:__main__:process[4] - epoch 8 - train iter 500 - 1430.1 words/s - loss: 0.1962
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 1450.6 words/s - loss: 0.2109
INFO:__main__:process[5] - epoch 8 - train iter 500 - 1424.1 words/s - loss: 0.2153
INFO:__main__:process[9] - epoch 8 - train iter 500 - 1426.4 words/s - loss: 0.2216
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 1412.0 words/s - loss: 0.2128
INFO:__main__:process[0] - epoch 8 - train iter 500 - 1427.4 words/s - loss: 0.1871
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 1435.3 words/s - loss: 0.2315
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 1586.2 words/s - loss: 0.1982
INFO:__main__:process[7] - epoch 9 - train iter 500 - 1531.7 words/s - loss: 0.2125
INFO:__main__:process[6]: training epoch 10 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 1582.3 words/s - loss: 0.2111
INFO:__main__:process[1] - epoch 9 - train iter 500 - 1582.1 words/s - loss: 0.1886
INFO:__main__:process[8] - epoch 9 - train iter 500 - 1607.5 words/s - loss: 0.1805
INFO:__main__:process[7]: training epoch 10 ...
INFO:__main__:process[1]: training epoch 10 ...
INFO:__main__:process[9]: training epoch 10 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 1596.6 words/s - loss: 0.1838
INFO:__main__:process[8]: training epoch 10 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 1609.5 words/s - loss: 0.2204
INFO:__main__:process[0] - epoch 9 - train iter 500 - 1580.8 words/s - loss: 0.2145
INFO:__main__:process[5]: training epoch 10 ...
INFO:__main__:process[0]: training epoch 10 ...
INFO:__main__:process[4]: training epoch 10 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 1620.0 words/s - loss: 0.1571
INFO:__main__:process[3]: training epoch 10 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 1599.4 words/s - loss: 0.1785
INFO:__main__:process[2]: training epoch 10 ...
INFO:__main__:process[6] - epoch 10 - train iter 500 - 1640.4 words/s - loss: 0.1767
INFO:__main__:process[7] - epoch 10 - train iter 500 - 1671.7 words/s - loss: 0.1803
INFO:__main__:process[9] - epoch 10 - train iter 500 - 1667.0 words/s - loss: 0.1798
INFO:__main__:process[3] - epoch 10 - train iter 500 - 1612.3 words/s - loss: 0.2281
INFO:__main__:process[4] - epoch 10 - train iter 500 - 1635.3 words/s - loss: 0.1859
INFO:__main__:process[8] - epoch 10 - train iter 500 - 1682.8 words/s - loss: 0.1894
INFO:__main__:process[1] - epoch 10 - train iter 500 - 1659.2 words/s - loss: 0.2128
INFO:__main__:process[4]: training epoch 11 ...
INFO:__main__:process[9]: training epoch 11 ...
INFO:__main__:process[7]: training epoch 11 ...
INFO:__main__:process[6]: training epoch 11 ...
INFO:__main__:process[3]: training epoch 11 ...
INFO:__main__:process[8]: training epoch 11 ...
INFO:__main__:process[0] - epoch 10 - train iter 500 - 1676.2 words/s - loss: 0.2159
INFO:__main__:process[1]: training epoch 11 ...
INFO:__main__:process[0]: training epoch 11 ...
INFO:__main__:process[5] - epoch 10 - train iter 500 - 1666.2 words/s - loss: 0.1903
INFO:__main__:process[5]: training epoch 11 ...
INFO:__main__:process[2] - epoch 10 - train iter 500 - 1677.4 words/s - loss: 0.1794
INFO:__main__:process[2]: training epoch 11 ...
INFO:__main__:process[6] - epoch 11 - train iter 500 - 1453.0 words/s - loss: 0.1771
INFO:__main__:process[8] - epoch 11 - train iter 500 - 1401.5 words/s - loss: 0.1842
INFO:__main__:process[4] - epoch 11 - train iter 500 - 1455.2 words/s - loss: 0.1610
INFO:__main__:process[9] - epoch 11 - train iter 500 - 1465.3 words/s - loss: 0.2070
INFO:__main__:process[7] - epoch 11 - train iter 500 - 1469.2 words/s - loss: 0.2007
INFO:__main__:process[8]: training epoch 12 ...
INFO:__main__:process[1] - epoch 11 - train iter 500 - 1469.7 words/s - loss: 0.1956
INFO:__main__:process[3] - epoch 11 - train iter 500 - 1427.0 words/s - loss: 0.1428
INFO:__main__:process[6]: training epoch 12 ...
INFO:__main__:process[4]: training epoch 12 ...
INFO:__main__:process[9]: training epoch 12 ...
INFO:__main__:process[7]: training epoch 12 ...
INFO:__main__:process[1]: training epoch 12 ...
INFO:__main__:process[3]: training epoch 12 ...
INFO:__main__:process[0] - epoch 11 - train iter 500 - 1453.8 words/s - loss: 0.1701
INFO:__main__:process[5] - epoch 11 - train iter 500 - 1483.5 words/s - loss: 0.1602
INFO:__main__:process[0]: training epoch 12 ...
INFO:__main__:process[5]: training epoch 12 ...
INFO:__main__:process[2] - epoch 11 - train iter 500 - 1410.6 words/s - loss: 0.1933
INFO:__main__:process[2]: training epoch 12 ...
INFO:__main__:process[3] - epoch 12 - train iter 500 - 1730.7 words/s - loss: 0.1671
INFO:__main__:process[1] - epoch 12 - train iter 500 - 1756.1 words/s - loss: 0.1458
INFO:__main__:process[7] - epoch 12 - train iter 500 - 1778.8 words/s - loss: 0.1661
INFO:__main__:process[4] - epoch 12 - train iter 500 - 1743.7 words/s - loss: 0.1863
INFO:__main__:process[9] - epoch 12 - train iter 500 - 1746.0 words/s - loss: 0.1458
INFO:__main__:process[6] - epoch 12 - train iter 500 - 1765.7 words/s - loss: 0.1845
INFO:__main__:process[1]: training epoch 13 ...
INFO:__main__:process[3]: training epoch 13 ...
INFO:__main__:process[7]: training epoch 13 ...
INFO:__main__:process[0] - epoch 12 - train iter 500 - 1767.6 words/s - loss: 0.1956
INFO:__main__:process[9]: training epoch 13 ...
INFO:__main__:process[8] - epoch 12 - train iter 500 - 1780.9 words/s - loss: 0.1607
INFO:__main__:process[4]: training epoch 13 ...
INFO:__main__:process[5] - epoch 12 - train iter 500 - 1763.3 words/s - loss: 0.2026
INFO:__main__:process[6]: training epoch 13 ...
INFO:__main__:process[0]: training epoch 13 ...
INFO:__main__:process[8]: training epoch 13 ...
INFO:__main__:process[5]: training epoch 13 ...
INFO:__main__:process[2] - epoch 12 - train iter 500 - 1763.9 words/s - loss: 0.1592
INFO:__main__:process[2]: training epoch 13 ...
INFO:__main__:process[3] - epoch 13 - train iter 500 - 1583.7 words/s - loss: 0.1553
INFO:__main__:process[6] - epoch 13 - train iter 500 - 1603.1 words/s - loss: 0.1679
INFO:__main__:process[6]: training epoch 14 ...
INFO:__main__:process[3]: training epoch 14 ...
INFO:__main__:process[4] - epoch 13 - train iter 500 - 1589.9 words/s - loss: 0.1857
INFO:__main__:process[0] - epoch 13 - train iter 500 - 1598.2 words/s - loss: 0.1808
INFO:__main__:process[1] - epoch 13 - train iter 500 - 1599.0 words/s - loss: 0.1707
INFO:__main__:process[7] - epoch 13 - train iter 500 - 1611.9 words/s - loss: 0.1592
INFO:__main__:process[8] - epoch 13 - train iter 500 - 1594.4 words/s - loss: 0.1678
INFO:__main__:process[9] - epoch 13 - train iter 500 - 1624.8 words/s - loss: 0.1508
INFO:__main__:process[5] - epoch 13 - train iter 500 - 1595.7 words/s - loss: 0.1611
INFO:__main__:process[4]: training epoch 14 ...
INFO:__main__:process[0]: training epoch 14 ...
INFO:__main__:process[1]: training epoch 14 ...
INFO:__main__:process[7]: training epoch 14 ...
INFO:__main__:process[8]: training epoch 14 ...
INFO:__main__:process[5]: training epoch 14 ...
INFO:__main__:process[9]: training epoch 14 ...
INFO:__main__:process[2] - epoch 13 - train iter 500 - 1582.8 words/s - loss: 0.1636
INFO:__main__:process[2]: training epoch 14 ...
INFO:__main__:process[1] - epoch 14 - train iter 500 - 1422.8 words/s - loss: 0.1727
INFO:__main__:process[4] - epoch 14 - train iter 500 - 1442.2 words/s - loss: 0.1690
INFO:__main__:process[6] - epoch 14 - train iter 500 - 1450.8 words/s - loss: 0.1681
INFO:__main__:process[3] - epoch 14 - train iter 500 - 1427.9 words/s - loss: 0.1555
INFO:__main__:process[4]: training epoch 15 ...
INFO:__main__:process[5] - epoch 14 - train iter 500 - 1432.4 words/s - loss: 0.1654
INFO:__main__:process[1]: training epoch 15 ...
INFO:__main__:process[7] - epoch 14 - train iter 500 - 1454.1 words/s - loss: 0.1594
INFO:__main__:process[9] - epoch 14 - train iter 500 - 1406.2 words/s - loss: 0.1791
INFO:__main__:process[5]: training epoch 15 ...
INFO:__main__:process[6]: training epoch 15 ...
INFO:__main__:process[3]: training epoch 15 ...
INFO:__main__:process[8] - epoch 14 - train iter 500 - 1442.2 words/s - loss: 0.1481
INFO:__main__:process[7]: training epoch 15 ...
INFO:__main__:process[9]: training epoch 15 ...
INFO:__main__:process[0] - epoch 14 - train iter 500 - 1429.1 words/s - loss: 0.1804
INFO:__main__:process[8]: training epoch 15 ...
INFO:__main__:process[0]: training epoch 15 ...
INFO:__main__:process[2] - epoch 14 - train iter 500 - 1477.4 words/s - loss: 0.1559
INFO:__main__:process[2]: training epoch 15 ...
INFO:__main__:process[3] - epoch 15 - train iter 500 - 1409.9 words/s - loss: 0.1488
INFO:__main__:process[7] - epoch 15 - train iter 500 - 1393.7 words/s - loss: 0.1426
INFO:__main__:process[4] - epoch 15 - train iter 500 - 1421.8 words/s - loss: 0.1401
INFO:__main__:process[3]: training epoch 16 ...
INFO:__main__:process[6] - epoch 15 - train iter 500 - 1386.5 words/s - loss: 0.1216
INFO:__main__:process[7]: training epoch 16 ...
INFO:__main__:process[9] - epoch 15 - train iter 500 - 1407.6 words/s - loss: 0.1748
INFO:__main__:process[4]: training epoch 16 ...
INFO:__main__:process[5] - epoch 15 - train iter 500 - 1426.1 words/s - loss: 0.1252
INFO:__main__:process[8] - epoch 15 - train iter 500 - 1405.5 words/s - loss: 0.1485
INFO:__main__:process[6]: training epoch 16 ...
INFO:__main__:process[9]: training epoch 16 ...
INFO:__main__:process[5]: training epoch 16 ...
INFO:__main__:process[0] - epoch 15 - train iter 500 - 1405.4 words/s - loss: 0.1522
INFO:__main__:process[8]: training epoch 16 ...
INFO:__main__:process[1] - epoch 15 - train iter 500 - 1392.8 words/s - loss: 0.1516
INFO:__main__:process[0]: training epoch 16 ...
INFO:__main__:process[1]: training epoch 16 ...
INFO:__main__:process[2] - epoch 15 - train iter 500 - 1433.8 words/s - loss: 0.1474
INFO:__main__:process[2]: training epoch 16 ...
INFO:__main__:process[3] - epoch 16 - train iter 500 - 1594.4 words/s - loss: 0.1379
INFO:__main__:process[3]: training epoch 17 ...
INFO:__main__:process[1] - epoch 16 - train iter 500 - 1535.1 words/s - loss: 0.1655
INFO:__main__:process[6] - epoch 16 - train iter 500 - 1547.2 words/s - loss: 0.1376
INFO:__main__:process[4] - epoch 16 - train iter 500 - 1580.2 words/s - loss: 0.1441
INFO:__main__:process[6]: training epoch 17 ...
INFO:__main__:process[1]: training epoch 17 ...
INFO:__main__:process[4]: training epoch 17 ...
INFO:__main__:process[9] - epoch 16 - train iter 500 - 1547.3 words/s - loss: 0.1388
INFO:__main__:process[7] - epoch 16 - train iter 500 - 1510.6 words/s - loss: 0.1570
INFO:__main__:process[0] - epoch 16 - train iter 500 - 1547.0 words/s - loss: 0.1191
INFO:__main__:process[5] - epoch 16 - train iter 500 - 1559.6 words/s - loss: 0.1479
INFO:__main__:process[8] - epoch 16 - train iter 500 - 1536.3 words/s - loss: 0.1246
INFO:__main__:process[5]: training epoch 17 ...
INFO:__main__:process[0]: training epoch 17 ...
INFO:__main__:process[7]: training epoch 17 ...
INFO:__main__:process[9]: training epoch 17 ...
INFO:__main__:process[8]: training epoch 17 ...
INFO:__main__:process[2] - epoch 16 - train iter 500 - 1527.9 words/s - loss: 0.1312
INFO:__main__:process[2]: training epoch 17 ...
INFO:__main__:process[7] - epoch 17 - train iter 500 - 1439.9 words/s - loss: 0.1491
INFO:__main__:process[6] - epoch 17 - train iter 500 - 1434.5 words/s - loss: 0.1345
INFO:__main__:process[3] - epoch 17 - train iter 500 - 1415.4 words/s - loss: 0.1245
INFO:__main__:process[9] - epoch 17 - train iter 500 - 1431.0 words/s - loss: 0.1384
INFO:__main__:process[1] - epoch 17 - train iter 500 - 1418.4 words/s - loss: 0.1500
INFO:__main__:process[8] - epoch 17 - train iter 500 - 1400.0 words/s - loss: 0.1488
INFO:__main__:process[7]: evaluating epoch 17 on f1 ...
INFO:__main__:process[3]: evaluating epoch 17 on f1 ...
INFO:__main__:process[4] - epoch 17 - train iter 500 - 1394.2 words/s - loss: 0.1278
INFO:__main__:process[6]: evaluating epoch 17 on f1 ...
INFO:__main__:process[1]: evaluating epoch 17 on f1 ...
INFO:__main__:process[9]: evaluating epoch 17 on f1 ...
INFO:__main__:process[8]: evaluating epoch 17 on f1 ...
INFO:__main__:process[5] - epoch 17 - train iter 500 - 1429.0 words/s - loss: 0.1372
INFO:__main__:process[4]: evaluating epoch 17 on f1 ...
INFO:__main__:process[5]: evaluating epoch 17 on f1 ...
INFO:__main__:process[0] - epoch 17 - train iter 500 - 1387.9 words/s - loss: 0.1262
INFO:__main__:process[0]: evaluating epoch 17 on f1 ...
INFO:__main__:process[2] - epoch 17 - train iter 500 - 1414.1 words/s - loss: 0.1351
INFO:__main__:process[2]: evaluating epoch 17 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[1] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[7] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 17 - mAP: 0.211 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 17 on f2 ...
INFO:__main__:process[9]: evaluating epoch 17 on f2 ...
INFO:__main__:process[2]: evaluating epoch 17 on f2 ...
INFO:__main__:process[3]: evaluating epoch 17 on f2 ...
INFO:__main__:process[8]: evaluating epoch 17 on f2 ...
INFO:__main__:process[7]: evaluating epoch 17 on f2 ...
INFO:__main__:process[1]: evaluating epoch 17 on f2 ...
INFO:__main__:process[4]: evaluating epoch 17 on f2 ...
INFO:__main__:process[6]: evaluating epoch 17 on f2 ...
INFO:__main__:removing file tmp/xlmr_ende_f1_9_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_3_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_4_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_8_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_1_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_6_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_5_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_7_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_0_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_2_17.txt
INFO:__main__:process[0]: evaluating epoch 17 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[6] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[4] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 17 - mAP: 0.262 w/ 96 queries
INFO:__main__:process[8]: training epoch 18 ...
INFO:__main__:process[1]: training epoch 18 ...
INFO:__main__:process[7]: training epoch 18 ...
INFO:__main__:process[3]: training epoch 18 ...
INFO:__main__:process[2]: training epoch 18 ...
INFO:__main__:process[4]: training epoch 18 ...
INFO:__main__:process[9]: training epoch 18 ...
INFO:__main__:process[6]: training epoch 18 ...
INFO:__main__:removing file tmp/xlmr_ende_f2_4_17.txt
INFO:__main__:process[5]: training epoch 18 ...
INFO:__main__:removing file tmp/xlmr_ende_f2_5_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_2_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_1_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_9_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_0_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_7_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_3_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_8_17.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_6_17.txt
INFO:__main__:process[0]: training epoch 18 ...
INFO:__main__:process[3] - epoch 18 - train iter 500 - 1423.9 words/s - loss: 0.1371
INFO:__main__:process[1] - epoch 18 - train iter 500 - 1455.1 words/s - loss: 0.1284
INFO:__main__:process[7] - epoch 18 - train iter 500 - 1430.6 words/s - loss: 0.1409
INFO:__main__:process[0] - epoch 18 - train iter 500 - 1434.9 words/s - loss: 0.1471
INFO:__main__:process[7]: evaluating epoch 18 on f1 ...
INFO:__main__:process[8] - epoch 18 - train iter 500 - 1447.8 words/s - loss: 0.1404
INFO:__main__:process[5] - epoch 18 - train iter 500 - 1457.4 words/s - loss: 0.1200
INFO:__main__:process[1]: evaluating epoch 18 on f1 ...
INFO:__main__:process[0]: evaluating epoch 18 on f1 ...
INFO:__main__:process[4] - epoch 18 - train iter 500 - 1454.1 words/s - loss: 0.1402
INFO:__main__:process[3]: evaluating epoch 18 on f1 ...
INFO:__main__:process[5]: evaluating epoch 18 on f1 ...
INFO:__main__:process[8]: evaluating epoch 18 on f1 ...
INFO:__main__:process[4]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9] - epoch 18 - train iter 500 - 1456.2 words/s - loss: 0.1366
INFO:__main__:process[6] - epoch 18 - train iter 500 - 1460.4 words/s - loss: 0.1276
INFO:__main__:process[6]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9]: evaluating epoch 18 on f1 ...
INFO:__main__:process[2] - epoch 18 - train iter 500 - 1476.1 words/s - loss: 0.1447
INFO:__main__:process[2]: evaluating epoch 18 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[0] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[8] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 18 - mAP: 0.226 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 18 on f2 ...
INFO:__main__:process[6]: evaluating epoch 18 on f2 ...
INFO:__main__:process[8]: evaluating epoch 18 on f2 ...
INFO:__main__:process[2]: evaluating epoch 18 on f2 ...
INFO:__main__:process[4]: evaluating epoch 18 on f2 ...
INFO:__main__:removing file tmp/xlmr_ende_f1_5_18.txt
INFO:__main__:process[1]: evaluating epoch 18 on f2 ...
INFO:__main__:process[5]: evaluating epoch 18 on f2 ...
INFO:__main__:process[9]: evaluating epoch 18 on f2 ...
INFO:__main__:process[7]: evaluating epoch 18 on f2 ...
INFO:__main__:removing file tmp/xlmr_ende_f1_7_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_2_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_9_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_1_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_6_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_3_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_4_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_0_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_8_18.txt
INFO:__main__:process[0]: evaluating epoch 18 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[4] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[9] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 18 - mAP: 0.275 w/ 96 queries
INFO:__main__:process[2]: training epoch 19 ...
INFO:__main__:process[8]: training epoch 19 ...
INFO:__main__:process[3]: training epoch 19 ...
INFO:__main__:process[5]: training epoch 19 ...
INFO:__main__:process[9]: training epoch 19 ...
INFO:__main__:process[4]: training epoch 19 ...
INFO:__main__:removing file tmp/xlmr_ende_f2_7_18.txt
INFO:__main__:process[6]: training epoch 19 ...
INFO:__main__:process[1]: training epoch 19 ...
INFO:__main__:process[7]: training epoch 19 ...
INFO:__main__:removing file tmp/xlmr_ende_f2_9_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_0_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_1_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_4_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_3_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_2_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_6_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_8_18.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_5_18.txt
INFO:__main__:process[0]: training epoch 19 ...
INFO:__main__:process[6] - epoch 19 - train iter 500 - 1561.9 words/s - loss: 0.1369
INFO:__main__:process[3] - epoch 19 - train iter 500 - 1562.7 words/s - loss: 0.1517
INFO:__main__:process[7] - epoch 19 - train iter 500 - 1572.6 words/s - loss: 0.1200
INFO:__main__:process[9] - epoch 19 - train iter 500 - 1592.8 words/s - loss: 0.1346
INFO:__main__:process[4] - epoch 19 - train iter 500 - 1560.7 words/s - loss: 0.1127
INFO:__main__:process[8] - epoch 19 - train iter 500 - 1593.6 words/s - loss: 0.1222
INFO:__main__:process[4]: evaluating epoch 19 on f1 ...
INFO:__main__:process[6]: evaluating epoch 19 on f1 ...
INFO:__main__:process[3]: evaluating epoch 19 on f1 ...
INFO:__main__:process[9]: evaluating epoch 19 on f1 ...
INFO:__main__:process[7]: evaluating epoch 19 on f1 ...
INFO:__main__:process[5] - epoch 19 - train iter 500 - 1566.5 words/s - loss: 0.1403
INFO:__main__:process[8]: evaluating epoch 19 on f1 ...
INFO:__main__:process[1] - epoch 19 - train iter 500 - 1591.3 words/s - loss: 0.1390
INFO:__main__:process[5]: evaluating epoch 19 on f1 ...
INFO:__main__:process[2] - epoch 19 - train iter 500 - 1588.8 words/s - loss: 0.1284
INFO:__main__:process[1]: evaluating epoch 19 on f1 ...
INFO:__main__:process[2]: evaluating epoch 19 on f1 ...
INFO:__main__:process[0] - epoch 19 - train iter 500 - 1586.0 words/s - loss: 0.1163
INFO:__main__:process[0]: evaluating epoch 19 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[7] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[9] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 19 - mAP: 0.230 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 19 on f2 ...
INFO:__main__:process[4]: evaluating epoch 19 on f2 ...
INFO:__main__:process[8]: evaluating epoch 19 on f2 ...
INFO:__main__:process[9]: evaluating epoch 19 on f2 ...
INFO:__main__:process[1]: evaluating epoch 19 on f2 ...
INFO:__main__:process[2]: evaluating epoch 19 on f2 ...
INFO:__main__:process[6]: evaluating epoch 19 on f2 ...
INFO:__main__:process[7]: evaluating epoch 19 on f2 ...
INFO:__main__:removing file tmp/xlmr_ende_f1_1_19.txt
INFO:__main__:process[5]: evaluating epoch 19 on f2 ...
INFO:__main__:removing file tmp/xlmr_ende_f1_6_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_2_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_7_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_0_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_3_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_4_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_8_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_9_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f1_5_19.txt
INFO:__main__:process[0]: evaluating epoch 19 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[5] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[4] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 19 - mAP: 0.277 w/ 96 queries
INFO:__main__:removing file tmp/xlmr_ende_f2_0_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_3_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_5_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_1_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_9_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_8_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_4_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_6_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_7_19.txt
INFO:__main__:removing file tmp/xlmr_ende_f2_2_19.txt
INFO:__main__:0.23030749373663584
INFO:__main__:0.27719504153474805
INFO:__main__:best MAP: 0.254
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 479864.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 446753.87it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 514159.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 501927.15it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 530776.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 478441.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 509623.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 501063.70it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 495839.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 480876.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s] 17%|█▋        | 32/192 [00:00<00:00, 258.83it/s]100%|██████████| 192/192 [00:00<00:00, 1458.24it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19450.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
 17%|█▋        | 32/192 [00:00<00:00, 247.23it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 32/192 [00:00<00:00, 179.71it/s]100%|██████████| 192/192 [00:00<00:00, 1397.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
 17%|█▋        | 32/192 [00:00<00:00, 249.37it/s]  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1032.31it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1409.44it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s] 17%|█▋        | 32/192 [00:00<00:00, 243.52it/s] 17%|█▋        | 32/192 [00:00<00:00, 242.72it/s]100%|██████████| 192/192 [00:00<00:00, 19735.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
 17%|█▋        | 32/192 [00:00<00:00, 201.26it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 32/192 [00:00<00:00, 209.48it/s]100%|██████████| 192/192 [00:00<00:00, 20092.98it/s]
100%|██████████| 192/192 [00:00<00:00, 1378.06it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1371.46it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
100%|██████████| 192/192 [00:00<00:00, 20076.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1148.80it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 32/192 [00:00<00:00, 236.79it/s]100%|██████████| 192/192 [00:00<00:00, 1191.84it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19921.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 19536.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 19315.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
100%|██████████| 192/192 [00:00<00:00, 1335.38it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 18875.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 17705.66it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/192 [00:00<?, ?it/s] 17%|█▋        | 32/192 [00:00<00:00, 176.97it/s]100%|██████████| 192/192 [00:00<00:00, 1012.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34657
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 17321.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34332
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3611.9 words/s - loss: 0.4176
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3467.1 words/s - loss: 0.3817
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3126.3 words/s - loss: 0.4250
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3076.2 words/s - loss: 0.3970
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3108.4 words/s - loss: 0.3952
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3032.1 words/s - loss: 0.3907
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3011.2 words/s - loss: 0.3843
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2921.5 words/s - loss: 0.4014
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2753.8 words/s - loss: 0.4164
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2489.9 words/s - loss: 0.4147
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3522.3 words/s - loss: 0.2517
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2983.4 words/s - loss: 0.2110
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3123.2 words/s - loss: 0.2498
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3016.9 words/s - loss: 0.2393
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2857.0 words/s - loss: 0.2529
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3098.3 words/s - loss: 0.2480
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2794.5 words/s - loss: 0.2295
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3236.5 words/s - loss: 0.2577
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2772.0 words/s - loss: 0.2519
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2730.6 words/s - loss: 0.2551
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3958.3 words/s - loss: 0.1844
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3216.2 words/s - loss: 0.2079
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3608.4 words/s - loss: 0.1974
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3420.8 words/s - loss: 0.1935
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3488.3 words/s - loss: 0.1697
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3229.7 words/s - loss: 0.1878
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3133.8 words/s - loss: 0.2121
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3009.9 words/s - loss: 0.1993
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3203.5 words/s - loss: 0.2135
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2992.7 words/s - loss: 0.1916
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3520.5 words/s - loss: 0.1721
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3619.5 words/s - loss: 0.1479
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2840.3 words/s - loss: 0.1779
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3118.0 words/s - loss: 0.1716
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3151.4 words/s - loss: 0.1681
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3491.9 words/s - loss: 0.1803
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3167.0 words/s - loss: 0.1893
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2401.7 words/s - loss: 0.1789
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2382.8 words/s - loss: 0.1809
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3569.3 words/s - loss: 0.1536
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3755.8 words/s - loss: 0.1879
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3446.5 words/s - loss: 0.1441
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3410.0 words/s - loss: 0.1582
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3684.2 words/s - loss: 0.1658
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3649.2 words/s - loss: 0.1849
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3286.1 words/s - loss: 0.1423
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3039.0 words/s - loss: 0.1426
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2948.5 words/s - loss: 0.1643
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3878.4 words/s - loss: 0.1126
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2783.8 words/s - loss: 0.1478
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2578.7 words/s - loss: 0.1431
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3145.2 words/s - loss: 0.1667
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2675.1 words/s - loss: 0.1361
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3184.0 words/s - loss: 0.1399
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2649.7 words/s - loss: 0.1475
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2473.2 words/s - loss: 0.1313
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2733.1 words/s - loss: 0.1500
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3436.1 words/s - loss: 0.1409
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2654.9 words/s - loss: 0.1427
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2612.6 words/s - loss: 0.1253
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2694.3 words/s - loss: 0.1772
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3939.9 words/s - loss: 0.1279
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2850.1 words/s - loss: 0.1411
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2275.6 words/s - loss: 0.1161
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3258.3 words/s - loss: 0.1444
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2628.0 words/s - loss: 0.1402
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3150.7 words/s - loss: 0.1525
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2534.6 words/s - loss: 0.1255
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2957.3 words/s - loss: 0.1502
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2940.3 words/s - loss: 0.1318
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2507.0 words/s - loss: 0.1178
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2854.6 words/s - loss: 0.1182
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3344.9 words/s - loss: 0.0961
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2553.2 words/s - loss: 0.1257
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3642.2 words/s - loss: 0.1254
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3741.1 words/s - loss: 0.1309
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3494.5 words/s - loss: 0.1413
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3149.0 words/s - loss: 0.0855
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2877.6 words/s - loss: 0.1145
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3341.1 words/s - loss: 0.1242
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3501.5 words/s - loss: 0.1191
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2908.5 words/s - loss: 0.1350
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2711.6 words/s - loss: 0.1139
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2763.6 words/s - loss: 0.1217
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3005.2 words/s - loss: 0.1129
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2618.4 words/s - loss: 0.1139
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2362.9 words/s - loss: 0.1230
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2747.2 words/s - loss: 0.1199
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3285.4 words/s - loss: 0.1356
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3047.6 words/s - loss: 0.1031
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2908.6 words/s - loss: 0.1192
INFO:__main__:process[2]: training epoch 10 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2411.2 words/s - loss: 0.1051
INFO:__main__:process[0]: training epoch 10 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3552.8 words/s - loss: 0.1121
INFO:__main__:process[1]: training epoch 10 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3882.1 words/s - loss: 0.1358
INFO:__main__:process[7]: training epoch 10 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2506.9 words/s - loss: 0.1181
INFO:__main__:process[9]: training epoch 10 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3133.8 words/s - loss: 0.1192
INFO:__main__:process[5]: training epoch 10 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2610.9 words/s - loss: 0.1092
INFO:__main__:process[8]: training epoch 10 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3023.4 words/s - loss: 0.1134
INFO:__main__:process[6]: training epoch 10 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3065.6 words/s - loss: 0.1203
INFO:__main__:process[3]: training epoch 10 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3168.2 words/s - loss: 0.1394
INFO:__main__:process[4]: training epoch 10 ...
INFO:__main__:process[2] - epoch 10 - train iter 500 - 3623.2 words/s - loss: 0.1120
INFO:__main__:process[2]: training epoch 11 ...
INFO:__main__:process[1] - epoch 10 - train iter 500 - 3681.9 words/s - loss: 0.1530
INFO:__main__:process[1]: training epoch 11 ...
INFO:__main__:process[0] - epoch 10 - train iter 500 - 2908.9 words/s - loss: 0.0850
INFO:__main__:process[0]: training epoch 11 ...
INFO:__main__:process[9] - epoch 10 - train iter 500 - 3565.9 words/s - loss: 0.1280
INFO:__main__:process[9]: training epoch 11 ...
INFO:__main__:process[5] - epoch 10 - train iter 500 - 3471.8 words/s - loss: 0.1057
INFO:__main__:process[5]: training epoch 11 ...
INFO:__main__:process[8] - epoch 10 - train iter 500 - 3486.3 words/s - loss: 0.1125
INFO:__main__:process[8]: training epoch 11 ...
INFO:__main__:process[7] - epoch 10 - train iter 500 - 3032.0 words/s - loss: 0.1106
INFO:__main__:process[7]: training epoch 11 ...
INFO:__main__:process[6] - epoch 10 - train iter 500 - 3311.1 words/s - loss: 0.1119
INFO:__main__:process[6]: training epoch 11 ...
INFO:__main__:process[3] - epoch 10 - train iter 500 - 3217.7 words/s - loss: 0.1122
INFO:__main__:process[3]: training epoch 11 ...
INFO:__main__:process[2] - epoch 11 - train iter 500 - 3851.3 words/s - loss: 0.0815
INFO:__main__:process[2]: training epoch 12 ...
INFO:__main__:process[4] - epoch 10 - train iter 500 - 2687.2 words/s - loss: 0.0965
INFO:__main__:process[4]: training epoch 11 ...
INFO:__main__:process[1] - epoch 11 - train iter 500 - 3302.7 words/s - loss: 0.1254
INFO:__main__:process[1]: training epoch 12 ...
INFO:__main__:process[0] - epoch 11 - train iter 500 - 3337.5 words/s - loss: 0.1191
INFO:__main__:process[0]: training epoch 12 ...
INFO:__main__:process[8] - epoch 11 - train iter 500 - 3684.7 words/s - loss: 0.1180
INFO:__main__:process[8]: training epoch 12 ...
INFO:__main__:process[6] - epoch 11 - train iter 500 - 3736.1 words/s - loss: 0.1206
INFO:__main__:process[6]: training epoch 12 ...
INFO:__main__:process[5] - epoch 11 - train iter 500 - 3550.8 words/s - loss: 0.1100
INFO:__main__:process[5]: training epoch 12 ...
INFO:__main__:process[9] - epoch 11 - train iter 500 - 2796.5 words/s - loss: 0.1221
INFO:__main__:process[9]: training epoch 12 ...
INFO:__main__:process[7] - epoch 11 - train iter 500 - 2780.8 words/s - loss: 0.1058
INFO:__main__:process[7]: training epoch 12 ...
INFO:__main__:process[3] - epoch 11 - train iter 500 - 2882.8 words/s - loss: 0.1058
INFO:__main__:process[3]: training epoch 12 ...
INFO:__main__:process[2] - epoch 12 - train iter 500 - 2838.4 words/s - loss: 0.1229
INFO:__main__:process[2]: training epoch 13 ...
INFO:__main__:process[4] - epoch 11 - train iter 500 - 3216.4 words/s - loss: 0.0985
INFO:__main__:process[4]: training epoch 12 ...
INFO:__main__:process[0] - epoch 12 - train iter 500 - 3150.9 words/s - loss: 0.0996
INFO:__main__:process[0]: training epoch 13 ...
INFO:__main__:process[5] - epoch 12 - train iter 500 - 3139.3 words/s - loss: 0.1032
INFO:__main__:process[5]: training epoch 13 ...
INFO:__main__:process[1] - epoch 12 - train iter 500 - 2425.3 words/s - loss: 0.1130
INFO:__main__:process[1]: training epoch 13 ...
INFO:__main__:process[8] - epoch 12 - train iter 500 - 2877.9 words/s - loss: 0.1020
INFO:__main__:process[8]: training epoch 13 ...
INFO:__main__:process[6] - epoch 12 - train iter 500 - 2920.4 words/s - loss: 0.1194
INFO:__main__:process[6]: training epoch 13 ...
INFO:__main__:process[9] - epoch 12 - train iter 500 - 3257.2 words/s - loss: 0.1020
INFO:__main__:process[9]: training epoch 13 ...
INFO:__main__:process[7] - epoch 12 - train iter 500 - 3247.4 words/s - loss: 0.0932
INFO:__main__:process[7]: training epoch 13 ...
INFO:__main__:process[3] - epoch 12 - train iter 500 - 2979.8 words/s - loss: 0.0941
INFO:__main__:process[3]: training epoch 13 ...
INFO:__main__:process[2] - epoch 13 - train iter 500 - 3528.0 words/s - loss: 0.0976
INFO:__main__:process[2]: training epoch 14 ...
INFO:__main__:process[4] - epoch 12 - train iter 500 - 3094.5 words/s - loss: 0.1142
INFO:__main__:process[4]: training epoch 13 ...
INFO:__main__:process[0] - epoch 13 - train iter 500 - 2879.8 words/s - loss: 0.1034
INFO:__main__:process[0]: training epoch 14 ...
INFO:__main__:process[5] - epoch 13 - train iter 500 - 3552.4 words/s - loss: 0.0952
INFO:__main__:process[5]: training epoch 14 ...
INFO:__main__:process[9] - epoch 13 - train iter 500 - 3421.8 words/s - loss: 0.1137
INFO:__main__:process[9]: training epoch 14 ...
INFO:__main__:process[6] - epoch 13 - train iter 500 - 3015.0 words/s - loss: 0.0894
INFO:__main__:process[6]: training epoch 14 ...
INFO:__main__:process[1] - epoch 13 - train iter 500 - 2835.0 words/s - loss: 0.1010
INFO:__main__:process[1]: training epoch 14 ...
INFO:__main__:process[3] - epoch 13 - train iter 500 - 3601.7 words/s - loss: 0.1173
INFO:__main__:process[3]: training epoch 14 ...
INFO:__main__:process[7] - epoch 13 - train iter 500 - 3218.2 words/s - loss: 0.1165
INFO:__main__:process[7]: training epoch 14 ...
INFO:__main__:process[8] - epoch 13 - train iter 500 - 2434.8 words/s - loss: 0.0979
INFO:__main__:process[8]: training epoch 14 ...
INFO:__main__:process[2] - epoch 14 - train iter 500 - 3317.2 words/s - loss: 0.1083
INFO:__main__:process[2]: training epoch 15 ...
INFO:__main__:process[4] - epoch 13 - train iter 500 - 3004.6 words/s - loss: 0.1136
INFO:__main__:process[4]: training epoch 14 ...
INFO:__main__:process[5] - epoch 14 - train iter 500 - 3166.3 words/s - loss: 0.0930
INFO:__main__:process[5]: training epoch 15 ...
INFO:__main__:process[9] - epoch 14 - train iter 500 - 3106.9 words/s - loss: 0.0915
INFO:__main__:process[9]: training epoch 15 ...
INFO:__main__:process[0] - epoch 14 - train iter 500 - 2563.7 words/s - loss: 0.1433
INFO:__main__:process[0]: training epoch 15 ...
INFO:__main__:process[1] - epoch 14 - train iter 500 - 3271.3 words/s - loss: 0.0897
INFO:__main__:process[1]: training epoch 15 ...
INFO:__main__:process[7] - epoch 14 - train iter 500 - 3050.6 words/s - loss: 0.0940
INFO:__main__:process[7]: training epoch 15 ...
INFO:__main__:process[2] - epoch 15 - train iter 500 - 3343.7 words/s - loss: 0.0844
INFO:__main__:process[2]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3] - epoch 14 - train iter 500 - 2750.3 words/s - loss: 0.1209
INFO:__main__:process[3]: training epoch 15 ...
INFO:__main__:process[8] - epoch 14 - train iter 500 - 2683.7 words/s - loss: 0.1119
INFO:__main__:process[8]: training epoch 15 ...
INFO:__main__:process[6] - epoch 14 - train iter 500 - 2316.0 words/s - loss: 0.1077
INFO:__main__:process[6]: training epoch 15 ...
INFO:__main__:process[4] - epoch 14 - train iter 500 - 2902.0 words/s - loss: 0.0985
INFO:__main__:process[4]: training epoch 15 ...
INFO:__main__:process[5] - epoch 15 - train iter 500 - 3654.3 words/s - loss: 0.0879
INFO:__main__:process[5]: evaluating epoch 15 on f1 ...
INFO:__main__:process[1] - epoch 15 - train iter 500 - 3837.2 words/s - loss: 0.1171
INFO:__main__:process[1]: evaluating epoch 15 on f1 ...
INFO:__main__:process[9] - epoch 15 - train iter 500 - 2760.9 words/s - loss: 0.0811
INFO:__main__:process[9]: evaluating epoch 15 on f1 ...
INFO:__main__:process[0] - epoch 15 - train iter 500 - 2921.2 words/s - loss: 0.1163
INFO:__main__:process[0]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3] - epoch 15 - train iter 500 - 3347.7 words/s - loss: 0.1236
INFO:__main__:process[3]: evaluating epoch 15 on f1 ...
INFO:__main__:process[7] - epoch 15 - train iter 500 - 2636.7 words/s - loss: 0.0931
INFO:__main__:process[7]: evaluating epoch 15 on f1 ...
INFO:__main__:process[6] - epoch 15 - train iter 500 - 3250.5 words/s - loss: 0.0891
INFO:__main__:process[6]: evaluating epoch 15 on f1 ...
INFO:__main__:process[8] - epoch 15 - train iter 500 - 2812.0 words/s - loss: 0.1005
INFO:__main__:process[8]: evaluating epoch 15 on f1 ...
INFO:__main__:process[4] - epoch 15 - train iter 500 - 2897.4 words/s - loss: 0.1080
INFO:__main__:process[4]: evaluating epoch 15 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[4] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[2] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 15 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[7]: evaluating epoch 15 on f2 ...
INFO:__main__:process[5]: evaluating epoch 15 on f2 ...
INFO:__main__:process[3]: evaluating epoch 15 on f2 ...
INFO:__main__:process[2]: evaluating epoch 15 on f2 ...
INFO:__main__:process[8]: evaluating epoch 15 on f2 ...
INFO:__main__:process[6]: evaluating epoch 15 on f2 ...
INFO:__main__:process[4]: evaluating epoch 15 on f2 ...
INFO:__main__:process[9]: evaluating epoch 15 on f2 ...
INFO:__main__:process[1]: evaluating epoch 15 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_9_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_15.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_15.txt
INFO:__main__:process[0]: evaluating epoch 15 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[0] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[4] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[9] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 15 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[5]: training epoch 16 ...
INFO:__main__:process[7]: training epoch 16 ...
INFO:__main__:process[8]: training epoch 16 ...
INFO:__main__:process[1]: training epoch 16 ...
INFO:__main__:process[6]: training epoch 16 ...
INFO:__main__:process[2]: training epoch 16 ...
INFO:__main__:process[9]: training epoch 16 ...
INFO:__main__:process[3]: training epoch 16 ...
INFO:__main__:removing file tmp/mbert_ende_f2_8_15.txt
INFO:__main__:process[4]: training epoch 16 ...
INFO:__main__:removing file tmp/mbert_ende_f2_9_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_15.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_15.txt
INFO:__main__:process[0]: training epoch 16 ...
INFO:__main__:process[8] - epoch 16 - train iter 500 - 4127.6 words/s - loss: 0.0790
INFO:__main__:process[8]: evaluating epoch 16 on f1 ...
INFO:__main__:process[3] - epoch 16 - train iter 500 - 3624.0 words/s - loss: 0.0670
INFO:__main__:process[3]: evaluating epoch 16 on f1 ...
INFO:__main__:process[9] - epoch 16 - train iter 500 - 3257.0 words/s - loss: 0.1113
INFO:__main__:process[9]: evaluating epoch 16 on f1 ...
INFO:__main__:process[1] - epoch 16 - train iter 500 - 3211.5 words/s - loss: 0.1213
INFO:__main__:process[1]: evaluating epoch 16 on f1 ...
INFO:__main__:process[0] - epoch 16 - train iter 500 - 3120.5 words/s - loss: 0.0861
INFO:__main__:process[0]: evaluating epoch 16 on f1 ...
INFO:__main__:process[6] - epoch 16 - train iter 500 - 2985.8 words/s - loss: 0.1420
INFO:__main__:process[6]: evaluating epoch 16 on f1 ...
INFO:__main__:process[7] - epoch 16 - train iter 500 - 2501.3 words/s - loss: 0.0879
INFO:__main__:process[7]: evaluating epoch 16 on f1 ...
INFO:__main__:process[4] - epoch 16 - train iter 500 - 2630.1 words/s - loss: 0.0772
INFO:__main__:process[4]: evaluating epoch 16 on f1 ...
INFO:__main__:process[5] - epoch 16 - train iter 500 - 2538.8 words/s - loss: 0.0959
INFO:__main__:process[5]: evaluating epoch 16 on f1 ...
INFO:__main__:process[2] - epoch 16 - train iter 500 - 2255.7 words/s - loss: 0.1087
INFO:__main__:process[2]: evaluating epoch 16 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[2] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[9] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 16 - mAP: 0.391 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 16 on f2 ...
INFO:__main__:process[7]: evaluating epoch 16 on f2 ...
INFO:__main__:process[2]: evaluating epoch 16 on f2 ...
INFO:__main__:process[3]: evaluating epoch 16 on f2 ...
INFO:__main__:process[8]: evaluating epoch 16 on f2 ...
INFO:__main__:process[6]: evaluating epoch 16 on f2 ...
INFO:__main__:process[4]: evaluating epoch 16 on f2 ...
INFO:__main__:process[1]: evaluating epoch 16 on f2 ...
INFO:__main__:process[9]: evaluating epoch 16 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_0_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_16.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_16.txt
INFO:__main__:process[0]: evaluating epoch 16 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[4] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[9] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[7] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 16 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[1]: training epoch 17 ...
INFO:__main__:process[8]: training epoch 17 ...
INFO:__main__:process[2]: training epoch 17 ...
INFO:__main__:process[6]: training epoch 17 ...
INFO:__main__:process[3]: training epoch 17 ...
INFO:__main__:process[9]: training epoch 17 ...
INFO:__main__:process[4]: training epoch 17 ...
INFO:__main__:process[7]: training epoch 17 ...
INFO:__main__:process[5]: training epoch 17 ...
INFO:__main__:removing file tmp/mbert_ende_f2_1_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_16.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_16.txt
INFO:__main__:process[0]: training epoch 17 ...
INFO:__main__:process[4] - epoch 17 - train iter 500 - 3958.2 words/s - loss: 0.0960
INFO:__main__:process[4]: evaluating epoch 17 on f1 ...
INFO:__main__:process[5] - epoch 17 - train iter 500 - 3503.6 words/s - loss: 0.0852
INFO:__main__:process[5]: evaluating epoch 17 on f1 ...
INFO:__main__:process[9] - epoch 17 - train iter 500 - 3403.6 words/s - loss: 0.0833
INFO:__main__:process[9]: evaluating epoch 17 on f1 ...
INFO:__main__:process[8] - epoch 17 - train iter 500 - 3035.6 words/s - loss: 0.1000
INFO:__main__:process[8]: evaluating epoch 17 on f1 ...
INFO:__main__:process[3] - epoch 17 - train iter 500 - 3009.9 words/s - loss: 0.1153
INFO:__main__:process[3]: evaluating epoch 17 on f1 ...
INFO:__main__:process[2] - epoch 17 - train iter 500 - 2994.1 words/s - loss: 0.0858
INFO:__main__:process[2]: evaluating epoch 17 on f1 ...
INFO:__main__:process[1] - epoch 17 - train iter 500 - 2911.1 words/s - loss: 0.1017
INFO:__main__:process[1]: evaluating epoch 17 on f1 ...
INFO:__main__:process[0] - epoch 17 - train iter 500 - 2789.2 words/s - loss: 0.0886
INFO:__main__:process[0]: evaluating epoch 17 on f1 ...
INFO:__main__:process[6] - epoch 17 - train iter 500 - 2735.4 words/s - loss: 0.1021
INFO:__main__:process[6]: evaluating epoch 17 on f1 ...
INFO:__main__:process[7] - epoch 17 - train iter 500 - 2291.6 words/s - loss: 0.1066
INFO:__main__:process[7]: evaluating epoch 17 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[7] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[1] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 17 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 17 on f2 ...
INFO:__main__:process[7]: evaluating epoch 17 on f2 ...
INFO:__main__:process[5]: evaluating epoch 17 on f2 ...
INFO:__main__:process[9]: evaluating epoch 17 on f2 ...
INFO:__main__:process[6]: evaluating epoch 17 on f2 ...
INFO:__main__:process[8]: evaluating epoch 17 on f2 ...
INFO:__main__:process[3]: evaluating epoch 17 on f2 ...
INFO:__main__:process[1]: evaluating epoch 17 on f2 ...
INFO:__main__:process[4]: evaluating epoch 17 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_2_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_17.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_17.txt
INFO:__main__:process[0]: evaluating epoch 17 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[3] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 17 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[1]: training epoch 18 ...
INFO:__main__:process[8]: training epoch 18 ...
INFO:__main__:process[4]: training epoch 18 ...
INFO:__main__:process[5]: training epoch 18 ...
INFO:__main__:process[6]: training epoch 18 ...
INFO:__main__:process[7]: training epoch 18 ...
INFO:__main__:process[3]: training epoch 18 ...
INFO:__main__:process[2]: training epoch 18 ...
INFO:__main__:removing file tmp/mbert_ende_f2_2_17.txt
INFO:__main__:process[9]: training epoch 18 ...
INFO:__main__:removing file tmp/mbert_ende_f2_6_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_17.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_17.txt
INFO:__main__:process[0]: training epoch 18 ...
INFO:__main__:process[5] - epoch 18 - train iter 500 - 4086.7 words/s - loss: 0.1204
INFO:__main__:process[5]: evaluating epoch 18 on f1 ...
INFO:__main__:process[1] - epoch 18 - train iter 500 - 3633.7 words/s - loss: 0.0766
INFO:__main__:process[1]: evaluating epoch 18 on f1 ...
INFO:__main__:process[3] - epoch 18 - train iter 500 - 3389.7 words/s - loss: 0.0960
INFO:__main__:process[3]: evaluating epoch 18 on f1 ...
INFO:__main__:process[6] - epoch 18 - train iter 500 - 3474.0 words/s - loss: 0.0873
INFO:__main__:process[6]: evaluating epoch 18 on f1 ...
INFO:__main__:process[0] - epoch 18 - train iter 500 - 3256.1 words/s - loss: 0.0888
INFO:__main__:process[0]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9] - epoch 18 - train iter 500 - 3139.1 words/s - loss: 0.0821
INFO:__main__:process[9]: evaluating epoch 18 on f1 ...
INFO:__main__:process[4] - epoch 18 - train iter 500 - 3051.1 words/s - loss: 0.0768
INFO:__main__:process[4]: evaluating epoch 18 on f1 ...
INFO:__main__:process[2] - epoch 18 - train iter 500 - 2992.4 words/s - loss: 0.0954
INFO:__main__:process[2]: evaluating epoch 18 on f1 ...
INFO:__main__:process[8] - epoch 18 - train iter 500 - 2718.6 words/s - loss: 0.0742
INFO:__main__:process[8]: evaluating epoch 18 on f1 ...
INFO:__main__:process[7] - epoch 18 - train iter 500 - 2271.5 words/s - loss: 0.0855
INFO:__main__:process[7]: evaluating epoch 18 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[7] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[9] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 18 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 18 on f2 ...
INFO:__main__:process[5]: evaluating epoch 18 on f2 ...
INFO:__main__:process[8]: evaluating epoch 18 on f2 ...
INFO:__main__:process[3]: evaluating epoch 18 on f2 ...
INFO:__main__:process[9]: evaluating epoch 18 on f2 ...
INFO:__main__:process[6]: evaluating epoch 18 on f2 ...
INFO:__main__:process[4]: evaluating epoch 18 on f2 ...
INFO:__main__:process[1]: evaluating epoch 18 on f2 ...
INFO:__main__:process[7]: evaluating epoch 18 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_5_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_18.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_18.txt
INFO:__main__:process[0]: evaluating epoch 18 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[7] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[9] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 18 - mAP: 0.401 w/ 96 queries
INFO:__main__:process[5]: training epoch 19 ...
INFO:__main__:process[2]: training epoch 19 ...
INFO:__main__:process[1]: training epoch 19 ...
INFO:__main__:process[9]: training epoch 19 ...
INFO:__main__:process[3]: training epoch 19 ...
INFO:__main__:process[7]: training epoch 19 ...
INFO:__main__:process[6]: training epoch 19 ...
INFO:__main__:process[8]: training epoch 19 ...
INFO:__main__:process[4]: training epoch 19 ...
INFO:__main__:removing file tmp/mbert_ende_f2_5_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_18.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_18.txt
INFO:__main__:process[0]: training epoch 19 ...
INFO:__main__:process[9] - epoch 19 - train iter 500 - 4503.1 words/s - loss: 0.0922
INFO:__main__:process[9]: evaluating epoch 19 on f1 ...
INFO:__main__:process[7] - epoch 19 - train iter 500 - 3654.2 words/s - loss: 0.0844
INFO:__main__:process[7]: evaluating epoch 19 on f1 ...
INFO:__main__:process[3] - epoch 19 - train iter 500 - 3524.2 words/s - loss: 0.0944
INFO:__main__:process[3]: evaluating epoch 19 on f1 ...
INFO:__main__:process[5] - epoch 19 - train iter 500 - 3270.7 words/s - loss: 0.0926
INFO:__main__:process[5]: evaluating epoch 19 on f1 ...
INFO:__main__:process[6] - epoch 19 - train iter 500 - 3155.6 words/s - loss: 0.0867
INFO:__main__:process[6]: evaluating epoch 19 on f1 ...
INFO:__main__:process[0] - epoch 19 - train iter 500 - 3211.2 words/s - loss: 0.0913
INFO:__main__:process[0]: evaluating epoch 19 on f1 ...
INFO:__main__:process[8] - epoch 19 - train iter 500 - 2858.6 words/s - loss: 0.1115
INFO:__main__:process[8]: evaluating epoch 19 on f1 ...
INFO:__main__:process[2] - epoch 19 - train iter 500 - 2604.3 words/s - loss: 0.0902
INFO:__main__:process[2]: evaluating epoch 19 on f1 ...
INFO:__main__:process[4] - epoch 19 - train iter 500 - 2573.7 words/s - loss: 0.1007
INFO:__main__:process[4]: evaluating epoch 19 on f1 ...
INFO:__main__:process[1] - epoch 19 - train iter 500 - 2567.6 words/s - loss: 0.0839
INFO:__main__:process[1]: evaluating epoch 19 on f1 ...
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[1] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:f1 set during evaluation: 34660/34657
INFO:__main__:process[4] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 19 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[9]: evaluating epoch 19 on f2 ...
INFO:__main__:process[6]: evaluating epoch 19 on f2 ...
INFO:__main__:process[1]: evaluating epoch 19 on f2 ...
INFO:__main__:process[2]: evaluating epoch 19 on f2 ...
INFO:__main__:process[5]: evaluating epoch 19 on f2 ...
INFO:__main__:process[7]: evaluating epoch 19 on f2 ...
INFO:__main__:process[3]: evaluating epoch 19 on f2 ...
INFO:__main__:process[8]: evaluating epoch 19 on f2 ...
INFO:__main__:process[4]: evaluating epoch 19 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_1_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_19.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_19.txt
INFO:__main__:process[0]: evaluating epoch 19 on f2 ...
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[3] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:f2 set during evaluation: 34340/34332
INFO:__main__:process[2] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 19 - mAP: 0.403 w/ 96 queries
INFO:__main__:removing file tmp/mbert_ende_f2_7_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_19.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_19.txt
INFO:__main__:0.396056168581518
INFO:__main__:0.4009882751257002
INFO:__main__:best MAP: 0.399
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 590896.85it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23041.52it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 26274.76it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/5000 [00:00<?, ?it/s]INFO:__main__:Evaluating every 1 epochs ...
100%|██████████| 5000/5000 [00:00<00:00, 593707.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 612325.03it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 585159.18it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 530870.80it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 609566.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 575539.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 566905.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 559315.11it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 19935.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 20148.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21814.01it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23344.45it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
100%|██████████| 185/185 [00:00<00:00, 23497.86it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 24785.07it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22162.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 20596.88it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25131.86it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
100%|██████████| 185/185 [00:00<00:00, 23597.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22376.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24855.73it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21777.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24524.99it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 20557.59it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23112.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 555595.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 18781.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 26105.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 1398.7 words/s - loss: 0.6756
INFO:__main__:process[1] - epoch 0 - train iter 500 - 1406.9 words/s - loss: 0.6692
INFO:__main__:process[3] - epoch 0 - train iter 500 - 1418.5 words/s - loss: 0.6715
INFO:__main__:process[4] - epoch 0 - train iter 500 - 1402.2 words/s - loss: 0.6645
INFO:__main__:process[6] - epoch 0 - train iter 500 - 1404.8 words/s - loss: 0.6718
INFO:__main__:process[9] - epoch 0 - train iter 500 - 1373.7 words/s - loss: 0.6735
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 1417.3 words/s - loss: 0.6605
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 1359.5 words/s - loss: 0.6728
INFO:__main__:process[7] - epoch 0 - train iter 500 - 1413.8 words/s - loss: 0.6655
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 1391.7 words/s - loss: 0.6772
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 1415.9 words/s - loss: 0.6215
INFO:__main__:process[9] - epoch 1 - train iter 500 - 1396.3 words/s - loss: 0.5910
INFO:__main__:process[8] - epoch 1 - train iter 500 - 1398.8 words/s - loss: 0.6058
INFO:__main__:process[0] - epoch 1 - train iter 500 - 1423.5 words/s - loss: 0.6128
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 1409.7 words/s - loss: 0.6246
INFO:__main__:process[6] - epoch 1 - train iter 500 - 1444.0 words/s - loss: 0.6145
INFO:__main__:process[2] - epoch 1 - train iter 500 - 1407.0 words/s - loss: 0.5836
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 1458.3 words/s - loss: 0.5982
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 1367.5 words/s - loss: 0.6005
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 1371.8 words/s - loss: 0.6025
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 1294.3 words/s - loss: 0.5762
INFO:__main__:process[5] - epoch 2 - train iter 500 - 1279.9 words/s - loss: 0.5683
INFO:__main__:process[6] - epoch 2 - train iter 500 - 1314.2 words/s - loss: 0.5495
INFO:__main__:process[3] - epoch 2 - train iter 500 - 1304.1 words/s - loss: 0.5586
INFO:__main__:process[0] - epoch 2 - train iter 500 - 1287.4 words/s - loss: 0.5543
INFO:__main__:process[9] - epoch 2 - train iter 500 - 1317.9 words/s - loss: 0.5699
INFO:__main__:process[8] - epoch 2 - train iter 500 - 1285.0 words/s - loss: 0.5600
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 1276.8 words/s - loss: 0.5732
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 1317.8 words/s - loss: 0.5568
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 1297.5 words/s - loss: 0.5750
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 1429.0 words/s - loss: 0.4872
INFO:__main__:process[8] - epoch 3 - train iter 500 - 1415.0 words/s - loss: 0.4799
INFO:__main__:process[5] - epoch 3 - train iter 500 - 1424.7 words/s - loss: 0.4592
INFO:__main__:process[1] - epoch 3 - train iter 500 - 1465.9 words/s - loss: 0.4746
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 1397.5 words/s - loss: 0.4612
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 1408.3 words/s - loss: 0.4490
INFO:__main__:process[3] - epoch 3 - train iter 500 - 1375.2 words/s - loss: 0.4738
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 1418.8 words/s - loss: 0.4820
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 1408.0 words/s - loss: 0.4584
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 1397.7 words/s - loss: 0.4844
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 1473.7 words/s - loss: 0.3915
INFO:__main__:process[4] - epoch 4 - train iter 500 - 1471.3 words/s - loss: 0.3884
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 1437.4 words/s - loss: 0.4001
INFO:__main__:process[5] - epoch 4 - train iter 500 - 1491.6 words/s - loss: 0.4019
INFO:__main__:process[6] - epoch 4 - train iter 500 - 1443.0 words/s - loss: 0.4123
INFO:__main__:process[1] - epoch 4 - train iter 500 - 1467.3 words/s - loss: 0.4474
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 1457.6 words/s - loss: 0.4132
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 1445.6 words/s - loss: 0.4245
INFO:__main__:process[7] - epoch 4 - train iter 500 - 1422.1 words/s - loss: 0.3990
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 1481.6 words/s - loss: 0.4033
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 1433.4 words/s - loss: 0.3847
INFO:__main__:process[6] - epoch 5 - train iter 500 - 1456.7 words/s - loss: 0.3688
INFO:__main__:process[0] - epoch 5 - train iter 500 - 1427.5 words/s - loss: 0.3782
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 1379.0 words/s - loss: 0.3381
INFO:__main__:process[1] - epoch 5 - train iter 500 - 1452.1 words/s - loss: 0.3448
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 1432.1 words/s - loss: 0.3727
INFO:__main__:process[2] - epoch 5 - train iter 500 - 1443.4 words/s - loss: 0.3586
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 1409.9 words/s - loss: 0.3668
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 1432.5 words/s - loss: 0.3713
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 1419.5 words/s - loss: 0.3681
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 1379.9 words/s - loss: 0.3210
INFO:__main__:process[6] - epoch 6 - train iter 500 - 1391.2 words/s - loss: 0.3059
INFO:__main__:process[5] - epoch 6 - train iter 500 - 1370.7 words/s - loss: 0.3462
INFO:__main__:process[0] - epoch 6 - train iter 500 - 1332.9 words/s - loss: 0.3515
INFO:__main__:process[8] - epoch 6 - train iter 500 - 1379.4 words/s - loss: 0.3264
INFO:__main__:process[1] - epoch 6 - train iter 500 - 1352.2 words/s - loss: 0.3141
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 1384.9 words/s - loss: 0.3353
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 1335.1 words/s - loss: 0.3843
INFO:__main__:process[2] - epoch 6 - train iter 500 - 1354.8 words/s - loss: 0.3179
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 1391.7 words/s - loss: 0.3519
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 1509.4 words/s - loss: 0.3411
INFO:__main__:process[4] - epoch 7 - train iter 500 - 1456.3 words/s - loss: 0.3146
INFO:__main__:process[5] - epoch 7 - train iter 500 - 1522.4 words/s - loss: 0.3132
INFO:__main__:process[6] - epoch 7 - train iter 500 - 1501.1 words/s - loss: 0.3181
INFO:__main__:process[0] - epoch 7 - train iter 500 - 1464.4 words/s - loss: 0.3402
INFO:__main__:process[3] - epoch 7 - train iter 500 - 1491.9 words/s - loss: 0.3338
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 1502.6 words/s - loss: 0.3026
INFO:__main__:process[7] - epoch 7 - train iter 500 - 1518.7 words/s - loss: 0.3255
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 1523.2 words/s - loss: 0.3171
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 1470.4 words/s - loss: 0.3471
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 1485.0 words/s - loss: 0.2725
INFO:__main__:process[1] - epoch 8 - train iter 500 - 1459.5 words/s - loss: 0.3077
INFO:__main__:process[4] - epoch 8 - train iter 500 - 1471.7 words/s - loss: 0.2855
INFO:__main__:process[2] - epoch 8 - train iter 500 - 1433.4 words/s - loss: 0.3030
INFO:__main__:process[5] - epoch 8 - train iter 500 - 1488.1 words/s - loss: 0.2670
INFO:__main__:process[6] - epoch 8 - train iter 500 - 1467.0 words/s - loss: 0.3111
INFO:__main__:process[9] - epoch 8 - train iter 500 - 1501.4 words/s - loss: 0.3093
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 1450.4 words/s - loss: 0.2944
INFO:__main__:process[8] - epoch 8 - train iter 500 - 1457.9 words/s - loss: 0.3140
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 1459.4 words/s - loss: 0.2902
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 1337.3 words/s - loss: 0.2769
INFO:__main__:process[1] - epoch 9 - train iter 500 - 1323.9 words/s - loss: 0.2668
INFO:__main__:process[3] - epoch 9 - train iter 500 - 1328.1 words/s - loss: 0.2608
INFO:__main__:process[6] - epoch 9 - train iter 500 - 1286.8 words/s - loss: 0.2741
INFO:__main__:process[8] - epoch 9 - train iter 500 - 1316.8 words/s - loss: 0.3116
INFO:__main__:process[0] - epoch 9 - train iter 500 - 1296.7 words/s - loss: 0.2775
INFO:__main__:process[4]: training epoch 10 ...
INFO:__main__:process[3]: training epoch 10 ...
INFO:__main__:process[1]: training epoch 10 ...
INFO:__main__:process[8]: training epoch 10 ...
INFO:__main__:process[6]: training epoch 10 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 1284.8 words/s - loss: 0.2970
INFO:__main__:process[0]: training epoch 10 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 1286.3 words/s - loss: 0.2599
INFO:__main__:process[5] - epoch 9 - train iter 500 - 1301.2 words/s - loss: 0.2766
INFO:__main__:process[7]: training epoch 10 ...
INFO:__main__:process[5]: training epoch 10 ...
INFO:__main__:process[2]: training epoch 10 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 1305.3 words/s - loss: 0.2915
INFO:__main__:process[9]: training epoch 10 ...
INFO:__main__:process[1] - epoch 10 - train iter 500 - 1479.1 words/s - loss: 0.2571
INFO:__main__:process[2] - epoch 10 - train iter 500 - 1462.9 words/s - loss: 0.2793
INFO:__main__:process[3] - epoch 10 - train iter 500 - 1445.1 words/s - loss: 0.3233
INFO:__main__:process[6] - epoch 10 - train iter 500 - 1464.0 words/s - loss: 0.2519
INFO:__main__:process[0] - epoch 10 - train iter 500 - 1464.6 words/s - loss: 0.3194
INFO:__main__:process[1]: training epoch 11 ...
INFO:__main__:process[4] - epoch 10 - train iter 500 - 1467.0 words/s - loss: 0.2779
INFO:__main__:process[3]: training epoch 11 ...
INFO:__main__:process[6]: training epoch 11 ...
INFO:__main__:process[2]: training epoch 11 ...
INFO:__main__:process[5] - epoch 10 - train iter 500 - 1463.7 words/s - loss: 0.3001
INFO:__main__:process[0]: training epoch 11 ...
INFO:__main__:process[8] - epoch 10 - train iter 500 - 1488.7 words/s - loss: 0.2910
INFO:__main__:process[7] - epoch 10 - train iter 500 - 1484.1 words/s - loss: 0.2780
INFO:__main__:process[4]: training epoch 11 ...
INFO:__main__:process[5]: training epoch 11 ...
INFO:__main__:process[7]: training epoch 11 ...
INFO:__main__:process[8]: training epoch 11 ...
INFO:__main__:process[9] - epoch 10 - train iter 500 - 1490.9 words/s - loss: 0.2776
INFO:__main__:process[9]: training epoch 11 ...
INFO:__main__:process[2] - epoch 11 - train iter 500 - 1319.7 words/s - loss: 0.2417
INFO:__main__:process[4] - epoch 11 - train iter 500 - 1269.6 words/s - loss: 0.2958
INFO:__main__:process[0] - epoch 11 - train iter 500 - 1291.4 words/s - loss: 0.2804
INFO:__main__:process[2]: training epoch 12 ...
INFO:__main__:process[4]: training epoch 12 ...
INFO:__main__:process[0]: training epoch 12 ...
INFO:__main__:process[1] - epoch 11 - train iter 500 - 1274.8 words/s - loss: 0.2566
INFO:__main__:process[6] - epoch 11 - train iter 500 - 1271.9 words/s - loss: 0.2818
INFO:__main__:process[3] - epoch 11 - train iter 500 - 1291.9 words/s - loss: 0.2694
INFO:__main__:process[5] - epoch 11 - train iter 500 - 1299.2 words/s - loss: 0.2656
INFO:__main__:process[6]: training epoch 12 ...
INFO:__main__:process[1]: training epoch 12 ...
INFO:__main__:process[3]: training epoch 12 ...
INFO:__main__:process[7] - epoch 11 - train iter 500 - 1298.1 words/s - loss: 0.2856
INFO:__main__:process[5]: training epoch 12 ...
INFO:__main__:process[8] - epoch 11 - train iter 500 - 1267.5 words/s - loss: 0.2949
INFO:__main__:process[7]: training epoch 12 ...
INFO:__main__:process[8]: training epoch 12 ...
INFO:__main__:process[9] - epoch 11 - train iter 500 - 1260.3 words/s - loss: 0.2845
INFO:__main__:process[9]: training epoch 12 ...
INFO:__main__:process[5] - epoch 12 - train iter 500 - 1297.4 words/s - loss: 0.2682
INFO:__main__:process[3] - epoch 12 - train iter 500 - 1309.7 words/s - loss: 0.2506
INFO:__main__:process[8] - epoch 12 - train iter 500 - 1281.0 words/s - loss: 0.2567
INFO:__main__:process[1] - epoch 12 - train iter 500 - 1232.2 words/s - loss: 0.2735
INFO:__main__:process[4] - epoch 12 - train iter 500 - 1287.9 words/s - loss: 0.2739
INFO:__main__:process[3]: training epoch 13 ...
INFO:__main__:process[5]: training epoch 13 ...
INFO:__main__:process[0] - epoch 12 - train iter 500 - 1269.6 words/s - loss: 0.2833
INFO:__main__:process[2] - epoch 12 - train iter 500 - 1276.9 words/s - loss: 0.2709
INFO:__main__:process[8]: training epoch 13 ...
INFO:__main__:process[7] - epoch 12 - train iter 500 - 1297.6 words/s - loss: 0.2507
INFO:__main__:process[4]: training epoch 13 ...
INFO:__main__:process[1]: training epoch 13 ...
INFO:__main__:process[0]: training epoch 13 ...
INFO:__main__:process[7]: training epoch 13 ...
INFO:__main__:process[2]: training epoch 13 ...
INFO:__main__:process[6] - epoch 12 - train iter 500 - 1304.3 words/s - loss: 0.2511
INFO:__main__:process[6]: training epoch 13 ...
INFO:__main__:process[9] - epoch 12 - train iter 500 - 1302.0 words/s - loss: 0.2480
INFO:__main__:process[9]: training epoch 13 ...
INFO:__main__:process[6] - epoch 13 - train iter 500 - 1370.3 words/s - loss: 0.2521
INFO:__main__:process[4] - epoch 13 - train iter 500 - 1346.8 words/s - loss: 0.2410
INFO:__main__:process[1] - epoch 13 - train iter 500 - 1355.2 words/s - loss: 0.2572
INFO:__main__:process[2] - epoch 13 - train iter 500 - 1364.7 words/s - loss: 0.2544
INFO:__main__:process[7] - epoch 13 - train iter 500 - 1409.0 words/s - loss: 0.2514
INFO:__main__:process[8] - epoch 13 - train iter 500 - 1360.9 words/s - loss: 0.2579
INFO:__main__:process[2]: training epoch 14 ...
INFO:__main__:process[3] - epoch 13 - train iter 500 - 1360.8 words/s - loss: 0.2468
INFO:__main__:process[1]: training epoch 14 ...
INFO:__main__:process[4]: training epoch 14 ...
INFO:__main__:process[0] - epoch 13 - train iter 500 - 1390.9 words/s - loss: 0.2058
INFO:__main__:process[6]: training epoch 14 ...
INFO:__main__:process[8]: training epoch 14 ...
INFO:__main__:process[7]: training epoch 14 ...
INFO:__main__:process[0]: training epoch 14 ...
INFO:__main__:process[3]: training epoch 14 ...
INFO:__main__:process[5] - epoch 13 - train iter 500 - 1373.8 words/s - loss: 0.2472
INFO:__main__:process[5]: training epoch 14 ...
INFO:__main__:process[9] - epoch 13 - train iter 500 - 1411.8 words/s - loss: 0.2253
INFO:__main__:process[9]: training epoch 14 ...
INFO:__main__:process[4] - epoch 14 - train iter 500 - 1402.3 words/s - loss: 0.2308
INFO:__main__:process[1] - epoch 14 - train iter 500 - 1414.5 words/s - loss: 0.2861
INFO:__main__:process[0] - epoch 14 - train iter 500 - 1415.9 words/s - loss: 0.2567
INFO:__main__:process[4]: training epoch 15 ...
INFO:__main__:process[5] - epoch 14 - train iter 500 - 1440.0 words/s - loss: 0.2413
INFO:__main__:process[0]: training epoch 15 ...
INFO:__main__:process[1]: training epoch 15 ...
INFO:__main__:process[5]: training epoch 15 ...
INFO:__main__:process[3] - epoch 14 - train iter 500 - 1410.9 words/s - loss: 0.2430
INFO:__main__:process[6] - epoch 14 - train iter 500 - 1425.5 words/s - loss: 0.2181
INFO:__main__:process[8] - epoch 14 - train iter 500 - 1468.6 words/s - loss: 0.2322
INFO:__main__:process[7] - epoch 14 - train iter 500 - 1417.8 words/s - loss: 0.2347
INFO:__main__:process[3]: training epoch 15 ...
INFO:__main__:process[8]: training epoch 15 ...
INFO:__main__:process[6]: training epoch 15 ...
INFO:__main__:process[7]: training epoch 15 ...
INFO:__main__:process[2] - epoch 14 - train iter 500 - 1397.1 words/s - loss: 0.2327
INFO:__main__:process[2]: training epoch 15 ...
INFO:__main__:process[9] - epoch 14 - train iter 500 - 1433.4 words/s - loss: 0.2116
INFO:__main__:process[9]: training epoch 15 ...
INFO:__main__:process[6] - epoch 15 - train iter 500 - 1320.3 words/s - loss: 0.2202
INFO:__main__:process[7] - epoch 15 - train iter 500 - 1296.9 words/s - loss: 0.2484
INFO:__main__:process[5] - epoch 15 - train iter 500 - 1327.1 words/s - loss: 0.2726
INFO:__main__:process[8] - epoch 15 - train iter 500 - 1330.0 words/s - loss: 0.2400
INFO:__main__:process[2] - epoch 15 - train iter 500 - 1305.5 words/s - loss: 0.2163
INFO:__main__:process[1] - epoch 15 - train iter 500 - 1327.3 words/s - loss: 0.2288
INFO:__main__:process[0] - epoch 15 - train iter 500 - 1341.2 words/s - loss: 0.1978
INFO:__main__:process[2]: evaluating epoch 15 on f1 ...
INFO:__main__:process[7]: evaluating epoch 15 on f1 ...
INFO:__main__:process[8]: evaluating epoch 15 on f1 ...
INFO:__main__:process[6]: evaluating epoch 15 on f1 ...
INFO:__main__:process[1]: evaluating epoch 15 on f1 ...
INFO:__main__:process[5]: evaluating epoch 15 on f1 ...
INFO:__main__:process[0]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3] - epoch 15 - train iter 500 - 1315.3 words/s - loss: 0.2185
INFO:__main__:process[4] - epoch 15 - train iter 500 - 1285.5 words/s - loss: 0.2134
INFO:__main__:process[3]: evaluating epoch 15 on f1 ...
INFO:__main__:process[4]: evaluating epoch 15 on f1 ...
INFO:__main__:process[9] - epoch 15 - train iter 500 - 1292.5 words/s - loss: 0.2357
INFO:__main__:process[9]: evaluating epoch 15 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[9] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[8] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 15 - mAP: 0.220 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 15 on f2 ...
INFO:__main__:process[1]: evaluating epoch 15 on f2 ...
INFO:__main__:process[6]: evaluating epoch 15 on f2 ...
INFO:__main__:process[3]: evaluating epoch 15 on f2 ...
INFO:__main__:process[2]: evaluating epoch 15 on f2 ...
INFO:__main__:process[8]: evaluating epoch 15 on f2 ...
INFO:__main__:process[9]: evaluating epoch 15 on f2 ...
INFO:__main__:process[7]: evaluating epoch 15 on f2 ...
INFO:__main__:removing file tmp/xlmr_enfr_f1_0_15.txt
INFO:__main__:process[5]: evaluating epoch 15 on f2 ...
INFO:__main__:removing file tmp/xlmr_enfr_f1_9_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_7_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_6_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_3_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_5_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_4_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_8_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_1_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_2_15.txt
INFO:__main__:process[0]: evaluating epoch 15 on f2 ...
INFO:__main__:process[2] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 15 - mAP: 0.219 w/ 92 queries
INFO:__main__:process[1]: training epoch 16 ...
INFO:__main__:process[7]: training epoch 16 ...
INFO:__main__:process[8]: training epoch 16 ...
INFO:__main__:process[3]: training epoch 16 ...
INFO:__main__:process[5]: training epoch 16 ...
INFO:__main__:process[4]: training epoch 16 ...
INFO:__main__:process[9]: training epoch 16 ...
INFO:__main__:process[6]: training epoch 16 ...
INFO:__main__:process[2]: training epoch 16 ...
INFO:__main__:removing file tmp/xlmr_enfr_f2_7_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_0_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_6_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_5_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_3_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_9_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_4_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_8_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_2_15.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_1_15.txt
INFO:__main__:process[0]: training epoch 16 ...
INFO:__main__:process[4] - epoch 16 - train iter 500 - 1463.6 words/s - loss: 0.2235
INFO:__main__:process[2] - epoch 16 - train iter 500 - 1455.4 words/s - loss: 0.2288
INFO:__main__:process[1] - epoch 16 - train iter 500 - 1447.9 words/s - loss: 0.2385
INFO:__main__:process[7] - epoch 16 - train iter 500 - 1433.8 words/s - loss: 0.2298
INFO:__main__:process[5] - epoch 16 - train iter 500 - 1433.4 words/s - loss: 0.2436
INFO:__main__:process[4]: evaluating epoch 16 on f1 ...
INFO:__main__:process[0] - epoch 16 - train iter 500 - 1440.9 words/s - loss: 0.2665
INFO:__main__:process[1]: evaluating epoch 16 on f1 ...
INFO:__main__:process[2]: evaluating epoch 16 on f1 ...
INFO:__main__:process[3] - epoch 16 - train iter 500 - 1443.6 words/s - loss: 0.2085
INFO:__main__:process[7]: evaluating epoch 16 on f1 ...
INFO:__main__:process[0]: evaluating epoch 16 on f1 ...
INFO:__main__:process[5]: evaluating epoch 16 on f1 ...
INFO:__main__:process[3]: evaluating epoch 16 on f1 ...
INFO:__main__:process[6] - epoch 16 - train iter 500 - 1459.1 words/s - loss: 0.2536
INFO:__main__:process[6]: evaluating epoch 16 on f1 ...
INFO:__main__:process[8] - epoch 16 - train iter 500 - 1426.1 words/s - loss: 0.2317
INFO:__main__:process[8]: evaluating epoch 16 on f1 ...
INFO:__main__:process[9] - epoch 16 - train iter 500 - 1433.2 words/s - loss: 0.2755
INFO:__main__:process[9]: evaluating epoch 16 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[9] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[4] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 16 - mAP: 0.237 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 16 on f2 ...
INFO:__main__:process[5]: evaluating epoch 16 on f2 ...
INFO:__main__:process[1]: evaluating epoch 16 on f2 ...
INFO:__main__:process[7]: evaluating epoch 16 on f2 ...
INFO:__main__:process[6]: evaluating epoch 16 on f2 ...
INFO:__main__:process[4]: evaluating epoch 16 on f2 ...
INFO:__main__:process[9]: evaluating epoch 16 on f2 ...
INFO:__main__:process[8]: evaluating epoch 16 on f2 ...
INFO:__main__:removing file tmp/xlmr_enfr_f1_0_16.txt
INFO:__main__:process[3]: evaluating epoch 16 on f2 ...
INFO:__main__:removing file tmp/xlmr_enfr_f1_7_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_8_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_6_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_1_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_4_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_5_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_3_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_2_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_9_16.txt
INFO:__main__:process[0]: evaluating epoch 16 on f2 ...
INFO:__main__:process[1] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 16 - mAP: 0.240 w/ 92 queries
INFO:__main__:process[3]: training epoch 17 ...
INFO:__main__:process[2]: training epoch 17 ...
INFO:__main__:process[6]: training epoch 17 ...
INFO:__main__:process[4]: training epoch 17 ...
INFO:__main__:process[9]: training epoch 17 ...
INFO:__main__:process[7]: training epoch 17 ...
INFO:__main__:removing file tmp/xlmr_enfr_f2_0_16.txt
INFO:__main__:process[5]: training epoch 17 ...
INFO:__main__:process[8]: training epoch 17 ...
INFO:__main__:process[1]: training epoch 17 ...
INFO:__main__:removing file tmp/xlmr_enfr_f2_3_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_7_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_9_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_8_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_5_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_2_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_6_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_1_16.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_4_16.txt
INFO:__main__:process[0]: training epoch 17 ...
INFO:__main__:process[2] - epoch 17 - train iter 500 - 1316.0 words/s - loss: 0.2196
INFO:__main__:process[2]: evaluating epoch 17 on f1 ...
INFO:__main__:process[0] - epoch 17 - train iter 500 - 1289.9 words/s - loss: 0.2309
INFO:__main__:process[6] - epoch 17 - train iter 500 - 1287.6 words/s - loss: 0.2516
INFO:__main__:process[4] - epoch 17 - train iter 500 - 1285.1 words/s - loss: 0.2539
INFO:__main__:process[5] - epoch 17 - train iter 500 - 1315.6 words/s - loss: 0.1972
INFO:__main__:process[7] - epoch 17 - train iter 500 - 1304.1 words/s - loss: 0.2377
INFO:__main__:process[1] - epoch 17 - train iter 500 - 1303.5 words/s - loss: 0.2340
INFO:__main__:process[0]: evaluating epoch 17 on f1 ...
INFO:__main__:process[9] - epoch 17 - train iter 500 - 1275.9 words/s - loss: 0.2281
INFO:__main__:process[6]: evaluating epoch 17 on f1 ...
INFO:__main__:process[8] - epoch 17 - train iter 500 - 1315.2 words/s - loss: 0.2275
INFO:__main__:process[7]: evaluating epoch 17 on f1 ...
INFO:__main__:process[4]: evaluating epoch 17 on f1 ...
INFO:__main__:process[5]: evaluating epoch 17 on f1 ...
INFO:__main__:process[1]: evaluating epoch 17 on f1 ...
INFO:__main__:process[9]: evaluating epoch 17 on f1 ...
INFO:__main__:process[8]: evaluating epoch 17 on f1 ...
INFO:__main__:process[3] - epoch 17 - train iter 500 - 1286.6 words/s - loss: 0.2178
INFO:__main__:process[3]: evaluating epoch 17 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[7] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[5] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[1] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 17 - mAP: 0.224 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 17 on f2 ...
INFO:__main__:process[3]: evaluating epoch 17 on f2 ...
INFO:__main__:process[1]: evaluating epoch 17 on f2 ...
INFO:__main__:process[2]: evaluating epoch 17 on f2 ...
INFO:__main__:process[8]: evaluating epoch 17 on f2 ...
INFO:__main__:process[9]: evaluating epoch 17 on f2 ...
INFO:__main__:process[7]: evaluating epoch 17 on f2 ...
INFO:__main__:process[6]: evaluating epoch 17 on f2 ...
INFO:__main__:process[5]: evaluating epoch 17 on f2 ...
INFO:__main__:removing file tmp/xlmr_enfr_f1_8_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_1_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_6_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_4_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_5_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_9_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_7_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_2_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_0_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_3_17.txt
INFO:__main__:process[0]: evaluating epoch 17 on f2 ...
INFO:__main__:process[3] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 17 - mAP: 0.235 w/ 92 queries
INFO:__main__:process[1]: training epoch 18 ...
INFO:__main__:process[4]: training epoch 18 ...
INFO:__main__:process[5]: training epoch 18 ...
INFO:__main__:process[2]: training epoch 18 ...
INFO:__main__:process[3]: training epoch 18 ...
INFO:__main__:process[6]: training epoch 18 ...
INFO:__main__:process[9]: training epoch 18 ...
INFO:__main__:process[8]: training epoch 18 ...
INFO:__main__:process[7]: training epoch 18 ...
INFO:__main__:removing file tmp/xlmr_enfr_f2_3_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_7_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_2_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_5_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_0_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_6_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_4_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_9_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_8_17.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_1_17.txt
INFO:__main__:process[0]: training epoch 18 ...
INFO:__main__:process[2] - epoch 18 - train iter 500 - 1330.6 words/s - loss: 0.2405
INFO:__main__:process[4] - epoch 18 - train iter 500 - 1351.5 words/s - loss: 0.2392
INFO:__main__:process[3] - epoch 18 - train iter 500 - 1348.2 words/s - loss: 0.2195
INFO:__main__:process[4]: evaluating epoch 18 on f1 ...
INFO:__main__:process[2]: evaluating epoch 18 on f1 ...
INFO:__main__:process[1] - epoch 18 - train iter 500 - 1391.7 words/s - loss: 0.1892
INFO:__main__:process[0] - epoch 18 - train iter 500 - 1350.7 words/s - loss: 0.2123
INFO:__main__:process[3]: evaluating epoch 18 on f1 ...
INFO:__main__:process[7] - epoch 18 - train iter 500 - 1318.7 words/s - loss: 0.2152
INFO:__main__:process[8] - epoch 18 - train iter 500 - 1345.3 words/s - loss: 0.2299
INFO:__main__:process[1]: evaluating epoch 18 on f1 ...
INFO:__main__:process[5] - epoch 18 - train iter 500 - 1387.8 words/s - loss: 0.2405
INFO:__main__:process[0]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9] - epoch 18 - train iter 500 - 1323.5 words/s - loss: 0.1991
INFO:__main__:process[7]: evaluating epoch 18 on f1 ...
INFO:__main__:process[8]: evaluating epoch 18 on f1 ...
INFO:__main__:process[5]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9]: evaluating epoch 18 on f1 ...
INFO:__main__:process[6] - epoch 18 - train iter 500 - 1312.0 words/s - loss: 0.2263
INFO:__main__:process[6]: evaluating epoch 18 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[9] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[6] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[4] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 18 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[1]: evaluating epoch 18 on f2 ...
INFO:__main__:process[3]: evaluating epoch 18 on f2 ...
INFO:__main__:process[2]: evaluating epoch 18 on f2 ...
INFO:__main__:process[4]: evaluating epoch 18 on f2 ...
INFO:__main__:process[5]: evaluating epoch 18 on f2 ...
INFO:__main__:process[9]: evaluating epoch 18 on f2 ...
INFO:__main__:process[8]: evaluating epoch 18 on f2 ...
INFO:__main__:process[7]: evaluating epoch 18 on f2 ...
INFO:__main__:process[6]: evaluating epoch 18 on f2 ...
INFO:__main__:removing file tmp/xlmr_enfr_f1_1_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_6_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_2_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_9_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_4_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_3_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_5_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_0_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_8_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_7_18.txt
INFO:__main__:process[0]: evaluating epoch 18 on f2 ...
INFO:__main__:process[1] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 18 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[9]: training epoch 19 ...
INFO:__main__:process[8]: training epoch 19 ...
INFO:__main__:process[5]: training epoch 19 ...
INFO:__main__:process[6]: training epoch 19 ...
INFO:__main__:process[2]: training epoch 19 ...
INFO:__main__:process[4]: training epoch 19 ...
INFO:__main__:process[1]: training epoch 19 ...
INFO:__main__:process[7]: training epoch 19 ...
INFO:__main__:process[3]: training epoch 19 ...
INFO:__main__:removing file tmp/xlmr_enfr_f2_0_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_9_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_1_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_7_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_2_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_6_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_3_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_5_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_8_18.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_4_18.txt
INFO:__main__:process[0]: training epoch 19 ...
INFO:__main__:process[2] - epoch 19 - train iter 500 - 1299.4 words/s - loss: 0.2056
INFO:__main__:process[4] - epoch 19 - train iter 500 - 1269.3 words/s - loss: 0.2041
INFO:__main__:process[2]: evaluating epoch 19 on f1 ...
INFO:__main__:process[0] - epoch 19 - train iter 500 - 1325.7 words/s - loss: 0.2049
INFO:__main__:process[6] - epoch 19 - train iter 500 - 1297.5 words/s - loss: 0.1803
INFO:__main__:process[9] - epoch 19 - train iter 500 - 1340.4 words/s - loss: 0.2207
INFO:__main__:process[5] - epoch 19 - train iter 500 - 1339.9 words/s - loss: 0.1911
INFO:__main__:process[4]: evaluating epoch 19 on f1 ...
INFO:__main__:process[9]: evaluating epoch 19 on f1 ...
INFO:__main__:process[0]: evaluating epoch 19 on f1 ...
INFO:__main__:process[6]: evaluating epoch 19 on f1 ...
INFO:__main__:process[5]: evaluating epoch 19 on f1 ...
INFO:__main__:process[3] - epoch 19 - train iter 500 - 1313.5 words/s - loss: 0.1961
INFO:__main__:process[7] - epoch 19 - train iter 500 - 1342.3 words/s - loss: 0.2174
INFO:__main__:process[3]: evaluating epoch 19 on f1 ...
INFO:__main__:process[7]: evaluating epoch 19 on f1 ...
INFO:__main__:process[8] - epoch 19 - train iter 500 - 1322.3 words/s - loss: 0.2146
INFO:__main__:process[8]: evaluating epoch 19 on f1 ...
INFO:__main__:process[1] - epoch 19 - train iter 500 - 1293.4 words/s - loss: 0.2078
INFO:__main__:process[1]: evaluating epoch 19 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[7] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[1] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 19 - mAP: 0.225 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 19 on f2 ...
INFO:__main__:process[6]: evaluating epoch 19 on f2 ...
INFO:__main__:process[1]: evaluating epoch 19 on f2 ...
INFO:__main__:process[2]: evaluating epoch 19 on f2 ...
INFO:__main__:process[7]: evaluating epoch 19 on f2 ...
INFO:__main__:process[9]: evaluating epoch 19 on f2 ...
INFO:__main__:process[8]: evaluating epoch 19 on f2 ...
INFO:__main__:process[3]: evaluating epoch 19 on f2 ...
INFO:__main__:process[5]: evaluating epoch 19 on f2 ...
INFO:__main__:removing file tmp/xlmr_enfr_f1_7_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_1_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_5_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_8_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_0_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_9_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_3_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_6_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_2_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f1_4_19.txt
INFO:__main__:process[0]: evaluating epoch 19 on f2 ...
INFO:__main__:process[4] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 19 - mAP: 0.229 w/ 92 queries
INFO:__main__:removing file tmp/xlmr_enfr_f2_7_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_2_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_0_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_8_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_9_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_4_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_5_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_1_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_6_19.txt
INFO:__main__:removing file tmp/xlmr_enfr_f2_3_19.txt
INFO:__main__:[0.21981369532095069, 0.23691028528178656, 0.22398163287450643, 0.22482129290635444, 0.2249082422714708]
INFO:__main__:[0.21887485227217104, 0.24049232749314617, 0.23458754499449647, 0.22897153332060663, 0.22860145005661436]
INFO:__main__:0.23691028528178656
INFO:__main__:0.24049232749314617
INFO:__main__:best MAP: 0.239
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 642923.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s] 29%|██▉       | 54/185 [00:00<00:00, 445.19it/s]100%|██████████| 185/185 [00:00<00:00, 1455.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25466.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 569460.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s] 29%|██▉       | 54/185 [00:00<00:00, 359.63it/s]100%|██████████| 185/185 [00:00<00:00, 1184.32it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23897.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 589999.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 591530.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 588641.20it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 559643.48it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 563205.50it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 580590.79it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 555934.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 551765.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s] 29%|██▉       | 54/185 [00:00<00:00, 434.31it/s]100%|██████████| 185/185 [00:00<00:00, 1419.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23687.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/185 [00:00<?, ?it/s] 29%|██▉       | 54/185 [00:00<00:00, 444.54it/s]100%|██████████| 185/185 [00:00<00:00, 1449.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24082.00it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 29%|██▉       | 54/185 [00:00<00:00, 432.24it/s]100%|██████████| 185/185 [00:00<00:00, 1411.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23781.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 29%|██▉       | 54/185 [00:00<00:00, 413.24it/s]100%|██████████| 185/185 [00:00<00:00, 1350.82it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s] 29%|██▉       | 54/185 [00:00<00:00, 370.69it/s]100%|██████████| 185/185 [00:00<00:00, 12790.46it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
100%|██████████| 185/185 [00:00<00:00, 1219.01it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/185 [00:00<?, ?it/s] 29%|██▉       | 54/185 [00:00<00:00, 395.41it/s] 29%|██▉       | 54/185 [00:00<00:00, 295.61it/s]100%|██████████| 185/185 [00:00<00:00, 1298.99it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 980.98it/s]
100%|██████████| 185/185 [00:00<00:00, 23910.58it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 29%|██▉       | 54/185 [00:00<00:00, 284.68it/s]100%|██████████| 185/185 [00:00<00:00, 24387.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
100%|██████████| 185/185 [00:00<00:00, 24465.45it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 943.66it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 27336
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23014.18it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 26180
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3302.3 words/s - loss: 0.5511
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3134.4 words/s - loss: 0.5831
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2973.1 words/s - loss: 0.5744
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2715.9 words/s - loss: 0.5946
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2708.1 words/s - loss: 0.5666
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2549.3 words/s - loss: 0.5647
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2447.2 words/s - loss: 0.5538
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2438.1 words/s - loss: 0.5686
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2321.7 words/s - loss: 0.5712
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2317.6 words/s - loss: 0.5708
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2941.7 words/s - loss: 0.3534
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2594.9 words/s - loss: 0.3190
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2817.6 words/s - loss: 0.3044
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2739.8 words/s - loss: 0.3234
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2823.8 words/s - loss: 0.3478
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2856.2 words/s - loss: 0.2853
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2835.7 words/s - loss: 0.2814
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2523.5 words/s - loss: 0.2951
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2417.4 words/s - loss: 0.3105
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2668.9 words/s - loss: 0.3041
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2993.7 words/s - loss: 0.2752
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2527.5 words/s - loss: 0.2625
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2753.5 words/s - loss: 0.2485
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3178.6 words/s - loss: 0.2819
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2887.3 words/s - loss: 0.3110
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2894.2 words/s - loss: 0.2824
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2536.4 words/s - loss: 0.2831
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2379.3 words/s - loss: 0.3131
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2470.0 words/s - loss: 0.2783
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2590.3 words/s - loss: 0.2748
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2647.0 words/s - loss: 0.2737
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3216.6 words/s - loss: 0.2775
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3803.8 words/s - loss: 0.2709
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2484.0 words/s - loss: 0.2548
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2696.8 words/s - loss: 0.2276
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2301.9 words/s - loss: 0.2530
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2633.4 words/s - loss: 0.2590
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2933.2 words/s - loss: 0.2782
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 1973.0 words/s - loss: 0.2629
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2033.7 words/s - loss: 0.2651
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2711.7 words/s - loss: 0.2483
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2703.1 words/s - loss: 0.2281
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2457.2 words/s - loss: 0.2874
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2815.2 words/s - loss: 0.2289
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2605.0 words/s - loss: 0.2511
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2671.4 words/s - loss: 0.2811
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2599.5 words/s - loss: 0.2683
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2175.2 words/s - loss: 0.2587
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2892.4 words/s - loss: 0.2642
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3075.8 words/s - loss: 0.2620
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3523.1 words/s - loss: 0.2215
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3022.4 words/s - loss: 0.2271
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2853.3 words/s - loss: 0.2472
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2701.8 words/s - loss: 0.2422
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2582.3 words/s - loss: 0.2439
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2860.0 words/s - loss: 0.2515
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2372.9 words/s - loss: 0.2592
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2531.0 words/s - loss: 0.2283
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3104.2 words/s - loss: 0.2361
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2720.6 words/s - loss: 0.2556
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2763.5 words/s - loss: 0.2417
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3031.0 words/s - loss: 0.2307
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2839.0 words/s - loss: 0.2406
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2499.2 words/s - loss: 0.2410
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2788.5 words/s - loss: 0.2360
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2323.5 words/s - loss: 0.2398
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3026.8 words/s - loss: 0.2618
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 2679.7 words/s - loss: 0.2234
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2565.6 words/s - loss: 0.2378
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2697.2 words/s - loss: 0.2479
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3167.5 words/s - loss: 0.2362
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2444.0 words/s - loss: 0.2547
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2973.0 words/s - loss: 0.2282
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2224.3 words/s - loss: 0.2409
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2117.5 words/s - loss: 0.2224
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2381.3 words/s - loss: 0.2343
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2644.1 words/s - loss: 0.2642
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2300.3 words/s - loss: 0.2381
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2774.4 words/s - loss: 0.2324
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2340.4 words/s - loss: 0.2328
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2961.5 words/s - loss: 0.2066
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2697.6 words/s - loss: 0.2435
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2762.8 words/s - loss: 0.2050
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2690.6 words/s - loss: 0.2281
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2178.6 words/s - loss: 0.2217
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2397.3 words/s - loss: 0.2239
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2565.1 words/s - loss: 0.2396
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2614.4 words/s - loss: 0.2368
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2805.5 words/s - loss: 0.2197
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2959.0 words/s - loss: 0.2395
INFO:__main__:process[4]: training epoch 10 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2435.2 words/s - loss: 0.2241
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3461.3 words/s - loss: 0.2234
INFO:__main__:process[2]: training epoch 10 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2606.0 words/s - loss: 0.1914
INFO:__main__:process[5]: training epoch 10 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2653.6 words/s - loss: 0.2258
INFO:__main__:process[0]: training epoch 10 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3313.3 words/s - loss: 0.2396
INFO:__main__:process[7]: training epoch 10 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3115.9 words/s - loss: 0.2152
INFO:__main__:process[8]: training epoch 10 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2636.6 words/s - loss: 0.2175
INFO:__main__:process[9]: training epoch 10 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2539.0 words/s - loss: 0.2261
INFO:__main__:process[6]: training epoch 10 ...
INFO:__main__:process[2] - epoch 10 - train iter 500 - 2736.6 words/s - loss: 0.2016
INFO:__main__:process[2]: training epoch 11 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2555.5 words/s - loss: 0.1947
INFO:__main__:process[3]: training epoch 10 ...
INFO:__main__:process[4] - epoch 10 - train iter 500 - 2392.3 words/s - loss: 0.2096
INFO:__main__:process[4]: training epoch 11 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 1899.7 words/s - loss: 0.2496
INFO:__main__:process[1]: training epoch 10 ...
INFO:__main__:process[5] - epoch 10 - train iter 500 - 2750.3 words/s - loss: 0.2212
INFO:__main__:process[5]: training epoch 11 ...
INFO:__main__:process[7] - epoch 10 - train iter 500 - 2604.0 words/s - loss: 0.2053
INFO:__main__:process[7]: training epoch 11 ...
INFO:__main__:process[0] - epoch 10 - train iter 500 - 2194.3 words/s - loss: 0.2142
INFO:__main__:process[0]: training epoch 11 ...
INFO:__main__:process[9] - epoch 10 - train iter 500 - 2772.6 words/s - loss: 0.2122
INFO:__main__:process[9]: training epoch 11 ...
INFO:__main__:process[8] - epoch 10 - train iter 500 - 2425.0 words/s - loss: 0.2073
INFO:__main__:process[8]: training epoch 11 ...
INFO:__main__:process[6] - epoch 10 - train iter 500 - 2458.5 words/s - loss: 0.2295
INFO:__main__:process[6]: training epoch 11 ...
INFO:__main__:process[1] - epoch 10 - train iter 500 - 3307.8 words/s - loss: 0.2179
INFO:__main__:process[1]: training epoch 11 ...
INFO:__main__:process[3] - epoch 10 - train iter 500 - 2648.0 words/s - loss: 0.2088
INFO:__main__:process[3]: training epoch 11 ...
INFO:__main__:process[2] - epoch 11 - train iter 500 - 2603.2 words/s - loss: 0.1967
INFO:__main__:process[2]: training epoch 12 ...
INFO:__main__:process[4] - epoch 11 - train iter 500 - 2581.7 words/s - loss: 0.2551
INFO:__main__:process[4]: training epoch 12 ...
INFO:__main__:process[5] - epoch 11 - train iter 500 - 2865.7 words/s - loss: 0.2319
INFO:__main__:process[5]: training epoch 12 ...
INFO:__main__:process[7] - epoch 11 - train iter 500 - 3311.8 words/s - loss: 0.1943
INFO:__main__:process[7]: training epoch 12 ...
INFO:__main__:process[0] - epoch 11 - train iter 500 - 3134.4 words/s - loss: 0.2139
INFO:__main__:process[0]: training epoch 12 ...
INFO:__main__:process[8] - epoch 11 - train iter 500 - 3112.3 words/s - loss: 0.2062
INFO:__main__:process[8]: training epoch 12 ...
INFO:__main__:process[9] - epoch 11 - train iter 500 - 2590.9 words/s - loss: 0.2177
INFO:__main__:process[9]: training epoch 12 ...
INFO:__main__:process[6] - epoch 11 - train iter 500 - 2605.9 words/s - loss: 0.2069
INFO:__main__:process[6]: training epoch 12 ...
INFO:__main__:process[1] - epoch 11 - train iter 500 - 2546.5 words/s - loss: 0.1923
INFO:__main__:process[1]: training epoch 12 ...
INFO:__main__:process[3] - epoch 11 - train iter 500 - 2839.1 words/s - loss: 0.2209
INFO:__main__:process[3]: training epoch 12 ...
INFO:__main__:process[7] - epoch 12 - train iter 500 - 3055.4 words/s - loss: 0.2010
INFO:__main__:process[7]: training epoch 13 ...
INFO:__main__:process[5] - epoch 12 - train iter 500 - 2709.0 words/s - loss: 0.2210
INFO:__main__:process[5]: training epoch 13 ...
INFO:__main__:process[2] - epoch 12 - train iter 500 - 2493.6 words/s - loss: 0.1930
INFO:__main__:process[2]: training epoch 13 ...
INFO:__main__:process[0] - epoch 12 - train iter 500 - 3177.3 words/s - loss: 0.2130
INFO:__main__:process[0]: training epoch 13 ...
INFO:__main__:process[4] - epoch 12 - train iter 500 - 2421.4 words/s - loss: 0.2023
INFO:__main__:process[4]: training epoch 13 ...
INFO:__main__:process[9] - epoch 12 - train iter 500 - 3104.6 words/s - loss: 0.1881
INFO:__main__:process[9]: training epoch 13 ...
INFO:__main__:process[8] - epoch 12 - train iter 500 - 2414.2 words/s - loss: 0.2137
INFO:__main__:process[8]: training epoch 13 ...
INFO:__main__:process[1] - epoch 12 - train iter 500 - 2797.1 words/s - loss: 0.2141
INFO:__main__:process[1]: training epoch 13 ...
INFO:__main__:process[5] - epoch 13 - train iter 500 - 3345.6 words/s - loss: 0.2004
INFO:__main__:process[5]: training epoch 14 ...
INFO:__main__:process[3] - epoch 12 - train iter 500 - 3060.7 words/s - loss: 0.2164
INFO:__main__:process[3]: training epoch 13 ...
INFO:__main__:process[6] - epoch 12 - train iter 500 - 2282.9 words/s - loss: 0.1904
INFO:__main__:process[6]: training epoch 13 ...
INFO:__main__:process[7] - epoch 13 - train iter 500 - 2433.3 words/s - loss: 0.1997
INFO:__main__:process[7]: training epoch 14 ...
INFO:__main__:process[2] - epoch 13 - train iter 500 - 2495.7 words/s - loss: 0.2141
INFO:__main__:process[2]: training epoch 14 ...
INFO:__main__:process[0] - epoch 13 - train iter 500 - 2404.5 words/s - loss: 0.1916
INFO:__main__:process[0]: training epoch 14 ...
INFO:__main__:process[4] - epoch 13 - train iter 500 - 2447.5 words/s - loss: 0.2060
INFO:__main__:process[4]: training epoch 14 ...
INFO:__main__:process[9] - epoch 13 - train iter 500 - 2965.8 words/s - loss: 0.1964
INFO:__main__:process[9]: training epoch 14 ...
INFO:__main__:process[8] - epoch 13 - train iter 500 - 2760.7 words/s - loss: 0.1909
INFO:__main__:process[8]: training epoch 14 ...
INFO:__main__:process[1] - epoch 13 - train iter 500 - 3010.2 words/s - loss: 0.1781
INFO:__main__:process[1]: training epoch 14 ...
INFO:__main__:process[5] - epoch 14 - train iter 500 - 2638.2 words/s - loss: 0.1925
INFO:__main__:process[5]: training epoch 15 ...
INFO:__main__:process[6] - epoch 13 - train iter 500 - 2722.3 words/s - loss: 0.1988
INFO:__main__:process[6]: training epoch 14 ...
INFO:__main__:process[3] - epoch 13 - train iter 500 - 2248.9 words/s - loss: 0.2079
INFO:__main__:process[3]: training epoch 14 ...
INFO:__main__:process[0] - epoch 14 - train iter 500 - 3161.2 words/s - loss: 0.1858
INFO:__main__:process[0]: training epoch 15 ...
INFO:__main__:process[7] - epoch 14 - train iter 500 - 2568.1 words/s - loss: 0.2197
INFO:__main__:process[7]: training epoch 15 ...
INFO:__main__:process[2] - epoch 14 - train iter 500 - 2576.0 words/s - loss: 0.2277
INFO:__main__:process[2]: training epoch 15 ...
INFO:__main__:process[4] - epoch 14 - train iter 500 - 2704.7 words/s - loss: 0.1784
INFO:__main__:process[4]: training epoch 15 ...
INFO:__main__:process[9] - epoch 14 - train iter 500 - 2750.9 words/s - loss: 0.1913
INFO:__main__:process[9]: training epoch 15 ...
INFO:__main__:process[8] - epoch 14 - train iter 500 - 3359.8 words/s - loss: 0.1911
INFO:__main__:process[8]: training epoch 15 ...
INFO:__main__:process[1] - epoch 14 - train iter 500 - 2627.8 words/s - loss: 0.1995
INFO:__main__:process[1]: training epoch 15 ...
INFO:__main__:process[5] - epoch 15 - train iter 500 - 2566.2 words/s - loss: 0.2150
INFO:__main__:process[5]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3] - epoch 14 - train iter 500 - 2983.2 words/s - loss: 0.2016
INFO:__main__:process[3]: training epoch 15 ...
INFO:__main__:process[6] - epoch 14 - train iter 500 - 2622.5 words/s - loss: 0.2064
INFO:__main__:process[6]: training epoch 15 ...
INFO:__main__:process[4] - epoch 15 - train iter 500 - 3085.4 words/s - loss: 0.2145
INFO:__main__:process[4]: evaluating epoch 15 on f1 ...
INFO:__main__:process[7] - epoch 15 - train iter 500 - 2774.3 words/s - loss: 0.2086
INFO:__main__:process[7]: evaluating epoch 15 on f1 ...
INFO:__main__:process[8] - epoch 15 - train iter 500 - 3344.8 words/s - loss: 0.1736
INFO:__main__:process[8]: evaluating epoch 15 on f1 ...
INFO:__main__:process[2] - epoch 15 - train iter 500 - 2712.1 words/s - loss: 0.1755
INFO:__main__:process[2]: evaluating epoch 15 on f1 ...
INFO:__main__:process[9] - epoch 15 - train iter 500 - 2852.3 words/s - loss: 0.1747
INFO:__main__:process[9]: evaluating epoch 15 on f1 ...
INFO:__main__:process[0] - epoch 15 - train iter 500 - 2252.4 words/s - loss: 0.2157
INFO:__main__:process[0]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3] - epoch 15 - train iter 500 - 3365.8 words/s - loss: 0.2100
INFO:__main__:process[3]: evaluating epoch 15 on f1 ...
INFO:__main__:process[1] - epoch 15 - train iter 500 - 2476.1 words/s - loss: 0.2315
INFO:__main__:process[1]: evaluating epoch 15 on f1 ...
INFO:__main__:process[6] - epoch 15 - train iter 500 - 3047.4 words/s - loss: 0.1884
INFO:__main__:process[6]: evaluating epoch 15 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[1] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[4] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 15 - mAP: 0.370 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 15 on f2 ...
INFO:__main__:process[3]: evaluating epoch 15 on f2 ...
INFO:__main__:process[6]: evaluating epoch 15 on f2 ...
INFO:__main__:process[7]: evaluating epoch 15 on f2 ...
INFO:__main__:process[1]: evaluating epoch 15 on f2 ...
INFO:__main__:process[5]: evaluating epoch 15 on f2 ...
INFO:__main__:process[8]: evaluating epoch 15 on f2 ...
INFO:__main__:process[9]: evaluating epoch 15 on f2 ...
INFO:__main__:process[4]: evaluating epoch 15 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_5_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_15.txt
INFO:__main__:process[0]: evaluating epoch 15 on f2 ...
INFO:__main__:process[2] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 15 - mAP: 0.362 w/ 92 queries
INFO:__main__:process[4]: training epoch 16 ...
INFO:__main__:process[6]: training epoch 16 ...
INFO:__main__:process[7]: training epoch 16 ...
INFO:__main__:process[3]: training epoch 16 ...
INFO:__main__:process[9]: training epoch 16 ...
INFO:__main__:process[8]: training epoch 16 ...
INFO:__main__:process[2]: training epoch 16 ...
INFO:__main__:process[1]: training epoch 16 ...
INFO:__main__:process[5]: training epoch 16 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_6_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_15.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_15.txt
INFO:__main__:process[0]: training epoch 16 ...
INFO:__main__:process[1] - epoch 16 - train iter 500 - 3162.9 words/s - loss: 0.2057
INFO:__main__:process[1]: evaluating epoch 16 on f1 ...
INFO:__main__:process[3] - epoch 16 - train iter 500 - 3084.5 words/s - loss: 0.1743
INFO:__main__:process[3]: evaluating epoch 16 on f1 ...
INFO:__main__:process[9] - epoch 16 - train iter 500 - 2896.6 words/s - loss: 0.1975
INFO:__main__:process[9]: evaluating epoch 16 on f1 ...
INFO:__main__:process[0] - epoch 16 - train iter 500 - 2791.1 words/s - loss: 0.2217
INFO:__main__:process[0]: evaluating epoch 16 on f1 ...
INFO:__main__:process[2] - epoch 16 - train iter 500 - 2760.6 words/s - loss: 0.1607
INFO:__main__:process[2]: evaluating epoch 16 on f1 ...
INFO:__main__:process[8] - epoch 16 - train iter 500 - 2599.3 words/s - loss: 0.2094
INFO:__main__:process[8]: evaluating epoch 16 on f1 ...
INFO:__main__:process[4] - epoch 16 - train iter 500 - 2479.5 words/s - loss: 0.1978
INFO:__main__:process[4]: evaluating epoch 16 on f1 ...
INFO:__main__:process[7] - epoch 16 - train iter 500 - 2461.8 words/s - loss: 0.2125
INFO:__main__:process[7]: evaluating epoch 16 on f1 ...
INFO:__main__:process[6] - epoch 16 - train iter 500 - 2374.0 words/s - loss: 0.2081
INFO:__main__:process[6]: evaluating epoch 16 on f1 ...
INFO:__main__:process[5] - epoch 16 - train iter 500 - 2214.8 words/s - loss: 0.1863
INFO:__main__:process[5]: evaluating epoch 16 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[5] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[6] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 16 - mAP: 0.365 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 16 on f2 ...
INFO:__main__:process[8]: evaluating epoch 16 on f2 ...
INFO:__main__:process[2]: evaluating epoch 16 on f2 ...
INFO:__main__:process[7]: evaluating epoch 16 on f2 ...
INFO:__main__:process[9]: evaluating epoch 16 on f2 ...
INFO:__main__:process[5]: evaluating epoch 16 on f2 ...
INFO:__main__:process[1]: evaluating epoch 16 on f2 ...
INFO:__main__:process[6]: evaluating epoch 16 on f2 ...
INFO:__main__:process[4]: evaluating epoch 16 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_6_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_16.txt
INFO:__main__:process[0]: evaluating epoch 16 on f2 ...
INFO:__main__:process[2] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 16 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[3]: training epoch 17 ...
INFO:__main__:process[4]: training epoch 17 ...
INFO:__main__:process[2]: training epoch 17 ...
INFO:__main__:process[1]: training epoch 17 ...
INFO:__main__:process[7]: training epoch 17 ...
INFO:__main__:process[5]: training epoch 17 ...
INFO:__main__:process[8]: training epoch 17 ...
INFO:__main__:process[6]: training epoch 17 ...
INFO:__main__:process[9]: training epoch 17 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_16.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_16.txt
INFO:__main__:process[0]: training epoch 17 ...
INFO:__main__:process[0] - epoch 17 - train iter 500 - 3162.8 words/s - loss: 0.1888
INFO:__main__:process[0]: evaluating epoch 17 on f1 ...
INFO:__main__:process[2] - epoch 17 - train iter 500 - 3246.2 words/s - loss: 0.1956
INFO:__main__:process[2]: evaluating epoch 17 on f1 ...
INFO:__main__:process[1] - epoch 17 - train iter 500 - 2938.8 words/s - loss: 0.1761
INFO:__main__:process[1]: evaluating epoch 17 on f1 ...
INFO:__main__:process[9] - epoch 17 - train iter 500 - 2766.9 words/s - loss: 0.1896
INFO:__main__:process[9]: evaluating epoch 17 on f1 ...
INFO:__main__:process[6] - epoch 17 - train iter 500 - 2745.9 words/s - loss: 0.2029
INFO:__main__:process[6]: evaluating epoch 17 on f1 ...
INFO:__main__:process[8] - epoch 17 - train iter 500 - 2773.7 words/s - loss: 0.1809
INFO:__main__:process[8]: evaluating epoch 17 on f1 ...
INFO:__main__:process[3] - epoch 17 - train iter 500 - 2731.5 words/s - loss: 0.1759
INFO:__main__:process[3]: evaluating epoch 17 on f1 ...
INFO:__main__:process[5] - epoch 17 - train iter 500 - 2662.9 words/s - loss: 0.1956
INFO:__main__:process[5]: evaluating epoch 17 on f1 ...
INFO:__main__:process[7] - epoch 17 - train iter 500 - 2613.5 words/s - loss: 0.2114
INFO:__main__:process[7]: evaluating epoch 17 on f1 ...
INFO:__main__:process[4] - epoch 17 - train iter 500 - 2460.4 words/s - loss: 0.2016
INFO:__main__:process[4]: evaluating epoch 17 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[7] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[4] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[1] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 17 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 17 on f2 ...
INFO:__main__:process[1]: evaluating epoch 17 on f2 ...
INFO:__main__:process[2]: evaluating epoch 17 on f2 ...
INFO:__main__:process[7]: evaluating epoch 17 on f2 ...
INFO:__main__:process[5]: evaluating epoch 17 on f2 ...
INFO:__main__:process[3]: evaluating epoch 17 on f2 ...
INFO:__main__:process[8]: evaluating epoch 17 on f2 ...
INFO:__main__:process[9]: evaluating epoch 17 on f2 ...
INFO:__main__:process[4]: evaluating epoch 17 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_4_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_17.txt
INFO:__main__:process[0]: evaluating epoch 17 on f2 ...
INFO:__main__:process[6] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 17 - mAP: 0.403 w/ 92 queries
INFO:__main__:process[1]: training epoch 18 ...
INFO:__main__:process[2]: training epoch 18 ...
INFO:__main__:process[6]: training epoch 18 ...
INFO:__main__:process[9]: training epoch 18 ...
INFO:__main__:process[3]: training epoch 18 ...
INFO:__main__:process[5]: training epoch 18 ...
INFO:__main__:process[8]: training epoch 18 ...
INFO:__main__:process[4]: training epoch 18 ...
INFO:__main__:process[7]: training epoch 18 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_17.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_17.txt
INFO:__main__:process[0]: training epoch 18 ...
INFO:__main__:process[0] - epoch 18 - train iter 500 - 3547.2 words/s - loss: 0.1969
INFO:__main__:process[0]: evaluating epoch 18 on f1 ...
INFO:__main__:process[2] - epoch 18 - train iter 500 - 2903.9 words/s - loss: 0.2346
INFO:__main__:process[2]: evaluating epoch 18 on f1 ...
INFO:__main__:process[5] - epoch 18 - train iter 500 - 3024.3 words/s - loss: 0.1771
INFO:__main__:process[5]: evaluating epoch 18 on f1 ...
INFO:__main__:process[8] - epoch 18 - train iter 500 - 2726.5 words/s - loss: 0.2209
INFO:__main__:process[8]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9] - epoch 18 - train iter 500 - 2733.7 words/s - loss: 0.1562
INFO:__main__:process[9]: evaluating epoch 18 on f1 ...
INFO:__main__:process[7] - epoch 18 - train iter 500 - 2576.5 words/s - loss: 0.2102
INFO:__main__:process[7]: evaluating epoch 18 on f1 ...
INFO:__main__:process[3] - epoch 18 - train iter 500 - 2616.9 words/s - loss: 0.1947
INFO:__main__:process[3]: evaluating epoch 18 on f1 ...
INFO:__main__:process[4] - epoch 18 - train iter 500 - 2445.6 words/s - loss: 0.1891
INFO:__main__:process[4]: evaluating epoch 18 on f1 ...
INFO:__main__:process[1] - epoch 18 - train iter 500 - 2345.8 words/s - loss: 0.1795
INFO:__main__:process[1]: evaluating epoch 18 on f1 ...
INFO:__main__:process[6] - epoch 18 - train iter 500 - 2071.9 words/s - loss: 0.2079
INFO:__main__:process[6]: evaluating epoch 18 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[6] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[8] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 18 - mAP: 0.388 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 18 on f2 ...
INFO:__main__:process[1]: evaluating epoch 18 on f2 ...
INFO:__main__:process[3]: evaluating epoch 18 on f2 ...
INFO:__main__:process[2]: evaluating epoch 18 on f2 ...
INFO:__main__:process[6]: evaluating epoch 18 on f2 ...
INFO:__main__:process[5]: evaluating epoch 18 on f2 ...
INFO:__main__:process[4]: evaluating epoch 18 on f2 ...
INFO:__main__:process[8]: evaluating epoch 18 on f2 ...
INFO:__main__:process[9]: evaluating epoch 18 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_2_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_18.txt
INFO:__main__:process[0]: evaluating epoch 18 on f2 ...
INFO:__main__:process[1] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 18 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[2]: training epoch 19 ...
INFO:__main__:process[5]: training epoch 19 ...
INFO:__main__:process[7]: training epoch 19 ...
INFO:__main__:process[8]: training epoch 19 ...
INFO:__main__:process[1]: training epoch 19 ...
INFO:__main__:process[6]: training epoch 19 ...
INFO:__main__:process[4]: training epoch 19 ...
INFO:__main__:process[3]: training epoch 19 ...
INFO:__main__:process[9]: training epoch 19 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_18.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_18.txt
INFO:__main__:process[0]: training epoch 19 ...
INFO:__main__:process[2] - epoch 19 - train iter 500 - 3128.5 words/s - loss: 0.2182
INFO:__main__:process[2]: evaluating epoch 19 on f1 ...
INFO:__main__:process[4] - epoch 19 - train iter 500 - 3089.2 words/s - loss: 0.2136
INFO:__main__:process[4]: evaluating epoch 19 on f1 ...
INFO:__main__:process[6] - epoch 19 - train iter 500 - 2883.6 words/s - loss: 0.1835
INFO:__main__:process[6]: evaluating epoch 19 on f1 ...
INFO:__main__:process[1] - epoch 19 - train iter 500 - 2858.1 words/s - loss: 0.1954
INFO:__main__:process[1]: evaluating epoch 19 on f1 ...
INFO:__main__:process[7] - epoch 19 - train iter 500 - 2850.4 words/s - loss: 0.1829
INFO:__main__:process[7]: evaluating epoch 19 on f1 ...
INFO:__main__:process[8] - epoch 19 - train iter 500 - 2779.1 words/s - loss: 0.1737
INFO:__main__:process[8]: evaluating epoch 19 on f1 ...
INFO:__main__:process[9] - epoch 19 - train iter 500 - 2577.9 words/s - loss: 0.1814
INFO:__main__:process[9]: evaluating epoch 19 on f1 ...
INFO:__main__:process[5] - epoch 19 - train iter 500 - 2570.7 words/s - loss: 0.1870
INFO:__main__:process[5]: evaluating epoch 19 on f1 ...
INFO:__main__:process[0] - epoch 19 - train iter 500 - 2415.6 words/s - loss: 0.2006
INFO:__main__:process[0]: evaluating epoch 19 on f1 ...
INFO:__main__:process[3] - epoch 19 - train iter 500 - 2350.1 words/s - loss: 0.1911
INFO:__main__:process[3]: evaluating epoch 19 on f1 ...
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[0] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:f1 set during evaluation: 27340/27336
INFO:__main__:process[5] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 19 - mAP: 0.385 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 19 on f2 ...
INFO:__main__:process[4]: evaluating epoch 19 on f2 ...
INFO:__main__:process[9]: evaluating epoch 19 on f2 ...
INFO:__main__:process[1]: evaluating epoch 19 on f2 ...
INFO:__main__:process[5]: evaluating epoch 19 on f2 ...
INFO:__main__:process[3]: evaluating epoch 19 on f2 ...
INFO:__main__:process[8]: evaluating epoch 19 on f2 ...
INFO:__main__:process[7]: evaluating epoch 19 on f2 ...
INFO:__main__:process[6]: evaluating epoch 19 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_0_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_19.txt
INFO:__main__:process[0]: evaluating epoch 19 on f2 ...
INFO:__main__:process[2] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 19 - mAP: 0.385 w/ 92 queries
INFO:__main__:removing file tmp/mbert_enfr_f2_1_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_19.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_19.txt
INFO:__main__:[0.37026907846130913, 0.3650028895788658, 0.38508608338933764, 0.38777664321876165, 0.3849246804649714]
INFO:__main__:[0.3618828851996414, 0.3661451532465326, 0.40334016762490676, 0.3910348204629521, 0.38523138513122585]
INFO:__main__:0.38508608338933764
INFO:__main__:0.3910348204629521
INFO:__main__:best MAP: 0.388
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='xlm-roberta-base', model_type='xlmr', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /mnt/home/puxuan/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8
INFO:transformers.configuration_utils:Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 589899.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 566705.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16444.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18889.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16610.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19031.75it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 561125.92it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 589087.64it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 592399.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 591480.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 579708.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 555993.53it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 609938.63it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16965.14it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19192.52it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16248.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18776.69it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16352.06it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17506.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 15644.03it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19045.04it/s]100%|██████████| 156/156 [00:00<00:00, 16553.95it/s]

INFO:root:Number of labelled query-document pairs in [f1] set: 28964
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18985.36it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 17262.33it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18991.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16376.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19063.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 533124.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17613.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18969.40it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2264.1 words/s - loss: 0.6911
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2322.1 words/s - loss: 0.6882
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2300.1 words/s - loss: 0.6891
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2291.7 words/s - loss: 0.6887
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2282.8 words/s - loss: 0.6890
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2277.1 words/s - loss: 0.6898
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2233.3 words/s - loss: 0.6863
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2347.5 words/s - loss: 0.6877
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2302.4 words/s - loss: 0.6900
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2263.1 words/s - loss: 0.6871
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2424.9 words/s - loss: 0.5925
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2415.2 words/s - loss: 0.5918
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2382.6 words/s - loss: 0.5724
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2400.0 words/s - loss: 0.5975
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2423.0 words/s - loss: 0.5861
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2446.7 words/s - loss: 0.5931
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2501.7 words/s - loss: 0.5899
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2496.8 words/s - loss: 0.5965
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2461.4 words/s - loss: 0.5765
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2455.6 words/s - loss: 0.6022
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2394.6 words/s - loss: 0.4722
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2455.1 words/s - loss: 0.4573
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2423.4 words/s - loss: 0.4840
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2511.6 words/s - loss: 0.4821
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2546.5 words/s - loss: 0.4952
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2429.8 words/s - loss: 0.4668
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2461.1 words/s - loss: 0.4734
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2466.8 words/s - loss: 0.4904
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2528.4 words/s - loss: 0.4836
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2449.8 words/s - loss: 0.4907
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2377.0 words/s - loss: 0.3731
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2461.6 words/s - loss: 0.3585
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2511.0 words/s - loss: 0.3653
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2489.9 words/s - loss: 0.3950
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2471.2 words/s - loss: 0.3752
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2403.3 words/s - loss: 0.4106
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2372.7 words/s - loss: 0.4086
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2442.0 words/s - loss: 0.4089
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2411.2 words/s - loss: 0.3903
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2553.0 words/s - loss: 0.4017
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2309.1 words/s - loss: 0.3412
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2330.3 words/s - loss: 0.3592
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2299.6 words/s - loss: 0.3474
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2303.9 words/s - loss: 0.3531
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2292.8 words/s - loss: 0.3494
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2325.8 words/s - loss: 0.3431
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2352.0 words/s - loss: 0.3211
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2243.9 words/s - loss: 0.3643
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2257.9 words/s - loss: 0.3322
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2289.7 words/s - loss: 0.3249
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2251.6 words/s - loss: 0.2768
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2238.9 words/s - loss: 0.3069
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2236.3 words/s - loss: 0.2994
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2246.9 words/s - loss: 0.3109
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2177.6 words/s - loss: 0.3074
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2271.5 words/s - loss: 0.2994
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2312.6 words/s - loss: 0.3230
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2201.8 words/s - loss: 0.3138
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2226.8 words/s - loss: 0.3117
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2300.3 words/s - loss: 0.2846
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2102.1 words/s - loss: 0.2843
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2212.9 words/s - loss: 0.2629
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2199.8 words/s - loss: 0.2756
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2183.0 words/s - loss: 0.2762
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2195.6 words/s - loss: 0.2736
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2209.6 words/s - loss: 0.2679
INFO:__main__:process[8] - epoch 6 - train iter 500 - 2197.3 words/s - loss: 0.2862
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2231.8 words/s - loss: 0.3077
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2199.7 words/s - loss: 0.2998
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2184.1 words/s - loss: 0.2782
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2420.8 words/s - loss: 0.2839
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2389.1 words/s - loss: 0.2752
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2388.7 words/s - loss: 0.2546
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2378.6 words/s - loss: 0.2753
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2394.9 words/s - loss: 0.2286
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2384.9 words/s - loss: 0.2750
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2370.4 words/s - loss: 0.2954
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2400.9 words/s - loss: 0.2667
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2417.5 words/s - loss: 0.2771
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2413.5 words/s - loss: 0.2580
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2464.9 words/s - loss: 0.2309
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2475.3 words/s - loss: 0.2279
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2565.4 words/s - loss: 0.2292
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2494.9 words/s - loss: 0.2233
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2502.2 words/s - loss: 0.2380
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2454.3 words/s - loss: 0.2450
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2446.2 words/s - loss: 0.2332
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2493.5 words/s - loss: 0.2297
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2388.9 words/s - loss: 0.2439
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2424.9 words/s - loss: 0.2384
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2345.6 words/s - loss: 0.2064
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2381.1 words/s - loss: 0.2238
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2320.3 words/s - loss: 0.2567
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2272.5 words/s - loss: 0.2128
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2363.6 words/s - loss: 0.2076
INFO:__main__:process[3]: training epoch 10 ...
INFO:__main__:process[6]: training epoch 10 ...
INFO:__main__:process[2]: training epoch 10 ...
INFO:__main__:process[0]: training epoch 10 ...
INFO:__main__:process[5]: training epoch 10 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2357.4 words/s - loss: 0.2238
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2311.4 words/s - loss: 0.2491
INFO:__main__:process[4]: training epoch 10 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2314.2 words/s - loss: 0.2060
INFO:__main__:process[1]: training epoch 10 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2402.4 words/s - loss: 0.2260
INFO:__main__:process[8]: training epoch 10 ...
INFO:__main__:process[9]: training epoch 10 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2290.7 words/s - loss: 0.2470
INFO:__main__:process[7]: training epoch 10 ...
INFO:__main__:process[5] - epoch 10 - train iter 500 - 2243.8 words/s - loss: 0.2203
INFO:__main__:process[6] - epoch 10 - train iter 500 - 2244.0 words/s - loss: 0.2169
INFO:__main__:process[6]: training epoch 11 ...
INFO:__main__:process[5]: training epoch 11 ...
INFO:__main__:process[2] - epoch 10 - train iter 500 - 2208.1 words/s - loss: 0.2169
INFO:__main__:process[0] - epoch 10 - train iter 500 - 2178.7 words/s - loss: 0.2446
INFO:__main__:process[7] - epoch 10 - train iter 500 - 2245.7 words/s - loss: 0.2134
INFO:__main__:process[0]: training epoch 11 ...
INFO:__main__:process[8] - epoch 10 - train iter 500 - 2240.1 words/s - loss: 0.1913
INFO:__main__:process[2]: training epoch 11 ...
INFO:__main__:process[4] - epoch 10 - train iter 500 - 2189.6 words/s - loss: 0.2180
INFO:__main__:process[7]: training epoch 11 ...
INFO:__main__:process[9] - epoch 10 - train iter 500 - 2184.3 words/s - loss: 0.2300
INFO:__main__:process[3] - epoch 10 - train iter 500 - 2228.8 words/s - loss: 0.2236
INFO:__main__:process[8]: training epoch 11 ...
INFO:__main__:process[4]: training epoch 11 ...
INFO:__main__:process[9]: training epoch 11 ...
INFO:__main__:process[3]: training epoch 11 ...
INFO:__main__:process[1] - epoch 10 - train iter 500 - 2295.4 words/s - loss: 0.2397
INFO:__main__:process[1]: training epoch 11 ...
INFO:__main__:process[2] - epoch 11 - train iter 500 - 2413.3 words/s - loss: 0.2304
INFO:__main__:process[9] - epoch 11 - train iter 500 - 2442.4 words/s - loss: 0.2175
INFO:__main__:process[8] - epoch 11 - train iter 500 - 2316.4 words/s - loss: 0.2257
INFO:__main__:process[0] - epoch 11 - train iter 500 - 2542.0 words/s - loss: 0.2067
INFO:__main__:process[5] - epoch 11 - train iter 500 - 2391.4 words/s - loss: 0.2007
INFO:__main__:process[7] - epoch 11 - train iter 500 - 2414.9 words/s - loss: 0.1922
INFO:__main__:process[2]: training epoch 12 ...
INFO:__main__:process[8]: training epoch 12 ...
INFO:__main__:process[9]: training epoch 12 ...
INFO:__main__:process[0]: training epoch 12 ...
INFO:__main__:process[1] - epoch 11 - train iter 500 - 2452.7 words/s - loss: 0.1951
INFO:__main__:process[7]: training epoch 12 ...
INFO:__main__:process[3] - epoch 11 - train iter 500 - 2432.5 words/s - loss: 0.2371
INFO:__main__:process[4] - epoch 11 - train iter 500 - 2380.3 words/s - loss: 0.2074
INFO:__main__:process[1]: training epoch 12 ...
INFO:__main__:process[6] - epoch 11 - train iter 500 - 2366.9 words/s - loss: 0.2064
INFO:__main__:process[4]: training epoch 12 ...
INFO:__main__:process[5]: training epoch 12 ...
INFO:__main__:process[3]: training epoch 12 ...
INFO:__main__:process[6]: training epoch 12 ...
INFO:__main__:process[0] - epoch 12 - train iter 500 - 2497.3 words/s - loss: 0.1992
INFO:__main__:process[5] - epoch 12 - train iter 500 - 2517.2 words/s - loss: 0.1878
INFO:__main__:process[2] - epoch 12 - train iter 500 - 2530.5 words/s - loss: 0.2289
INFO:__main__:process[8] - epoch 12 - train iter 500 - 2451.9 words/s - loss: 0.2331
INFO:__main__:process[7] - epoch 12 - train iter 500 - 2461.7 words/s - loss: 0.1992
INFO:__main__:process[0]: training epoch 13 ...
INFO:__main__:process[9] - epoch 12 - train iter 500 - 2576.1 words/s - loss: 0.1888
INFO:__main__:process[1] - epoch 12 - train iter 500 - 2529.1 words/s - loss: 0.1713
INFO:__main__:process[3] - epoch 12 - train iter 500 - 2531.9 words/s - loss: 0.1777
INFO:__main__:process[8]: training epoch 13 ...
INFO:__main__:process[4] - epoch 12 - train iter 500 - 2504.9 words/s - loss: 0.1879
INFO:__main__:process[2]: training epoch 13 ...
INFO:__main__:process[6] - epoch 12 - train iter 500 - 2467.2 words/s - loss: 0.2130
INFO:__main__:process[1]: training epoch 13 ...
INFO:__main__:process[9]: training epoch 13 ...
INFO:__main__:process[7]: training epoch 13 ...
INFO:__main__:process[3]: training epoch 13 ...
INFO:__main__:process[5]: training epoch 13 ...
INFO:__main__:process[4]: training epoch 13 ...
INFO:__main__:process[6]: training epoch 13 ...
INFO:__main__:process[6] - epoch 13 - train iter 500 - 2281.4 words/s - loss: 0.2091
INFO:__main__:process[5] - epoch 13 - train iter 500 - 2349.1 words/s - loss: 0.2243
INFO:__main__:process[6]: training epoch 14 ...
INFO:__main__:process[5]: training epoch 14 ...
INFO:__main__:process[0] - epoch 13 - train iter 500 - 2371.6 words/s - loss: 0.2202
INFO:__main__:process[8] - epoch 13 - train iter 500 - 2246.6 words/s - loss: 0.1891
INFO:__main__:process[9] - epoch 13 - train iter 500 - 2417.7 words/s - loss: 0.1713
INFO:__main__:process[2] - epoch 13 - train iter 500 - 2383.9 words/s - loss: 0.2081
INFO:__main__:process[0]: training epoch 14 ...
INFO:__main__:process[8]: training epoch 14 ...
INFO:__main__:process[7] - epoch 13 - train iter 500 - 2380.1 words/s - loss: 0.2132
INFO:__main__:process[9]: training epoch 14 ...
INFO:__main__:process[2]: training epoch 14 ...
INFO:__main__:process[3] - epoch 13 - train iter 500 - 2328.8 words/s - loss: 0.2065
INFO:__main__:process[7]: training epoch 14 ...
INFO:__main__:process[1] - epoch 13 - train iter 500 - 2336.1 words/s - loss: 0.1774
INFO:__main__:process[4] - epoch 13 - train iter 500 - 2339.0 words/s - loss: 0.2156
INFO:__main__:process[1]: training epoch 14 ...
INFO:__main__:process[3]: training epoch 14 ...
INFO:__main__:process[4]: training epoch 14 ...
INFO:__main__:process[6] - epoch 14 - train iter 500 - 2283.7 words/s - loss: 0.1935
INFO:__main__:process[5] - epoch 14 - train iter 500 - 2173.3 words/s - loss: 0.1772
INFO:__main__:process[9] - epoch 14 - train iter 500 - 2271.3 words/s - loss: 0.1935
INFO:__main__:process[6]: training epoch 15 ...
INFO:__main__:process[8] - epoch 14 - train iter 500 - 2332.4 words/s - loss: 0.2226
INFO:__main__:process[5]: training epoch 15 ...
INFO:__main__:process[3] - epoch 14 - train iter 500 - 2317.0 words/s - loss: 0.1974
INFO:__main__:process[9]: training epoch 15 ...
INFO:__main__:process[0] - epoch 14 - train iter 500 - 2367.0 words/s - loss: 0.1847
INFO:__main__:process[7] - epoch 14 - train iter 500 - 2312.6 words/s - loss: 0.1851
INFO:__main__:process[8]: training epoch 15 ...
INFO:__main__:process[1] - epoch 14 - train iter 500 - 2239.5 words/s - loss: 0.2143
INFO:__main__:process[0]: training epoch 15 ...
INFO:__main__:process[2] - epoch 14 - train iter 500 - 2301.5 words/s - loss: 0.1856
INFO:__main__:process[3]: training epoch 15 ...
INFO:__main__:process[1]: training epoch 15 ...
INFO:__main__:process[7]: training epoch 15 ...
INFO:__main__:process[2]: training epoch 15 ...
INFO:__main__:process[4] - epoch 14 - train iter 500 - 2250.0 words/s - loss: 0.1810
INFO:__main__:process[4]: training epoch 15 ...
INFO:__main__:process[2] - epoch 15 - train iter 500 - 2399.3 words/s - loss: 0.1759
INFO:__main__:process[6] - epoch 15 - train iter 500 - 2396.2 words/s - loss: 0.1833
INFO:__main__:process[0] - epoch 15 - train iter 500 - 2470.9 words/s - loss: 0.1764
INFO:__main__:process[5] - epoch 15 - train iter 500 - 2417.6 words/s - loss: 0.1747
INFO:__main__:process[2]: evaluating epoch 15 on f1 ...
INFO:__main__:process[6]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3] - epoch 15 - train iter 500 - 2354.6 words/s - loss: 0.2103
INFO:__main__:process[0]: evaluating epoch 15 on f1 ...
INFO:__main__:process[5]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3]: evaluating epoch 15 on f1 ...
INFO:__main__:process[9] - epoch 15 - train iter 500 - 2341.0 words/s - loss: 0.1868
INFO:__main__:process[4] - epoch 15 - train iter 500 - 2456.2 words/s - loss: 0.2009
INFO:__main__:process[1] - epoch 15 - train iter 500 - 2403.4 words/s - loss: 0.1703
INFO:__main__:process[8] - epoch 15 - train iter 500 - 2458.3 words/s - loss: 0.1774
INFO:__main__:process[1]: evaluating epoch 15 on f1 ...
INFO:__main__:process[9]: evaluating epoch 15 on f1 ...
INFO:__main__:process[4]: evaluating epoch 15 on f1 ...
INFO:__main__:process[8]: evaluating epoch 15 on f1 ...
INFO:__main__:process[7] - epoch 15 - train iter 500 - 2382.9 words/s - loss: 0.1736
INFO:__main__:process[7]: evaluating epoch 15 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[3] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[1] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[6] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 15 - mAP: 0.249 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 15 on f2 ...
INFO:__main__:process[9]: evaluating epoch 15 on f2 ...
INFO:__main__:process[7]: evaluating epoch 15 on f2 ...
INFO:__main__:process[2]: evaluating epoch 15 on f2 ...
INFO:__main__:process[6]: evaluating epoch 15 on f2 ...
INFO:__main__:process[5]: evaluating epoch 15 on f2 ...
INFO:__main__:process[8]: evaluating epoch 15 on f2 ...
INFO:__main__:process[3]: evaluating epoch 15 on f2 ...
INFO:__main__:process[1]: evaluating epoch 15 on f2 ...
INFO:__main__:removing file tmp/xlmr_enes_f1_9_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_5_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_6_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_4_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_8_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_2_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_3_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_7_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_1_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_0_15.txt
INFO:__main__:process[0]: evaluating epoch 15 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[3] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[7] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 15 - mAP: 0.368 w/ 78 queries
INFO:__main__:process[8]: training epoch 16 ...
INFO:__main__:process[9]: training epoch 16 ...
INFO:__main__:process[2]: training epoch 16 ...
INFO:__main__:process[5]: training epoch 16 ...
INFO:__main__:process[7]: training epoch 16 ...
INFO:__main__:process[1]: training epoch 16 ...
INFO:__main__:process[3]: training epoch 16 ...
INFO:__main__:process[6]: training epoch 16 ...
INFO:__main__:process[4]: training epoch 16 ...
INFO:__main__:removing file tmp/xlmr_enes_f2_7_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_3_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_1_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_4_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_5_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_8_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_2_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_6_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_9_15.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_0_15.txt
INFO:__main__:process[0]: training epoch 16 ...
INFO:__main__:process[2] - epoch 16 - train iter 500 - 2201.3 words/s - loss: 0.1864
INFO:__main__:process[6] - epoch 16 - train iter 500 - 2237.4 words/s - loss: 0.1613
INFO:__main__:process[1] - epoch 16 - train iter 500 - 2306.1 words/s - loss: 0.1504
INFO:__main__:process[2]: evaluating epoch 16 on f1 ...
INFO:__main__:process[6]: evaluating epoch 16 on f1 ...
INFO:__main__:process[5] - epoch 16 - train iter 500 - 2263.7 words/s - loss: 0.1874
INFO:__main__:process[9] - epoch 16 - train iter 500 - 2248.7 words/s - loss: 0.1550
INFO:__main__:process[0] - epoch 16 - train iter 500 - 2199.6 words/s - loss: 0.1730
INFO:__main__:process[3] - epoch 16 - train iter 500 - 2326.2 words/s - loss: 0.1428
INFO:__main__:process[1]: evaluating epoch 16 on f1 ...
INFO:__main__:process[7] - epoch 16 - train iter 500 - 2168.2 words/s - loss: 0.1832
INFO:__main__:process[9]: evaluating epoch 16 on f1 ...
INFO:__main__:process[0]: evaluating epoch 16 on f1 ...
INFO:__main__:process[5]: evaluating epoch 16 on f1 ...
INFO:__main__:process[4] - epoch 16 - train iter 500 - 2295.8 words/s - loss: 0.2046
INFO:__main__:process[3]: evaluating epoch 16 on f1 ...
INFO:__main__:process[7]: evaluating epoch 16 on f1 ...
INFO:__main__:process[4]: evaluating epoch 16 on f1 ...
INFO:__main__:process[8] - epoch 16 - train iter 500 - 2223.2 words/s - loss: 0.1809
INFO:__main__:process[8]: evaluating epoch 16 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[6] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[5] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[7] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 16 - mAP: 0.241 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 16 on f2 ...
INFO:__main__:process[3]: evaluating epoch 16 on f2 ...
INFO:__main__:process[9]: evaluating epoch 16 on f2 ...
INFO:__main__:process[1]: evaluating epoch 16 on f2 ...
INFO:__main__:process[4]: evaluating epoch 16 on f2 ...
INFO:__main__:process[5]: evaluating epoch 16 on f2 ...
INFO:__main__:process[8]: evaluating epoch 16 on f2 ...
INFO:__main__:process[6]: evaluating epoch 16 on f2 ...
INFO:__main__:process[7]: evaluating epoch 16 on f2 ...
INFO:__main__:removing file tmp/xlmr_enes_f1_5_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_6_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_4_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_1_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_0_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_8_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_2_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_7_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_3_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_9_16.txt
INFO:__main__:process[0]: evaluating epoch 16 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[6] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[7] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[5] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 16 - mAP: 0.365 w/ 78 queries
INFO:__main__:process[2]: training epoch 17 ...
INFO:__main__:process[4]: training epoch 17 ...
INFO:__main__:process[5]: training epoch 17 ...
INFO:__main__:process[9]: training epoch 17 ...
INFO:__main__:process[1]: training epoch 17 ...
INFO:__main__:process[8]: training epoch 17 ...
INFO:__main__:process[6]: training epoch 17 ...
INFO:__main__:process[3]: training epoch 17 ...
INFO:__main__:process[7]: training epoch 17 ...
INFO:__main__:removing file tmp/xlmr_enes_f2_2_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_8_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_7_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_3_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_1_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_0_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_6_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_9_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_4_16.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_5_16.txt
INFO:__main__:process[0]: training epoch 17 ...
INFO:__main__:process[6] - epoch 17 - train iter 500 - 2422.6 words/s - loss: 0.1406
INFO:__main__:process[5] - epoch 17 - train iter 500 - 2339.0 words/s - loss: 0.1621
INFO:__main__:process[7] - epoch 17 - train iter 500 - 2453.4 words/s - loss: 0.1621
INFO:__main__:process[6]: evaluating epoch 17 on f1 ...
INFO:__main__:process[7]: evaluating epoch 17 on f1 ...
INFO:__main__:process[5]: evaluating epoch 17 on f1 ...
INFO:__main__:process[2] - epoch 17 - train iter 500 - 2422.6 words/s - loss: 0.1731
INFO:__main__:process[8] - epoch 17 - train iter 500 - 2414.6 words/s - loss: 0.1798
INFO:__main__:process[9] - epoch 17 - train iter 500 - 2351.2 words/s - loss: 0.1615
INFO:__main__:process[2]: evaluating epoch 17 on f1 ...
INFO:__main__:process[8]: evaluating epoch 17 on f1 ...
INFO:__main__:process[1] - epoch 17 - train iter 500 - 2391.8 words/s - loss: 0.1796
INFO:__main__:process[9]: evaluating epoch 17 on f1 ...
INFO:__main__:process[0] - epoch 17 - train iter 500 - 2418.3 words/s - loss: 0.1238
INFO:__main__:process[1]: evaluating epoch 17 on f1 ...
INFO:__main__:process[4] - epoch 17 - train iter 500 - 2438.4 words/s - loss: 0.1736
INFO:__main__:process[0]: evaluating epoch 17 on f1 ...
INFO:__main__:process[4]: evaluating epoch 17 on f1 ...
INFO:__main__:process[3] - epoch 17 - train iter 500 - 2394.3 words/s - loss: 0.1569
INFO:__main__:process[3]: evaluating epoch 17 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[3] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[5] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 17 - mAP: 0.252 w/ 78 queries
INFO:__main__:process[9]: evaluating epoch 17 on f2 ...
INFO:__main__:process[4]: evaluating epoch 17 on f2 ...
INFO:__main__:process[2]: evaluating epoch 17 on f2 ...
INFO:__main__:process[7]: evaluating epoch 17 on f2 ...
INFO:__main__:process[6]: evaluating epoch 17 on f2 ...
INFO:__main__:process[8]: evaluating epoch 17 on f2 ...
INFO:__main__:process[5]: evaluating epoch 17 on f2 ...
INFO:__main__:process[3]: evaluating epoch 17 on f2 ...
INFO:__main__:process[1]: evaluating epoch 17 on f2 ...
INFO:__main__:removing file tmp/xlmr_enes_f1_4_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_2_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_9_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_1_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_0_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_5_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_3_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_8_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_6_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_7_17.txt
INFO:__main__:process[0]: evaluating epoch 17 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[5] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[4] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[1] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 17 - mAP: 0.372 w/ 78 queries
INFO:__main__:process[9]: training epoch 18 ...
INFO:__main__:process[2]: training epoch 18 ...
INFO:__main__:process[7]: training epoch 18 ...
INFO:__main__:process[8]: training epoch 18 ...
INFO:__main__:process[1]: training epoch 18 ...
INFO:__main__:process[4]: training epoch 18 ...
INFO:__main__:process[3]: training epoch 18 ...
INFO:__main__:process[5]: training epoch 18 ...
INFO:__main__:process[6]: training epoch 18 ...
INFO:__main__:removing file tmp/xlmr_enes_f2_4_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_7_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_8_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_0_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_1_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_5_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_2_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_6_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_9_17.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_3_17.txt
INFO:__main__:process[0]: training epoch 18 ...
INFO:__main__:process[6] - epoch 18 - train iter 500 - 2341.4 words/s - loss: 0.1500
INFO:__main__:process[5] - epoch 18 - train iter 500 - 2328.2 words/s - loss: 0.1634
INFO:__main__:process[6]: evaluating epoch 18 on f1 ...
INFO:__main__:process[5]: evaluating epoch 18 on f1 ...
INFO:__main__:process[2] - epoch 18 - train iter 500 - 2244.5 words/s - loss: 0.1610
INFO:__main__:process[0] - epoch 18 - train iter 500 - 2348.4 words/s - loss: 0.1658
INFO:__main__:process[2]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9] - epoch 18 - train iter 500 - 2265.5 words/s - loss: 0.1645
INFO:__main__:process[0]: evaluating epoch 18 on f1 ...
INFO:__main__:process[1] - epoch 18 - train iter 500 - 2376.5 words/s - loss: 0.1578
INFO:__main__:process[8] - epoch 18 - train iter 500 - 2291.0 words/s - loss: 0.1519
INFO:__main__:process[9]: evaluating epoch 18 on f1 ...
INFO:__main__:process[3] - epoch 18 - train iter 500 - 2199.8 words/s - loss: 0.1605
INFO:__main__:process[1]: evaluating epoch 18 on f1 ...
INFO:__main__:process[7] - epoch 18 - train iter 500 - 2322.0 words/s - loss: 0.1421
INFO:__main__:process[8]: evaluating epoch 18 on f1 ...
INFO:__main__:process[4] - epoch 18 - train iter 500 - 2319.5 words/s - loss: 0.1866
INFO:__main__:process[3]: evaluating epoch 18 on f1 ...
INFO:__main__:process[7]: evaluating epoch 18 on f1 ...
INFO:__main__:process[4]: evaluating epoch 18 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[9] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[7] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 18 - mAP: 0.232 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 18 on f2 ...
INFO:__main__:process[6]: evaluating epoch 18 on f2 ...
INFO:__main__:process[2]: evaluating epoch 18 on f2 ...
INFO:__main__:process[9]: evaluating epoch 18 on f2 ...
INFO:__main__:process[5]: evaluating epoch 18 on f2 ...
INFO:__main__:process[8]: evaluating epoch 18 on f2 ...
INFO:__main__:process[1]: evaluating epoch 18 on f2 ...
INFO:__main__:process[7]: evaluating epoch 18 on f2 ...
INFO:__main__:process[3]: evaluating epoch 18 on f2 ...
INFO:__main__:removing file tmp/xlmr_enes_f1_7_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_6_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_5_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_9_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_2_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_8_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_1_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_0_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_3_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_4_18.txt
INFO:__main__:process[0]: evaluating epoch 18 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[5] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[7] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 18 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[6]: training epoch 19 ...
INFO:__main__:process[3]: training epoch 19 ...
INFO:__main__:process[9]: training epoch 19 ...
INFO:__main__:process[1]: training epoch 19 ...
INFO:__main__:process[4]: training epoch 19 ...
INFO:__main__:process[2]: training epoch 19 ...
INFO:__main__:process[5]: training epoch 19 ...
INFO:__main__:process[8]: training epoch 19 ...
INFO:__main__:process[7]: training epoch 19 ...
INFO:__main__:removing file tmp/xlmr_enes_f2_7_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_5_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_1_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_6_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_8_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_9_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_3_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_4_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_0_18.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_2_18.txt
INFO:__main__:process[0]: training epoch 19 ...
INFO:__main__:process[2] - epoch 19 - train iter 500 - 2374.5 words/s - loss: 0.1481
INFO:__main__:process[3] - epoch 19 - train iter 500 - 2373.0 words/s - loss: 0.1576
INFO:__main__:process[5] - epoch 19 - train iter 500 - 2346.6 words/s - loss: 0.1888
INFO:__main__:process[2]: evaluating epoch 19 on f1 ...
INFO:__main__:process[5]: evaluating epoch 19 on f1 ...
INFO:__main__:process[3]: evaluating epoch 19 on f1 ...
INFO:__main__:process[6] - epoch 19 - train iter 500 - 2375.4 words/s - loss: 0.1495
INFO:__main__:process[0] - epoch 19 - train iter 500 - 2345.2 words/s - loss: 0.1656
INFO:__main__:process[6]: evaluating epoch 19 on f1 ...
INFO:__main__:process[8] - epoch 19 - train iter 500 - 2419.7 words/s - loss: 0.1539
INFO:__main__:process[9] - epoch 19 - train iter 500 - 2457.2 words/s - loss: 0.1333
INFO:__main__:process[0]: evaluating epoch 19 on f1 ...
INFO:__main__:process[8]: evaluating epoch 19 on f1 ...
INFO:__main__:process[9]: evaluating epoch 19 on f1 ...
INFO:__main__:process[7] - epoch 19 - train iter 500 - 2393.8 words/s - loss: 0.1721
INFO:__main__:process[7]: evaluating epoch 19 on f1 ...
INFO:__main__:process[1] - epoch 19 - train iter 500 - 2429.4 words/s - loss: 0.1523
INFO:__main__:process[1]: evaluating epoch 19 on f1 ...
INFO:__main__:process[4] - epoch 19 - train iter 500 - 2368.2 words/s - loss: 0.1523
INFO:__main__:process[4]: evaluating epoch 19 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[9] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[7] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[4] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 19 - mAP: 0.235 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 19 on f2 ...
INFO:__main__:process[2]: evaluating epoch 19 on f2 ...
INFO:__main__:process[3]: evaluating epoch 19 on f2 ...
INFO:__main__:process[1]: evaluating epoch 19 on f2 ...
INFO:__main__:process[8]: evaluating epoch 19 on f2 ...
INFO:__main__:process[4]: evaluating epoch 19 on f2 ...
INFO:__main__:process[6]: evaluating epoch 19 on f2 ...
INFO:__main__:process[7]: evaluating epoch 19 on f2 ...
INFO:__main__:process[9]: evaluating epoch 19 on f2 ...
INFO:__main__:removing file tmp/xlmr_enes_f1_4_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_1_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_3_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_5_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_9_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_6_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_7_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_2_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_0_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f1_8_19.txt
INFO:__main__:process[0]: evaluating epoch 19 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[2] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[8] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[4] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 19 - mAP: 0.354 w/ 78 queries
INFO:__main__:removing file tmp/xlmr_enes_f2_1_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_0_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_9_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_8_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_5_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_2_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_3_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_6_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_4_19.txt
INFO:__main__:removing file tmp/xlmr_enes_f2_7_19.txt
INFO:__main__:[0.24945082419696582, 0.24074959178610172, 0.251656601937904, 0.2322899414174898, 0.23503664963850895]
INFO:__main__:[0.36794802767724705, 0.36458692062152676, 0.3718478451301425, 0.34986319444785574, 0.3535537635356574]
INFO:__main__:0.251656601937904
INFO:__main__:0.3718478451301425
INFO:__main__:best MAP: 0.312
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=True, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-cased', model_type='mbert', num_epochs=20, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 606727.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 605798.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 605238.67it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 558481.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 521148.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 569043.25it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 522654.70it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 563129.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 468459.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 523111.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s] 22%|██▏       | 35/156 [00:00<00:00, 300.38it/s] 22%|██▏       | 35/156 [00:00<00:00, 284.48it/s]100%|██████████| 156/156 [00:00<00:00, 1267.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1204.13it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s] 22%|██▏       | 35/156 [00:00<00:00, 286.77it/s]100%|██████████| 156/156 [00:00<00:00, 18184.41it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
100%|██████████| 156/156 [00:00<00:00, 1216.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 12265.43it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18562.55it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 22%|██▏       | 35/156 [00:00<00:00, 238.84it/s]100%|██████████| 156/156 [00:00<00:00, 1019.24it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s] 22%|██▏       | 35/156 [00:00<00:00, 302.76it/s]100%|██████████| 156/156 [00:00<00:00, 1279.98it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18228.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 12780.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 22%|██▏       | 35/156 [00:00<00:00, 295.11it/s] 22%|██▏       | 35/156 [00:00<00:00, 244.66it/s]100%|██████████| 156/156 [00:00<00:00, 1248.24it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1044.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18339.87it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
100%|██████████| 156/156 [00:00<00:00, 18384.70it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s] 22%|██▏       | 35/156 [00:00<00:00, 276.57it/s] 22%|██▏       | 35/156 [00:00<00:00, 275.43it/s]100%|██████████| 156/156 [00:00<00:00, 1172.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1167.77it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18046.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18295.25it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/cased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/cased
  0%|          | 0/156 [00:00<?, ?it/s] 22%|██▏       | 35/156 [00:00<00:00, 265.69it/s]100%|██████████| 156/156 [00:00<00:00, 1123.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28964
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16474.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 28794
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 4733.3 words/s - loss: 0.4766
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4379.8 words/s - loss: 0.4787
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 4312.4 words/s - loss: 0.4805
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4249.2 words/s - loss: 0.4609
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 4196.7 words/s - loss: 0.4771
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 4011.6 words/s - loss: 0.4749
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3744.8 words/s - loss: 0.4689
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3678.2 words/s - loss: 0.4917
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3503.2 words/s - loss: 0.4814
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3269.8 words/s - loss: 0.4783
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 5007.3 words/s - loss: 0.2585
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4418.9 words/s - loss: 0.2450
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4326.0 words/s - loss: 0.2659
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 4146.8 words/s - loss: 0.2740
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 4694.4 words/s - loss: 0.2531
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3504.9 words/s - loss: 0.2636
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4709.1 words/s - loss: 0.2512
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3650.7 words/s - loss: 0.2903
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 4088.2 words/s - loss: 0.2418
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3551.2 words/s - loss: 0.2092
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4213.8 words/s - loss: 0.2123
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3522.6 words/s - loss: 0.2675
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3867.1 words/s - loss: 0.1968
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4410.4 words/s - loss: 0.1939
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3503.1 words/s - loss: 0.2165
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3916.1 words/s - loss: 0.2521
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3600.4 words/s - loss: 0.2124
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3894.1 words/s - loss: 0.2401
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3651.6 words/s - loss: 0.2258
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3739.9 words/s - loss: 0.2336
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4668.2 words/s - loss: 0.2083
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3796.0 words/s - loss: 0.2262
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3854.2 words/s - loss: 0.2205
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 4263.3 words/s - loss: 0.1938
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 4104.4 words/s - loss: 0.2117
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4110.8 words/s - loss: 0.1942
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4015.6 words/s - loss: 0.1998
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 4167.4 words/s - loss: 0.2255
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4208.7 words/s - loss: 0.2247
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 4223.4 words/s - loss: 0.2182
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 4368.9 words/s - loss: 0.2015
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4825.4 words/s - loss: 0.2145
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4254.3 words/s - loss: 0.2206
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 4013.0 words/s - loss: 0.1916
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3982.4 words/s - loss: 0.2138
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4218.4 words/s - loss: 0.2138
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3926.9 words/s - loss: 0.2276
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3469.4 words/s - loss: 0.1858
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3611.1 words/s - loss: 0.2042
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 4266.5 words/s - loss: 0.1890
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4069.6 words/s - loss: 0.1631
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 4522.3 words/s - loss: 0.1782
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 4059.7 words/s - loss: 0.1743
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4723.6 words/s - loss: 0.2146
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4104.0 words/s - loss: 0.1837
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3986.0 words/s - loss: 0.2022
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 4084.6 words/s - loss: 0.1928
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3898.8 words/s - loss: 0.1879
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3982.7 words/s - loss: 0.1935
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3690.2 words/s - loss: 0.1913
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 4721.5 words/s - loss: 0.1931
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 4201.7 words/s - loss: 0.1875
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4739.1 words/s - loss: 0.2069
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4594.5 words/s - loss: 0.1480
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3750.2 words/s - loss: 0.1892
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4041.1 words/s - loss: 0.1872
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3938.5 words/s - loss: 0.1923
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3623.1 words/s - loss: 0.1865
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 4187.3 words/s - loss: 0.1763
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3904.5 words/s - loss: 0.1997
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4090.3 words/s - loss: 0.1569
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3790.7 words/s - loss: 0.1612
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 4221.7 words/s - loss: 0.1488
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 4407.6 words/s - loss: 0.1762
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4066.6 words/s - loss: 0.1592
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3882.3 words/s - loss: 0.1727
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3774.6 words/s - loss: 0.1487
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3301.2 words/s - loss: 0.1689
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3568.6 words/s - loss: 0.1725
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3484.4 words/s - loss: 0.1599
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 4401.1 words/s - loss: 0.1860
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 4448.2 words/s - loss: 0.1426
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4745.7 words/s - loss: 0.1527
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4292.6 words/s - loss: 0.1727
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3395.9 words/s - loss: 0.1698
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 4782.5 words/s - loss: 0.1598
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 4085.8 words/s - loss: 0.1596
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3925.0 words/s - loss: 0.1828
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 4871.3 words/s - loss: 0.1607
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 4391.0 words/s - loss: 0.1683
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3543.6 words/s - loss: 0.1518
INFO:__main__:process[8]: training epoch 10 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3469.5 words/s - loss: 0.1473
INFO:__main__:process[0]: training epoch 10 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 4739.4 words/s - loss: 0.1596
INFO:__main__:process[2]: training epoch 10 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4170.8 words/s - loss: 0.1844
INFO:__main__:process[4]: training epoch 10 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4166.1 words/s - loss: 0.1793
INFO:__main__:process[1]: training epoch 10 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4311.9 words/s - loss: 0.1919
INFO:__main__:process[3]: training epoch 10 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 4152.7 words/s - loss: 0.1525
INFO:__main__:process[5]: training epoch 10 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 4234.5 words/s - loss: 0.1849
INFO:__main__:process[7]: training epoch 10 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 4269.0 words/s - loss: 0.1535
INFO:__main__:process[6]: training epoch 10 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 4118.6 words/s - loss: 0.1569
INFO:__main__:process[9]: training epoch 10 ...
INFO:__main__:process[0] - epoch 10 - train iter 500 - 4116.8 words/s - loss: 0.1768
INFO:__main__:process[0]: training epoch 11 ...
INFO:__main__:process[2] - epoch 10 - train iter 500 - 4585.0 words/s - loss: 0.1670
INFO:__main__:process[2]: training epoch 11 ...
INFO:__main__:process[8] - epoch 10 - train iter 500 - 3864.9 words/s - loss: 0.1513
INFO:__main__:process[8]: training epoch 11 ...
INFO:__main__:process[4] - epoch 10 - train iter 500 - 4404.7 words/s - loss: 0.1850
INFO:__main__:process[4]: training epoch 11 ...
INFO:__main__:process[1] - epoch 10 - train iter 500 - 4617.8 words/s - loss: 0.1755
INFO:__main__:process[1]: training epoch 11 ...
INFO:__main__:process[6] - epoch 10 - train iter 500 - 4819.1 words/s - loss: 0.1604
INFO:__main__:process[6]: training epoch 11 ...
INFO:__main__:process[3] - epoch 10 - train iter 500 - 4337.6 words/s - loss: 0.1563
INFO:__main__:process[3]: training epoch 11 ...
INFO:__main__:process[5] - epoch 10 - train iter 500 - 3938.2 words/s - loss: 0.1590
INFO:__main__:process[5]: training epoch 11 ...
INFO:__main__:process[9] - epoch 10 - train iter 500 - 5069.5 words/s - loss: 0.1764
INFO:__main__:process[9]: training epoch 11 ...
INFO:__main__:process[7] - epoch 10 - train iter 500 - 3530.6 words/s - loss: 0.1682
INFO:__main__:process[7]: training epoch 11 ...
INFO:__main__:process[0] - epoch 11 - train iter 500 - 4106.9 words/s - loss: 0.1728
INFO:__main__:process[0]: training epoch 12 ...
INFO:__main__:process[4] - epoch 11 - train iter 500 - 4279.3 words/s - loss: 0.1882
INFO:__main__:process[4]: training epoch 12 ...
INFO:__main__:process[2] - epoch 11 - train iter 500 - 3965.4 words/s - loss: 0.1820
INFO:__main__:process[2]: training epoch 12 ...
INFO:__main__:process[8] - epoch 11 - train iter 500 - 3787.8 words/s - loss: 0.1464
INFO:__main__:process[8]: training epoch 12 ...
INFO:__main__:process[1] - epoch 11 - train iter 500 - 3921.1 words/s - loss: 0.1491
INFO:__main__:process[1]: training epoch 12 ...
INFO:__main__:process[6] - epoch 11 - train iter 500 - 4251.0 words/s - loss: 0.1395
INFO:__main__:process[6]: training epoch 12 ...
INFO:__main__:process[3] - epoch 11 - train iter 500 - 3785.9 words/s - loss: 0.1481
INFO:__main__:process[3]: training epoch 12 ...
INFO:__main__:process[9] - epoch 11 - train iter 500 - 4468.0 words/s - loss: 0.1857
INFO:__main__:process[9]: training epoch 12 ...
INFO:__main__:process[5] - epoch 11 - train iter 500 - 3980.7 words/s - loss: 0.1815
INFO:__main__:process[5]: training epoch 12 ...
INFO:__main__:process[7] - epoch 11 - train iter 500 - 3676.9 words/s - loss: 0.1556
INFO:__main__:process[7]: training epoch 12 ...
INFO:__main__:process[4] - epoch 12 - train iter 500 - 4445.7 words/s - loss: 0.1645
INFO:__main__:process[4]: training epoch 13 ...
INFO:__main__:process[0] - epoch 12 - train iter 500 - 4257.2 words/s - loss: 0.1389
INFO:__main__:process[0]: training epoch 13 ...
INFO:__main__:process[2] - epoch 12 - train iter 500 - 4321.4 words/s - loss: 0.1684
INFO:__main__:process[2]: training epoch 13 ...
INFO:__main__:process[1] - epoch 12 - train iter 500 - 4517.2 words/s - loss: 0.1303
INFO:__main__:process[1]: training epoch 13 ...
INFO:__main__:process[8] - epoch 12 - train iter 500 - 3581.4 words/s - loss: 0.1627
INFO:__main__:process[8]: training epoch 13 ...
INFO:__main__:process[6] - epoch 12 - train iter 500 - 3964.3 words/s - loss: 0.1731
INFO:__main__:process[6]: training epoch 13 ...
INFO:__main__:process[5] - epoch 12 - train iter 500 - 4343.7 words/s - loss: 0.1874
INFO:__main__:process[5]: training epoch 13 ...
INFO:__main__:process[9] - epoch 12 - train iter 500 - 4240.6 words/s - loss: 0.1617
INFO:__main__:process[9]: training epoch 13 ...
INFO:__main__:process[3] - epoch 12 - train iter 500 - 3640.1 words/s - loss: 0.1392
INFO:__main__:process[3]: training epoch 13 ...
INFO:__main__:process[7] - epoch 12 - train iter 500 - 3849.3 words/s - loss: 0.1540
INFO:__main__:process[7]: training epoch 13 ...
INFO:__main__:process[2] - epoch 13 - train iter 500 - 4714.9 words/s - loss: 0.1706
INFO:__main__:process[2]: training epoch 14 ...
INFO:__main__:process[4] - epoch 13 - train iter 500 - 3538.9 words/s - loss: 0.1545
INFO:__main__:process[4]: training epoch 14 ...
INFO:__main__:process[1] - epoch 13 - train iter 500 - 4268.0 words/s - loss: 0.1362
INFO:__main__:process[1]: training epoch 14 ...
INFO:__main__:process[0] - epoch 13 - train iter 500 - 3436.5 words/s - loss: 0.1805
INFO:__main__:process[0]: training epoch 14 ...
INFO:__main__:process[8] - epoch 13 - train iter 500 - 3579.5 words/s - loss: 0.1684
INFO:__main__:process[8]: training epoch 14 ...
INFO:__main__:process[5] - epoch 13 - train iter 500 - 4344.0 words/s - loss: 0.1436
INFO:__main__:process[5]: training epoch 14 ...
INFO:__main__:process[6] - epoch 13 - train iter 500 - 3919.4 words/s - loss: 0.1610
INFO:__main__:process[6]: training epoch 14 ...
INFO:__main__:process[9] - epoch 13 - train iter 500 - 4086.8 words/s - loss: 0.1578
INFO:__main__:process[9]: training epoch 14 ...
INFO:__main__:process[3] - epoch 13 - train iter 500 - 3635.6 words/s - loss: 0.1403
INFO:__main__:process[3]: training epoch 14 ...
INFO:__main__:process[7] - epoch 13 - train iter 500 - 4376.6 words/s - loss: 0.1518
INFO:__main__:process[7]: training epoch 14 ...
INFO:__main__:process[2] - epoch 14 - train iter 500 - 4385.5 words/s - loss: 0.1403
INFO:__main__:process[2]: training epoch 15 ...
INFO:__main__:process[4] - epoch 14 - train iter 500 - 3826.0 words/s - loss: 0.1563
INFO:__main__:process[4]: training epoch 15 ...
INFO:__main__:process[1] - epoch 14 - train iter 500 - 3769.4 words/s - loss: 0.1572
INFO:__main__:process[1]: training epoch 15 ...
INFO:__main__:process[5] - epoch 14 - train iter 500 - 4266.7 words/s - loss: 0.1378
INFO:__main__:process[5]: training epoch 15 ...
INFO:__main__:process[6] - epoch 14 - train iter 500 - 4359.5 words/s - loss: 0.1575
INFO:__main__:process[6]: training epoch 15 ...
INFO:__main__:process[8] - epoch 14 - train iter 500 - 3703.8 words/s - loss: 0.1434
INFO:__main__:process[8]: training epoch 15 ...
INFO:__main__:process[0] - epoch 14 - train iter 500 - 3353.0 words/s - loss: 0.1348
INFO:__main__:process[0]: training epoch 15 ...
INFO:__main__:process[9] - epoch 14 - train iter 500 - 3733.3 words/s - loss: 0.1665
INFO:__main__:process[9]: training epoch 15 ...
INFO:__main__:process[7] - epoch 14 - train iter 500 - 4847.8 words/s - loss: 0.1527
INFO:__main__:process[7]: training epoch 15 ...
INFO:__main__:process[3] - epoch 14 - train iter 500 - 3923.9 words/s - loss: 0.2027
INFO:__main__:process[3]: training epoch 15 ...
INFO:__main__:process[2] - epoch 15 - train iter 500 - 4000.5 words/s - loss: 0.1122
INFO:__main__:process[2]: evaluating epoch 15 on f1 ...
INFO:__main__:process[4] - epoch 15 - train iter 500 - 3947.6 words/s - loss: 0.1849
INFO:__main__:process[4]: evaluating epoch 15 on f1 ...
INFO:__main__:process[5] - epoch 15 - train iter 500 - 4261.2 words/s - loss: 0.1538
INFO:__main__:process[5]: evaluating epoch 15 on f1 ...
INFO:__main__:process[1] - epoch 15 - train iter 500 - 3808.8 words/s - loss: 0.1573
INFO:__main__:process[1]: evaluating epoch 15 on f1 ...
INFO:__main__:process[6] - epoch 15 - train iter 500 - 4078.7 words/s - loss: 0.1324
INFO:__main__:process[6]: evaluating epoch 15 on f1 ...
INFO:__main__:process[8] - epoch 15 - train iter 500 - 4186.2 words/s - loss: 0.1399
INFO:__main__:process[8]: evaluating epoch 15 on f1 ...
INFO:__main__:process[0] - epoch 15 - train iter 500 - 3903.0 words/s - loss: 0.1642
INFO:__main__:process[0]: evaluating epoch 15 on f1 ...
INFO:__main__:process[9] - epoch 15 - train iter 500 - 4201.1 words/s - loss: 0.1629
INFO:__main__:process[9]: evaluating epoch 15 on f1 ...
INFO:__main__:process[3] - epoch 15 - train iter 500 - 3747.2 words/s - loss: 0.1619
INFO:__main__:process[3]: evaluating epoch 15 on f1 ...
INFO:__main__:process[7] - epoch 15 - train iter 500 - 3243.3 words/s - loss: 0.1466
INFO:__main__:process[7]: evaluating epoch 15 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[7] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[2] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 15 - mAP: 0.424 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 15 on f2 ...
INFO:__main__:process[4]: evaluating epoch 15 on f2 ...
INFO:__main__:process[8]: evaluating epoch 15 on f2 ...
INFO:__main__:process[1]: evaluating epoch 15 on f2 ...
INFO:__main__:process[3]: evaluating epoch 15 on f2 ...
INFO:__main__:process[9]: evaluating epoch 15 on f2 ...
INFO:__main__:process[2]: evaluating epoch 15 on f2 ...
INFO:__main__:process[7]: evaluating epoch 15 on f2 ...
INFO:__main__:process[6]: evaluating epoch 15 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_9_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_15.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_15.txt
INFO:__main__:process[0]: evaluating epoch 15 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[5] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[0] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[7] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[9] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 15 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[2]: training epoch 16 ...
INFO:__main__:process[4]: training epoch 16 ...
INFO:__main__:process[1]: training epoch 16 ...
INFO:__main__:process[5]: training epoch 16 ...
INFO:__main__:process[8]: training epoch 16 ...
INFO:__main__:process[9]: training epoch 16 ...
INFO:__main__:process[6]: training epoch 16 ...
INFO:__main__:process[3]: training epoch 16 ...
INFO:__main__:process[7]: training epoch 16 ...
INFO:__main__:removing file tmp/mbert_enes_f2_8_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_15.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_15.txt
INFO:__main__:process[0]: training epoch 16 ...
INFO:__main__:process[0] - epoch 16 - train iter 500 - 4729.3 words/s - loss: 0.1375
INFO:__main__:process[0]: evaluating epoch 16 on f1 ...
INFO:__main__:process[4] - epoch 16 - train iter 500 - 4741.8 words/s - loss: 0.1438
INFO:__main__:process[4]: evaluating epoch 16 on f1 ...
INFO:__main__:process[8] - epoch 16 - train iter 500 - 4425.3 words/s - loss: 0.1432
INFO:__main__:process[8]: evaluating epoch 16 on f1 ...
INFO:__main__:process[2] - epoch 16 - train iter 500 - 4330.2 words/s - loss: 0.1690
INFO:__main__:process[2]: evaluating epoch 16 on f1 ...
INFO:__main__:process[6] - epoch 16 - train iter 500 - 4110.1 words/s - loss: 0.1208
INFO:__main__:process[6]: evaluating epoch 16 on f1 ...
INFO:__main__:process[9] - epoch 16 - train iter 500 - 4135.8 words/s - loss: 0.1294
INFO:__main__:process[9]: evaluating epoch 16 on f1 ...
INFO:__main__:process[1] - epoch 16 - train iter 500 - 4186.6 words/s - loss: 0.1282
INFO:__main__:process[1]: evaluating epoch 16 on f1 ...
INFO:__main__:process[5] - epoch 16 - train iter 500 - 4009.9 words/s - loss: 0.1395
INFO:__main__:process[5]: evaluating epoch 16 on f1 ...
INFO:__main__:process[3] - epoch 16 - train iter 500 - 3876.0 words/s - loss: 0.1220
INFO:__main__:process[3]: evaluating epoch 16 on f1 ...
INFO:__main__:process[7] - epoch 16 - train iter 500 - 3353.7 words/s - loss: 0.1523
INFO:__main__:process[7]: evaluating epoch 16 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[7] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[9] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 16 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 16 on f2 ...
INFO:__main__:process[6]: evaluating epoch 16 on f2 ...
INFO:__main__:process[2]: evaluating epoch 16 on f2 ...
INFO:__main__:process[1]: evaluating epoch 16 on f2 ...
INFO:__main__:process[3]: evaluating epoch 16 on f2 ...
INFO:__main__:process[4]: evaluating epoch 16 on f2 ...
INFO:__main__:process[8]: evaluating epoch 16 on f2 ...
INFO:__main__:process[9]: evaluating epoch 16 on f2 ...
INFO:__main__:process[7]: evaluating epoch 16 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_16.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_16.txt
INFO:__main__:process[0]: evaluating epoch 16 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[9] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[8] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 16 - mAP: 0.514 w/ 78 queries
INFO:__main__:process[7]: training epoch 17 ...
INFO:__main__:process[3]: training epoch 17 ...
INFO:__main__:process[4]: training epoch 17 ...
INFO:__main__:process[1]: training epoch 17 ...
INFO:__main__:process[6]: training epoch 17 ...
INFO:__main__:process[9]: training epoch 17 ...
INFO:__main__:process[5]: training epoch 17 ...
INFO:__main__:process[2]: training epoch 17 ...
INFO:__main__:removing file tmp/mbert_enes_f2_8_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_16.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_16.txt
INFO:__main__:process[8]: training epoch 17 ...
INFO:__main__:process[0]: training epoch 17 ...
INFO:__main__:process[2] - epoch 17 - train iter 500 - 5323.3 words/s - loss: 0.1375
INFO:__main__:process[2]: evaluating epoch 17 on f1 ...
INFO:__main__:process[6] - epoch 17 - train iter 500 - 4620.5 words/s - loss: 0.1368
INFO:__main__:process[6]: evaluating epoch 17 on f1 ...
INFO:__main__:process[8] - epoch 17 - train iter 500 - 4428.9 words/s - loss: 0.1502
INFO:__main__:process[8]: evaluating epoch 17 on f1 ...
INFO:__main__:process[0] - epoch 17 - train iter 500 - 4568.8 words/s - loss: 0.1312
INFO:__main__:process[0]: evaluating epoch 17 on f1 ...
INFO:__main__:process[1] - epoch 17 - train iter 500 - 4195.4 words/s - loss: 0.1631
INFO:__main__:process[1]: evaluating epoch 17 on f1 ...
INFO:__main__:process[3] - epoch 17 - train iter 500 - 4201.0 words/s - loss: 0.1534
INFO:__main__:process[3]: evaluating epoch 17 on f1 ...
INFO:__main__:process[5] - epoch 17 - train iter 500 - 4087.5 words/s - loss: 0.1325
INFO:__main__:process[5]: evaluating epoch 17 on f1 ...
INFO:__main__:process[4] - epoch 17 - train iter 500 - 3991.0 words/s - loss: 0.1460
INFO:__main__:process[4]: evaluating epoch 17 on f1 ...
INFO:__main__:process[7] - epoch 17 - train iter 500 - 3940.1 words/s - loss: 0.1358
INFO:__main__:process[7]: evaluating epoch 17 on f1 ...
INFO:__main__:process[9] - epoch 17 - train iter 500 - 3527.3 words/s - loss: 0.1385
INFO:__main__:process[9]: evaluating epoch 17 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[9] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[7] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 17 - mAP: 0.458 w/ 78 queries
INFO:__main__:process[3]: evaluating epoch 17 on f2 ...
INFO:__main__:process[4]: evaluating epoch 17 on f2 ...
INFO:__main__:process[5]: evaluating epoch 17 on f2 ...
INFO:__main__:process[6]: evaluating epoch 17 on f2 ...
INFO:__main__:process[1]: evaluating epoch 17 on f2 ...
INFO:__main__:process[7]: evaluating epoch 17 on f2 ...
INFO:__main__:process[2]: evaluating epoch 17 on f2 ...
INFO:__main__:process[9]: evaluating epoch 17 on f2 ...
INFO:__main__:process[8]: evaluating epoch 17 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_0_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_17.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_17.txt
INFO:__main__:process[0]: evaluating epoch 17 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[5] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[1] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 17 - mAP: 0.523 w/ 78 queries
INFO:__main__:process[4]: training epoch 18 ...
INFO:__main__:process[2]: training epoch 18 ...
INFO:__main__:process[1]: training epoch 18 ...
INFO:__main__:process[5]: training epoch 18 ...
INFO:__main__:process[7]: training epoch 18 ...
INFO:__main__:process[6]: training epoch 18 ...
INFO:__main__:process[9]: training epoch 18 ...
INFO:__main__:process[3]: training epoch 18 ...
INFO:__main__:process[8]: training epoch 18 ...
INFO:__main__:removing file tmp/mbert_enes_f2_7_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_17.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_17.txt
INFO:__main__:process[0]: training epoch 18 ...
INFO:__main__:process[0] - epoch 18 - train iter 500 - 5133.3 words/s - loss: 0.1559
INFO:__main__:process[0]: evaluating epoch 18 on f1 ...
INFO:__main__:process[7] - epoch 18 - train iter 500 - 5010.4 words/s - loss: 0.1195
INFO:__main__:process[7]: evaluating epoch 18 on f1 ...
INFO:__main__:process[3] - epoch 18 - train iter 500 - 4400.3 words/s - loss: 0.1366
INFO:__main__:process[3]: evaluating epoch 18 on f1 ...
INFO:__main__:process[8] - epoch 18 - train iter 500 - 4223.8 words/s - loss: 0.1389
INFO:__main__:process[8]: evaluating epoch 18 on f1 ...
INFO:__main__:process[6] - epoch 18 - train iter 500 - 4205.0 words/s - loss: 0.1445
INFO:__main__:process[6]: evaluating epoch 18 on f1 ...
INFO:__main__:process[9] - epoch 18 - train iter 500 - 4069.6 words/s - loss: 0.1442
INFO:__main__:process[9]: evaluating epoch 18 on f1 ...
INFO:__main__:process[5] - epoch 18 - train iter 500 - 4002.4 words/s - loss: 0.2018
INFO:__main__:process[5]: evaluating epoch 18 on f1 ...
INFO:__main__:process[2] - epoch 18 - train iter 500 - 3828.6 words/s - loss: 0.1614
INFO:__main__:process[2]: evaluating epoch 18 on f1 ...
INFO:__main__:process[1] - epoch 18 - train iter 500 - 3885.6 words/s - loss: 0.1459
INFO:__main__:process[1]: evaluating epoch 18 on f1 ...
INFO:__main__:process[4] - epoch 18 - train iter 500 - 3820.9 words/s - loss: 0.1316
INFO:__main__:process[4]: evaluating epoch 18 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[4] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[8] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 18 - mAP: 0.444 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 18 on f2 ...
INFO:__main__:process[9]: evaluating epoch 18 on f2 ...
INFO:__main__:process[4]: evaluating epoch 18 on f2 ...
INFO:__main__:process[2]: evaluating epoch 18 on f2 ...
INFO:__main__:process[7]: evaluating epoch 18 on f2 ...
INFO:__main__:process[6]: evaluating epoch 18 on f2 ...
INFO:__main__:process[3]: evaluating epoch 18 on f2 ...
INFO:__main__:process[8]: evaluating epoch 18 on f2 ...
INFO:__main__:process[1]: evaluating epoch 18 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_0_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_18.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_18.txt
INFO:__main__:process[0]: evaluating epoch 18 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[8] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[3] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 18 - mAP: 0.520 w/ 78 queries
INFO:__main__:process[7]: training epoch 19 ...
INFO:__main__:process[2]: training epoch 19 ...
INFO:__main__:process[4]: training epoch 19 ...
INFO:__main__:process[9]: training epoch 19 ...
INFO:__main__:process[6]: training epoch 19 ...
INFO:__main__:process[5]: training epoch 19 ...
INFO:__main__:process[1]: training epoch 19 ...
INFO:__main__:process[3]: training epoch 19 ...
INFO:__main__:process[8]: training epoch 19 ...
INFO:__main__:removing file tmp/mbert_enes_f2_8_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_18.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_18.txt
INFO:__main__:process[0]: training epoch 19 ...
INFO:__main__:process[1] - epoch 19 - train iter 500 - 4607.1 words/s - loss: 0.1233
INFO:__main__:process[1]: evaluating epoch 19 on f1 ...
INFO:__main__:process[2] - epoch 19 - train iter 500 - 4303.9 words/s - loss: 0.1349
INFO:__main__:process[2]: evaluating epoch 19 on f1 ...
INFO:__main__:process[9] - epoch 19 - train iter 500 - 4484.8 words/s - loss: 0.1276
INFO:__main__:process[9]: evaluating epoch 19 on f1 ...
INFO:__main__:process[7] - epoch 19 - train iter 500 - 4223.9 words/s - loss: 0.1554
INFO:__main__:process[7]: evaluating epoch 19 on f1 ...
INFO:__main__:process[6] - epoch 19 - train iter 500 - 4054.7 words/s - loss: 0.1357
INFO:__main__:process[6]: evaluating epoch 19 on f1 ...
INFO:__main__:process[3] - epoch 19 - train iter 500 - 3950.4 words/s - loss: 0.1497
INFO:__main__:process[3]: evaluating epoch 19 on f1 ...
INFO:__main__:process[5] - epoch 19 - train iter 500 - 3849.1 words/s - loss: 0.1354
INFO:__main__:process[5]: evaluating epoch 19 on f1 ...
INFO:__main__:process[8] - epoch 19 - train iter 500 - 3751.0 words/s - loss: 0.1775
INFO:__main__:process[8]: evaluating epoch 19 on f1 ...
INFO:__main__:process[4] - epoch 19 - train iter 500 - 3690.1 words/s - loss: 0.1467
INFO:__main__:process[4]: evaluating epoch 19 on f1 ...
INFO:__main__:process[0] - epoch 19 - train iter 500 - 3659.8 words/s - loss: 0.1371
INFO:__main__:process[0]: evaluating epoch 19 on f1 ...
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[5] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:f1 set during evaluation: 28970/28964
INFO:__main__:process[2] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 19 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 19 on f2 ...
INFO:__main__:process[1]: evaluating epoch 19 on f2 ...
INFO:__main__:process[8]: evaluating epoch 19 on f2 ...
INFO:__main__:process[2]: evaluating epoch 19 on f2 ...
INFO:__main__:process[5]: evaluating epoch 19 on f2 ...
INFO:__main__:process[6]: evaluating epoch 19 on f2 ...
INFO:__main__:process[9]: evaluating epoch 19 on f2 ...
INFO:__main__:process[7]: evaluating epoch 19 on f2 ...
INFO:__main__:process[3]: evaluating epoch 19 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_19.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_19.txt
INFO:__main__:process[0]: evaluating epoch 19 on f2 ...
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[4] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:f2 set during evaluation: 28800/28794
INFO:__main__:process[6] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 19 - mAP: 0.526 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_6_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_19.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_19.txt
INFO:__main__:[0.4242168815426818, 0.4471487058137487, 0.458227052217381, 0.44390536834048544, 0.45205413422707263]
INFO:__main__:[0.4823641718752768, 0.5138665389645304, 0.5227377348342369, 0.5202034922390415, 0.5255469288188641]
INFO:__main__:0.45205413422707263
INFO:__main__:0.5227377348342369
INFO:__main__:best MAP: 0.487

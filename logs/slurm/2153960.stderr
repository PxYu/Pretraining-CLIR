INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 794345.67it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 182.62it/s]100%|██████████| 185/185 [00:00<00:00, 1291.20it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23579.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 809773.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 237.35it/s]100%|██████████| 185/185 [00:00<00:00, 1653.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23744.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 718966.03it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 718448.78it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 703836.76it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 708784.64it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 748528.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 691855.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 704593.47it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 739397.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 14%|█▎        | 25/185 [00:00<00:00, 223.21it/s]100%|██████████| 185/185 [00:00<00:00, 1559.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22652.72it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 206.76it/s]100%|██████████| 185/185 [00:00<00:00, 1447.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21207.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 200.84it/s] 14%|█▎        | 25/185 [00:00<00:00, 184.13it/s]100%|██████████| 185/185 [00:00<00:00, 1412.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 178.38it/s]100%|██████████| 185/185 [00:00<00:00, 1299.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1261.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22878.47it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22463.85it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22823.96it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 215.78it/s] 14%|█▎        | 25/185 [00:00<00:00, 218.71it/s]100%|██████████| 185/185 [00:00<00:00, 1508.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1528.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22413.24it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22603.89it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 201.37it/s]100%|██████████| 185/185 [00:00<00:00, 1408.23it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 20821.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3315.1 words/s - loss: 0.6432
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2846.8 words/s - loss: 0.6412
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2854.2 words/s - loss: 0.6537
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2726.8 words/s - loss: 0.6347
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2595.7 words/s - loss: 0.6491
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2549.1 words/s - loss: 0.6404
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2510.2 words/s - loss: 0.6315
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2471.6 words/s - loss: 0.6317
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2528.1 words/s - loss: 0.6322
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2289.2 words/s - loss: 0.6499
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2837.8 words/s - loss: 0.3605
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2589.8 words/s - loss: 0.3796
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2535.0 words/s - loss: 0.3792
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2912.6 words/s - loss: 0.3327
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2731.1 words/s - loss: 0.3304
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2655.5 words/s - loss: 0.3470
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2657.5 words/s - loss: 0.3391
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2326.1 words/s - loss: 0.3950
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2452.7 words/s - loss: 0.3821
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2841.9 words/s - loss: 0.3507
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2769.3 words/s - loss: 0.3526
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3066.3 words/s - loss: 0.3036
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3330.7 words/s - loss: 0.3260
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2659.8 words/s - loss: 0.2916
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2775.5 words/s - loss: 0.3585
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2757.1 words/s - loss: 0.3051
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2499.1 words/s - loss: 0.2884
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2545.9 words/s - loss: 0.3085
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2490.0 words/s - loss: 0.3254
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2313.0 words/s - loss: 0.3424
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3217.6 words/s - loss: 0.2830
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2378.7 words/s - loss: 0.2770
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2826.6 words/s - loss: 0.2940
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2937.2 words/s - loss: 0.2882
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3027.9 words/s - loss: 0.3009
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3026.9 words/s - loss: 0.2959
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2814.0 words/s - loss: 0.3202
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2559.9 words/s - loss: 0.2941
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2647.6 words/s - loss: 0.2917
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2784.7 words/s - loss: 0.3197
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2995.2 words/s - loss: 0.2812
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2645.7 words/s - loss: 0.3255
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2896.5 words/s - loss: 0.2872
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2712.4 words/s - loss: 0.2544
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2720.1 words/s - loss: 0.2728
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2581.9 words/s - loss: 0.2885
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3010.0 words/s - loss: 0.2593
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3261.7 words/s - loss: 0.2661
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2738.5 words/s - loss: 0.2411
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2320.6 words/s - loss: 0.3029
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3114.1 words/s - loss: 0.2300
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2507.0 words/s - loss: 0.2667
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2803.1 words/s - loss: 0.2702
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2555.0 words/s - loss: 0.2543
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2577.0 words/s - loss: 0.2500
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2801.4 words/s - loss: 0.2420
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2343.0 words/s - loss: 0.2708
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2535.7 words/s - loss: 0.2930
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2770.7 words/s - loss: 0.2425
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2086.7 words/s - loss: 0.2541
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.361 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.320 w/ 92 queries
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3146.3 words/s - loss: 0.2614
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3065.5 words/s - loss: 0.2640
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3112.3 words/s - loss: 0.2618
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2939.3 words/s - loss: 0.2836
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2602.4 words/s - loss: 0.2910
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2585.0 words/s - loss: 0.2946
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2591.9 words/s - loss: 0.2340
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2571.1 words/s - loss: 0.2299
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2449.5 words/s - loss: 0.2449
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2311.5 words/s - loss: 0.2434
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.356 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.317 w/ 92 queries
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3001.3 words/s - loss: 0.2572
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2918.3 words/s - loss: 0.2647
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2754.3 words/s - loss: 0.2745
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2686.9 words/s - loss: 0.2526
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2655.5 words/s - loss: 0.2511
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2582.3 words/s - loss: 0.2396
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2677.5 words/s - loss: 0.2504
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2595.4 words/s - loss: 0.2358
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2393.8 words/s - loss: 0.2428
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2044.9 words/s - loss: 0.2158
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.363 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_7.txt
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3174.3 words/s - loss: 0.2319
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3105.2 words/s - loss: 0.2455
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3070.3 words/s - loss: 0.2177
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2825.3 words/s - loss: 0.2504
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2766.1 words/s - loss: 0.2398
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2616.7 words/s - loss: 0.2686
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2656.2 words/s - loss: 0.2105
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2565.6 words/s - loss: 0.2541
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2427.8 words/s - loss: 0.2525
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2476.6 words/s - loss: 0.2371
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.367 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.330 w/ 92 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3211.2 words/s - loss: 0.2254
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3105.4 words/s - loss: 0.2154
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2737.8 words/s - loss: 0.2192
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2668.8 words/s - loss: 0.2583
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2629.2 words/s - loss: 0.2297
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2599.4 words/s - loss: 0.2302
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2488.0 words/s - loss: 0.2443
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2496.7 words/s - loss: 0.2479
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2392.4 words/s - loss: 0.2471
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2331.6 words/s - loss: 0.2533
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.371 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.319 w/ 92 queries
INFO:__main__:removing file tmp/mbert_enfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_9.txt
INFO:__main__:0.36663470823899824
INFO:__main__:0.3194086344556354
INFO:__main__:best MAP: 0.343
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 751640.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 328.24it/s]100%|██████████| 185/185 [00:00<00:00, 1555.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24156.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 740676.70it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 632434.26it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 734811.49it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 681911.95it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 708186.27it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 694904.40it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 708066.72it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 576853.80it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 228.57it/s]100%|██████████| 185/185 [00:00<00:00, 1098.77it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 292.90it/s]100%|██████████| 185/185 [00:00<00:00, 23259.78it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1392.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
100%|██████████| 185/185 [00:00<00:00, 22988.28it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 289.20it/s] 20%|██        | 37/185 [00:00<00:00, 312.58it/s]100%|██████████| 185/185 [00:00<00:00, 1377.97it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1484.13it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23417.72it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 24181.82it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 241.54it/s]100%|██████████| 185/185 [00:00<00:00, 1159.66it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23357.10it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 290.82it/s]100%|██████████| 185/185 [00:00<00:00, 1384.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 228.45it/s]100%|██████████| 185/185 [00:00<00:00, 23622.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1098.80it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23296.79it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 683155.91it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 20%|██        | 37/185 [00:00<00:00, 295.65it/s]100%|██████████| 185/185 [00:00<00:00, 1407.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23809.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 248.90it/s]100%|██████████| 185/185 [00:00<00:00, 1193.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23186.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2871.1 words/s - loss: 0.3708
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2795.8 words/s - loss: 0.3928
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2644.7 words/s - loss: 0.3940
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2697.1 words/s - loss: 0.3967
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2667.6 words/s - loss: 0.3815
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2600.1 words/s - loss: 0.3444
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2629.4 words/s - loss: 0.3862
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2521.9 words/s - loss: 0.3530
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2548.8 words/s - loss: 0.3882
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2207.1 words/s - loss: 0.3643
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3149.4 words/s - loss: 0.2731
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2942.3 words/s - loss: 0.2832
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2717.4 words/s - loss: 0.2579
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2512.6 words/s - loss: 0.3049
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2503.6 words/s - loss: 0.2632
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2584.5 words/s - loss: 0.2639
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2574.7 words/s - loss: 0.2891
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2615.0 words/s - loss: 0.2477
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2297.9 words/s - loss: 0.2579
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2618.0 words/s - loss: 0.2707
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3057.6 words/s - loss: 0.2541
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2979.7 words/s - loss: 0.2782
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3134.2 words/s - loss: 0.2621
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2694.0 words/s - loss: 0.2742
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2671.1 words/s - loss: 0.2455
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3002.0 words/s - loss: 0.2374
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2447.7 words/s - loss: 0.2286
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2462.2 words/s - loss: 0.2279
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2415.1 words/s - loss: 0.2407
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2508.6 words/s - loss: 0.2506
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2971.2 words/s - loss: 0.2250
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2848.2 words/s - loss: 0.2389
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2321.3 words/s - loss: 0.2434
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2160.6 words/s - loss: 0.2345
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2477.6 words/s - loss: 0.2624
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2354.0 words/s - loss: 0.2468
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2672.8 words/s - loss: 0.2331
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2349.7 words/s - loss: 0.2238
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2627.4 words/s - loss: 0.2141
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2294.5 words/s - loss: 0.2607
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2772.3 words/s - loss: 0.2454
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2930.7 words/s - loss: 0.2308
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3023.7 words/s - loss: 0.2397
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2842.6 words/s - loss: 0.2328
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2725.3 words/s - loss: 0.2129
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2580.9 words/s - loss: 0.2294
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2789.5 words/s - loss: 0.2124
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2416.5 words/s - loss: 0.2552
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2352.3 words/s - loss: 0.2292
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2297.3 words/s - loss: 0.2418
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2283.4 words/s - loss: 0.2727
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3032.0 words/s - loss: 0.2305
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2829.1 words/s - loss: 0.2149
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2742.1 words/s - loss: 0.2120
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3037.8 words/s - loss: 0.2199
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2984.3 words/s - loss: 0.2005
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2582.4 words/s - loss: 0.2065
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2496.9 words/s - loss: 0.2191
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2481.0 words/s - loss: 0.2250
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2600.8 words/s - loss: 0.1962
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3437.5 words/s - loss: 0.2018
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3422.5 words/s - loss: 0.2557
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3122.2 words/s - loss: 0.2349
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2972.6 words/s - loss: 0.2036
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3116.5 words/s - loss: 0.1944
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2952.0 words/s - loss: 0.2144
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2800.4 words/s - loss: 0.2228
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2670.5 words/s - loss: 0.2466
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2550.6 words/s - loss: 0.2131
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2388.8 words/s - loss: 0.2098
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.414 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2921.9 words/s - loss: 0.2241
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3009.1 words/s - loss: 0.1962
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2984.5 words/s - loss: 0.1952
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2669.9 words/s - loss: 0.2092
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2655.0 words/s - loss: 0.1925
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2675.6 words/s - loss: 0.1853
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2539.7 words/s - loss: 0.2092
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2568.4 words/s - loss: 0.2056
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2491.7 words/s - loss: 0.1945
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2476.6 words/s - loss: 0.2129
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.409 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2974.7 words/s - loss: 0.2361
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2991.9 words/s - loss: 0.2075
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2822.7 words/s - loss: 0.2060
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2903.5 words/s - loss: 0.1609
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2862.6 words/s - loss: 0.1803
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2691.3 words/s - loss: 0.1953
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2599.6 words/s - loss: 0.2176
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2362.8 words/s - loss: 0.1932
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2349.6 words/s - loss: 0.2096
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2228.9 words/s - loss: 0.2227
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.368 w/ 92 queries
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3203.2 words/s - loss: 0.2056
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2995.8 words/s - loss: 0.2092
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2881.0 words/s - loss: 0.2011
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2877.1 words/s - loss: 0.1931
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2712.8 words/s - loss: 0.2547
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2511.7 words/s - loss: 0.2149
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2525.2 words/s - loss: 0.1778
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2445.9 words/s - loss: 0.1816
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2486.6 words/s - loss: 0.1857
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2435.6 words/s - loss: 0.2031
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.423 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:removing file tmp/mbert_enfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_9.txt
INFO:__main__:0.414285771479949
INFO:__main__:0.368439284990727
INFO:__main__:best MAP: 0.391
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 821285.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 324.93it/s]100%|██████████| 185/185 [00:00<00:00, 1540.82it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24318.23it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 726940.97it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 758518.52it/s]
100%|██████████| 5000/5000 [00:00<00:00, 746662.87it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 701576.34it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 692952.68it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 715702.68it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 639239.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684404.41it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 682866.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 20%|██        | 37/185 [00:00<00:00, 310.05it/s]100%|██████████| 185/185 [00:00<00:00, 1472.75it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23160.50it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 271.91it/s] 20%|██        | 37/185 [00:00<00:00, 269.43it/s]100%|██████████| 185/185 [00:00<00:00, 1294.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1287.31it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22335.82it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
 20%|██        | 37/185 [00:00<00:00, 269.43it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 23504.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1286.91it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 238.22it/s] 20%|██        | 37/185 [00:00<00:00, 226.76it/s]100%|██████████| 185/185 [00:00<00:00, 22944.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1141.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1091.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21965.30it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 23742.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 295.63it/s] 20%|██        | 37/185 [00:00<00:00, 228.37it/s]100%|██████████| 185/185 [00:00<00:00, 1406.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1098.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23733.60it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 23500.70it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 294.57it/s]100%|██████████| 185/185 [00:00<00:00, 1403.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24031.29it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2846.6 words/s - loss: 0.3722
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2804.8 words/s - loss: 0.3861
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2818.5 words/s - loss: 0.3806
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2670.4 words/s - loss: 0.4133
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2636.3 words/s - loss: 0.3764
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2618.2 words/s - loss: 0.3930
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2552.6 words/s - loss: 0.3276
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2459.3 words/s - loss: 0.3799
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2352.3 words/s - loss: 0.4142
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2384.3 words/s - loss: 0.3563
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2969.1 words/s - loss: 0.2718
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2635.7 words/s - loss: 0.2512
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2760.9 words/s - loss: 0.2853
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2566.7 words/s - loss: 0.2705
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2708.6 words/s - loss: 0.2986
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2904.1 words/s - loss: 0.2785
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2358.7 words/s - loss: 0.3022
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2549.1 words/s - loss: 0.2863
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2378.9 words/s - loss: 0.2489
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2280.0 words/s - loss: 0.2847
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3101.3 words/s - loss: 0.2395
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3103.4 words/s - loss: 0.2423
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2923.7 words/s - loss: 0.2678
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3346.9 words/s - loss: 0.2734
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2586.2 words/s - loss: 0.2604
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2310.7 words/s - loss: 0.2776
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2554.3 words/s - loss: 0.2323
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2407.3 words/s - loss: 0.2609
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2804.9 words/s - loss: 0.2392
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2349.5 words/s - loss: 0.2606
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2824.8 words/s - loss: 0.2290
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2856.7 words/s - loss: 0.2325
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2389.9 words/s - loss: 0.2368
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2558.1 words/s - loss: 0.2352
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2719.2 words/s - loss: 0.2283
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3036.6 words/s - loss: 0.2240
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2365.7 words/s - loss: 0.2306
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3019.9 words/s - loss: 0.2412
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2575.5 words/s - loss: 0.2403
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3040.8 words/s - loss: 0.2237
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2388.0 words/s - loss: 0.2560
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2923.4 words/s - loss: 0.2188
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2946.9 words/s - loss: 0.2193
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2958.8 words/s - loss: 0.2505
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2879.2 words/s - loss: 0.1988
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2781.2 words/s - loss: 0.2290
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2341.2 words/s - loss: 0.2293
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2701.0 words/s - loss: 0.2359
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2371.2 words/s - loss: 0.2463
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2854.5 words/s - loss: 0.2113
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2269.5 words/s - loss: 0.2290
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2527.0 words/s - loss: 0.2232
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2687.5 words/s - loss: 0.2253
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3050.9 words/s - loss: 0.2162
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2357.9 words/s - loss: 0.2039
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2958.2 words/s - loss: 0.2216
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3148.4 words/s - loss: 0.2021
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2453.3 words/s - loss: 0.2467
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2290.9 words/s - loss: 0.2133
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2480.7 words/s - loss: 0.2165
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.393 w/ 92 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3038.5 words/s - loss: 0.1931
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3040.7 words/s - loss: 0.2041
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2948.2 words/s - loss: 0.2341
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2607.5 words/s - loss: 0.1968
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2507.4 words/s - loss: 0.1945
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2479.3 words/s - loss: 0.2075
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2340.6 words/s - loss: 0.2473
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2343.5 words/s - loss: 0.2328
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2324.8 words/s - loss: 0.2186
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2317.5 words/s - loss: 0.1914
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3688.6 words/s - loss: 0.1950
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2830.0 words/s - loss: 0.2192
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2850.1 words/s - loss: 0.2015
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2634.1 words/s - loss: 0.2000
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2679.4 words/s - loss: 0.1798
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2591.9 words/s - loss: 0.2147
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2623.1 words/s - loss: 0.1979
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2265.1 words/s - loss: 0.1836
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2156.1 words/s - loss: 0.1827
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2085.9 words/s - loss: 0.1808
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.392 w/ 92 queries
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2770.0 words/s - loss: 0.1862
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2741.8 words/s - loss: 0.2041
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2774.1 words/s - loss: 0.1845
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2708.6 words/s - loss: 0.1975
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2679.3 words/s - loss: 0.2408
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2478.5 words/s - loss: 0.2294
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2460.0 words/s - loss: 0.2241
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2238.8 words/s - loss: 0.2158
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2333.2 words/s - loss: 0.1870
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2338.9 words/s - loss: 0.1894
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.382 w/ 92 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3232.2 words/s - loss: 0.1843
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2996.4 words/s - loss: 0.1890
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2869.1 words/s - loss: 0.1791
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2830.8 words/s - loss: 0.1847
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2734.0 words/s - loss: 0.1953
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2612.7 words/s - loss: 0.2321
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2678.2 words/s - loss: 0.1671
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2561.5 words/s - loss: 0.2011
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2504.5 words/s - loss: 0.1890
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2428.7 words/s - loss: 0.1890
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enfr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f1_4_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.381 w/ 92 queries
INFO:__main__:removing file tmp/mbert_enfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enfr_f2_7_9.txt
Traceback (most recent call last):
  File "finetune-search.py", line 580, in <module>
    clir.run()
  File "finetune-search.py", line 377, in run
    logger.info(self.f1_maps)
NameError: name 'f1_maps' is not defined
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', '--dataset', 'mix', '--source_lang', 'en', '--target_lang', 'fr', '--batch_size', '16', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '4', '--seed', '611']' returned non-zero exit status 1.

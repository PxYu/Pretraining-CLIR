INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 908880.99it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 866520.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 881082.26it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 891911.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1993.23it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25399.22it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 2076.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24584.82it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1853.65it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25676.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 823413.56it/s]
100%|██████████| 5000/5000 [00:00<00:00, 804832.48it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 825650.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 33%|███▎      | 61/185 [00:00<00:00, 609.96it/s]100%|██████████| 185/185 [00:00<00:00, 1764.97it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25535.47it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 776838.05it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:01, 152.34it/s] 14%|█▎        | 25/185 [00:00<00:01, 149.33it/s]100%|██████████| 185/185 [00:00<00:00, 1088.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1067.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25635.01it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 24868.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 31/185 [00:00<00:00, 309.98it/s]100%|██████████| 185/185 [00:00<00:00, 1744.74it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25835.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 214.02it/s]100%|██████████| 185/185 [00:00<00:00, 1508.77it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 26105.04it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3642.2 words/s - loss: 0.4348
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3596.0 words/s - loss: 0.4421
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3191.6 words/s - loss: 0.4378
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 2995.2 words/s - loss: 0.4430
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 2884.9 words/s - loss: 0.4468
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 2736.4 words/s - loss: 0.4630
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 2606.0 words/s - loss: 0.4489
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 2385.8 words/s - loss: 0.4431
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 3224.4 words/s - loss: 0.3411
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 3062.9 words/s - loss: 0.3424
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3337.4 words/s - loss: 0.3309
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 2803.2 words/s - loss: 0.3355
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 2927.6 words/s - loss: 0.3359
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 2694.1 words/s - loss: 0.3214
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 2602.3 words/s - loss: 0.3254
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 2507.6 words/s - loss: 0.3177
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3108.2 words/s - loss: 0.2747
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 2846.8 words/s - loss: 0.2777
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3275.2 words/s - loss: 0.2917
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3675.9 words/s - loss: 0.3180
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 2909.9 words/s - loss: 0.3045
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 2496.1 words/s - loss: 0.2926
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 2581.5 words/s - loss: 0.3002
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 2847.3 words/s - loss: 0.3191
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3302.5 words/s - loss: 0.2661
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3158.9 words/s - loss: 0.3213
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3156.9 words/s - loss: 0.3087
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 3491.0 words/s - loss: 0.2940
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 3238.7 words/s - loss: 0.2735
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 2599.0 words/s - loss: 0.2807
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 2655.8 words/s - loss: 0.2834
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3576.5 words/s - loss: 0.2851
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 2558.3 words/s - loss: 0.2501
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3132.2 words/s - loss: 0.2662
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 2839.0 words/s - loss: 0.2654
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3278.5 words/s - loss: 0.2536
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 2541.8 words/s - loss: 0.2778
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 2953.8 words/s - loss: 0.2470
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3075.0 words/s - loss: 0.2722
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 2792.5 words/s - loss: 0.2657
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 2949.0 words/s - loss: 0.2656
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 2755.7 words/s - loss: 0.2548
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 2470.8 words/s - loss: 0.2645
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3279.2 words/s - loss: 0.2514
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3161.5 words/s - loss: 0.2596
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 2410.4 words/s - loss: 0.2720
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 2830.8 words/s - loss: 0.2534
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3078.1 words/s - loss: 0.2556
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.282 w/ 92 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_defr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3335.7 words/s - loss: 0.2539
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3199.2 words/s - loss: 0.2195
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3206.4 words/s - loss: 0.2286
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3065.7 words/s - loss: 0.2591
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3013.5 words/s - loss: 0.2330
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 2985.0 words/s - loss: 0.2582
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 2842.7 words/s - loss: 0.2706
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 2485.5 words/s - loss: 0.2456
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.302 w/ 92 queries
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3554.2 words/s - loss: 0.2675
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 3299.3 words/s - loss: 0.2497
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3001.0 words/s - loss: 0.2369
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 2816.7 words/s - loss: 0.2519
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 2859.7 words/s - loss: 0.2249
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 2826.3 words/s - loss: 0.2628
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 2772.0 words/s - loss: 0.2468
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 2598.5 words/s - loss: 0.2471
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.317 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.264 w/ 92 queries
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3363.9 words/s - loss: 0.2470
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3377.0 words/s - loss: 0.2395
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3409.3 words/s - loss: 0.2056
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3066.6 words/s - loss: 0.2464
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3205.0 words/s - loss: 0.2308
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 2940.2 words/s - loss: 0.2456
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 2722.3 words/s - loss: 0.2048
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 2540.7 words/s - loss: 0.2255
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.332 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.310 w/ 92 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_defr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 3333.2 words/s - loss: 0.2352
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3265.7 words/s - loss: 0.2485
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3329.7 words/s - loss: 0.2325
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 3043.5 words/s - loss: 0.2485
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3070.1 words/s - loss: 0.2366
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 2984.4 words/s - loss: 0.2291
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 2790.8 words/s - loss: 0.2465
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 2506.6 words/s - loss: 0.2227
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.323 w/ 93 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.315 w/ 92 queries
INFO:__main__:removing file tmp/mbert_defr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_9.txt
INFO:__main__:[0.3272172568466004, 0.3132346238001552, 0.3167434103957008, 0.3322277605605717, 0.3228958770658635]
INFO:__main__:[0.2820102902594563, 0.3017130944871414, 0.2637243023224442, 0.3099273653644243, 0.31529418557900546]
INFO:__main__:0.3228958770658635
INFO:__main__:0.3099273653644243
INFO:__main__:best MAP: 0.316
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 901109.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 809554.91it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 901651.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 886482.65it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 820771.01it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 815821.99it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 812282.90it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 46%|████▋     | 86/185 [00:00<00:00, 858.45it/s]100%|██████████| 185/185 [00:00<00:00, 1772.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 26586.25it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 298.23it/s]100%|██████████| 185/185 [00:00<00:00, 1424.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 309.39it/s]100%|██████████| 185/185 [00:00<00:00, 25840.76it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1474.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24872.46it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 193.16it/s]100%|██████████| 185/185 [00:00<00:00, 936.93it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24250.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 762517.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 20%|██        | 37/185 [00:00<00:00, 190.39it/s]100%|██████████| 185/185 [00:00<00:00, 923.87it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25712.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 268.57it/s]100%|██████████| 185/185 [00:00<00:00, 1285.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23788.17it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 178.40it/s]100%|██████████| 185/185 [00:00<00:00, 866.15it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23279.32it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 286.27it/s]100%|██████████| 185/185 [00:00<00:00, 1366.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24364.81it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3617.0 words/s - loss: 0.3971
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3276.6 words/s - loss: 0.3694
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3136.7 words/s - loss: 0.4156
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 3076.3 words/s - loss: 0.3961
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 2822.8 words/s - loss: 0.4011
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 2683.1 words/s - loss: 0.3873
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 2645.9 words/s - loss: 0.4017
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 2381.2 words/s - loss: 0.3898
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3239.4 words/s - loss: 0.2912
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3039.3 words/s - loss: 0.3026
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 2925.7 words/s - loss: 0.2952
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3583.4 words/s - loss: 0.2736
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 3355.8 words/s - loss: 0.3011
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 2856.3 words/s - loss: 0.2824
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 2976.1 words/s - loss: 0.2767
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 2703.4 words/s - loss: 0.2950
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 2735.3 words/s - loss: 0.2631
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3217.1 words/s - loss: 0.2714
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 3170.2 words/s - loss: 0.2778
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 2973.0 words/s - loss: 0.2615
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 2780.3 words/s - loss: 0.2507
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 2544.9 words/s - loss: 0.2755
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3551.6 words/s - loss: 0.2601
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 2478.8 words/s - loss: 0.2728
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3181.9 words/s - loss: 0.2618
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 3054.9 words/s - loss: 0.2787
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 2800.1 words/s - loss: 0.2567
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3280.7 words/s - loss: 0.2449
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 2773.1 words/s - loss: 0.2382
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3335.5 words/s - loss: 0.2776
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 3119.4 words/s - loss: 0.2223
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 2600.0 words/s - loss: 0.2444
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3161.1 words/s - loss: 0.2622
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3108.5 words/s - loss: 0.2537
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3665.4 words/s - loss: 0.2421
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 2793.2 words/s - loss: 0.2289
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 2805.6 words/s - loss: 0.2228
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 2362.9 words/s - loss: 0.2328
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 2957.1 words/s - loss: 0.2576
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 3002.0 words/s - loss: 0.2229
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3074.1 words/s - loss: 0.2241
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 2753.9 words/s - loss: 0.2115
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3224.1 words/s - loss: 0.2174
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 2939.6 words/s - loss: 0.2513
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 2731.0 words/s - loss: 0.2622
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3286.1 words/s - loss: 0.2124
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 2965.3 words/s - loss: 0.2331
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3171.7 words/s - loss: 0.2378
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_defr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3064.3 words/s - loss: 0.2476
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3020.2 words/s - loss: 0.2327
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3076.3 words/s - loss: 0.2012
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 2887.2 words/s - loss: 0.2391
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 2826.9 words/s - loss: 0.2236
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 2819.0 words/s - loss: 0.2183
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 2472.4 words/s - loss: 0.2464
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 2372.0 words/s - loss: 0.2482
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.381 w/ 92 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 3417.5 words/s - loss: 0.2338
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3072.7 words/s - loss: 0.2413
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3049.0 words/s - loss: 0.2263
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 2981.1 words/s - loss: 0.2319
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3031.2 words/s - loss: 0.2090
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 2876.6 words/s - loss: 0.2080
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 2654.8 words/s - loss: 0.2343
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 2660.4 words/s - loss: 0.2043
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.403 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3422.3 words/s - loss: 0.2366
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3280.6 words/s - loss: 0.2358
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3200.5 words/s - loss: 0.1954
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3008.4 words/s - loss: 0.2330
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3137.6 words/s - loss: 0.2141
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3038.2 words/s - loss: 0.2099
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 2911.4 words/s - loss: 0.1960
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 2703.0 words/s - loss: 0.2157
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.401 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_defr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3349.0 words/s - loss: 0.2148
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 3121.6 words/s - loss: 0.2011
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3093.3 words/s - loss: 0.2192
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3042.7 words/s - loss: 0.1949
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 2862.7 words/s - loss: 0.2040
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 2851.6 words/s - loss: 0.1978
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 2822.4 words/s - loss: 0.1855
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 2679.5 words/s - loss: 0.2198
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.404 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:removing file tmp/mbert_defr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_9.txt
INFO:__main__:[0.40350410381364654, 0.41766479966011394, 0.40325718725561427, 0.4013418343492862, 0.4042758239744002]
INFO:__main__:[0.38472497241663006, 0.38112072320635315, 0.37215713556920493, 0.37668742145339484, 0.3794212418179106]
INFO:__main__:0.40350410381364654
INFO:__main__:0.38112072320635315
INFO:__main__:best MAP: 0.392
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 898522.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 881563.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 861607.23it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 845046.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 831675.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 829602.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 886145.53it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 631463.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 278.95it/s] 30%|██▉       | 55/185 [00:00<00:00, 549.83it/s]100%|██████████| 185/185 [00:00<00:00, 1336.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1755.19it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 297.45it/s] 20%|██        | 37/185 [00:00<00:00, 289.35it/s]100%|██████████| 185/185 [00:00<00:00, 26068.21it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1419.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25889.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1381.87it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24810.43it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 25074.20it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 332.16it/s]100%|██████████| 185/185 [00:00<00:00, 1576.73it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25424.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 182.44it/s] 20%|██        | 37/185 [00:00<00:00, 182.31it/s]100%|██████████| 185/185 [00:00<00:00, 885.77it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
100%|██████████| 185/185 [00:00<00:00, 884.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25902.00it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 24344.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 176.96it/s]100%|██████████| 185/185 [00:00<00:00, 858.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23710.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3141.7 words/s - loss: 0.3948
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 3116.9 words/s - loss: 0.3845
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3062.7 words/s - loss: 0.4110
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 2942.1 words/s - loss: 0.3847
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 2839.9 words/s - loss: 0.3668
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 2735.5 words/s - loss: 0.3746
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 2723.1 words/s - loss: 0.4026
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 2668.6 words/s - loss: 0.3862
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3709.6 words/s - loss: 0.2973
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 3134.8 words/s - loss: 0.3018
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 3222.6 words/s - loss: 0.3007
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3191.1 words/s - loss: 0.2818
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 2883.7 words/s - loss: 0.2758
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3101.7 words/s - loss: 0.2580
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 2877.8 words/s - loss: 0.2624
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 2239.6 words/s - loss: 0.2788
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 2916.6 words/s - loss: 0.2716
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 2769.1 words/s - loss: 0.2570
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3064.2 words/s - loss: 0.2571
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3020.7 words/s - loss: 0.2557
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 2716.8 words/s - loss: 0.2884
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 2861.6 words/s - loss: 0.2781
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 2808.7 words/s - loss: 0.2803
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3021.6 words/s - loss: 0.2765
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 2786.0 words/s - loss: 0.2382
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3107.1 words/s - loss: 0.2675
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3202.6 words/s - loss: 0.2591
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 2794.6 words/s - loss: 0.2296
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 3117.1 words/s - loss: 0.2484
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 2637.2 words/s - loss: 0.2672
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 2763.6 words/s - loss: 0.2567
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 2670.5 words/s - loss: 0.2507
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3564.1 words/s - loss: 0.2413
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3454.0 words/s - loss: 0.2345
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3234.8 words/s - loss: 0.2515
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3663.0 words/s - loss: 0.2420
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3138.9 words/s - loss: 0.2537
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 2914.0 words/s - loss: 0.2267
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 2490.0 words/s - loss: 0.2429
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 2792.5 words/s - loss: 0.2394
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 3278.5 words/s - loss: 0.2266
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3183.4 words/s - loss: 0.2196
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 2933.4 words/s - loss: 0.2429
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3493.3 words/s - loss: 0.2292
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 2482.4 words/s - loss: 0.2427
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3040.2 words/s - loss: 0.2166
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3138.1 words/s - loss: 0.2199
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3293.0 words/s - loss: 0.2234
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.405 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.369 w/ 92 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_defr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3554.3 words/s - loss: 0.2456
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3226.6 words/s - loss: 0.2368
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3210.3 words/s - loss: 0.2267
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 2943.8 words/s - loss: 0.2102
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 2724.3 words/s - loss: 0.2281
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 2593.1 words/s - loss: 0.2086
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 2599.7 words/s - loss: 0.2216
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 2573.7 words/s - loss: 0.2053
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.416 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 3137.7 words/s - loss: 0.2129
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3009.3 words/s - loss: 0.2284
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 2975.2 words/s - loss: 0.2176
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 2918.7 words/s - loss: 0.2093
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 2839.5 words/s - loss: 0.2006
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 2860.8 words/s - loss: 0.2108
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 2799.4 words/s - loss: 0.2096
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 2523.5 words/s - loss: 0.2068
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.420 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3350.7 words/s - loss: 0.2071
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3259.2 words/s - loss: 0.2323
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 2964.3 words/s - loss: 0.2323
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3032.9 words/s - loss: 0.1895
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 2936.7 words/s - loss: 0.2028
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 2826.6 words/s - loss: 0.2530
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 2649.2 words/s - loss: 0.2129
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 2630.1 words/s - loss: 0.2181
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.418 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_defr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3358.0 words/s - loss: 0.2294
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3173.5 words/s - loss: 0.2085
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 2922.3 words/s - loss: 0.2146
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 3023.7 words/s - loss: 0.2227
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 2890.4 words/s - loss: 0.1970
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 2561.1 words/s - loss: 0.2038
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 2483.6 words/s - loss: 0.2146
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 2495.2 words/s - loss: 0.1938
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.431 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.387 w/ 92 queries
INFO:__main__:removing file tmp/mbert_defr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_9.txt
INFO:__main__:[0.4046475394287721, 0.41561242410559907, 0.42007290877693426, 0.41768461111420735, 0.4311646297730687]
INFO:__main__:[0.3686537600105226, 0.38389031793105804, 0.3869356890291864, 0.3766066908485155, 0.3867764041677745]
INFO:__main__:0.42007290877693426
INFO:__main__:0.3867764041677745
INFO:__main__:best MAP: 0.404

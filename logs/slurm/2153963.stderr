INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 803537.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 661791.79it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 746423.69it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 226.14it/s]100%|██████████| 185/185 [00:00<00:00, 1580.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23208.99it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 184.82it/s]100%|██████████| 185/185 [00:00<00:00, 1304.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
 14%|█▎        | 25/185 [00:00<00:00, 176.41it/s]  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1247.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22687.16it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22041.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 625138.46it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
 14%|█▎        | 25/185 [00:00<00:00, 204.47it/s]100%|██████████| 185/185 [00:00<00:00, 1437.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23549.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 811402.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 752260.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 728278.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 683668.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 738719.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 785509.03it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 197.34it/s]100%|██████████| 185/185 [00:00<00:00, 1387.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22530.38it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 197.20it/s]100%|██████████| 185/185 [00:00<00:00, 1385.10it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22232.14it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 203.32it/s]100%|██████████| 185/185 [00:00<00:00, 1427.22it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22888.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 203.16it/s]100%|██████████| 185/185 [00:00<00:00, 1426.73it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22552.64it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 196.30it/s]100%|██████████| 185/185 [00:00<00:00, 1381.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23335.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 177.37it/s]100%|██████████| 185/185 [00:00<00:00, 1253.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22902.78it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2980.5 words/s - loss: 0.5645
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2958.7 words/s - loss: 0.5712
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2875.6 words/s - loss: 0.5767
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2739.0 words/s - loss: 0.5567
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2780.4 words/s - loss: 0.5698
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2763.6 words/s - loss: 0.5528
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2604.9 words/s - loss: 0.5674
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2591.6 words/s - loss: 0.5481
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2513.3 words/s - loss: 0.5644
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2405.1 words/s - loss: 0.5512
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3324.9 words/s - loss: 0.3862
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3064.0 words/s - loss: 0.3900
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2852.5 words/s - loss: 0.3585
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3064.8 words/s - loss: 0.3493
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3043.4 words/s - loss: 0.3301
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2688.6 words/s - loss: 0.3656
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2673.6 words/s - loss: 0.3535
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2506.1 words/s - loss: 0.3663
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2734.0 words/s - loss: 0.3501
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2378.8 words/s - loss: 0.3718
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2843.8 words/s - loss: 0.3199
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2777.7 words/s - loss: 0.2884
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2609.9 words/s - loss: 0.2660
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3022.7 words/s - loss: 0.2902
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2813.7 words/s - loss: 0.2931
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2188.9 words/s - loss: 0.3183
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2980.3 words/s - loss: 0.2958
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2375.7 words/s - loss: 0.2924
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2558.1 words/s - loss: 0.2729
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2803.3 words/s - loss: 0.2786
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2537.8 words/s - loss: 0.2672
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2852.3 words/s - loss: 0.2851
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2632.6 words/s - loss: 0.2763
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2604.8 words/s - loss: 0.2794
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2458.8 words/s - loss: 0.2987
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2483.3 words/s - loss: 0.2626
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3018.8 words/s - loss: 0.2491
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2707.8 words/s - loss: 0.2677
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2490.3 words/s - loss: 0.2729
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2211.3 words/s - loss: 0.2826
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3079.4 words/s - loss: 0.2813
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2622.1 words/s - loss: 0.2574
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2629.9 words/s - loss: 0.2219
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2633.9 words/s - loss: 0.2686
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2952.8 words/s - loss: 0.2379
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3134.7 words/s - loss: 0.2753
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2458.3 words/s - loss: 0.2489
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2657.2 words/s - loss: 0.2359
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2878.9 words/s - loss: 0.2676
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2472.6 words/s - loss: 0.2764
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2384.6 words/s - loss: 0.2409
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3039.6 words/s - loss: 0.2569
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2860.5 words/s - loss: 0.2223
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2479.4 words/s - loss: 0.2287
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2307.0 words/s - loss: 0.2562
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2670.7 words/s - loss: 0.2525
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2589.9 words/s - loss: 0.2597
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3034.6 words/s - loss: 0.2177
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2340.1 words/s - loss: 0.2411
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2590.4 words/s - loss: 0.2210
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.329 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.276 w/ 92 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3080.8 words/s - loss: 0.2427
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2962.9 words/s - loss: 0.2276
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2887.6 words/s - loss: 0.2296
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2956.9 words/s - loss: 0.2272
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2840.8 words/s - loss: 0.2429
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2798.6 words/s - loss: 0.2237
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2635.6 words/s - loss: 0.2553
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2516.6 words/s - loss: 0.2175
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2421.3 words/s - loss: 0.2382
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2240.7 words/s - loss: 0.2160
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.310 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2902.3 words/s - loss: 0.2355
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2869.1 words/s - loss: 0.2379
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2867.3 words/s - loss: 0.2073
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2827.6 words/s - loss: 0.2358
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2731.9 words/s - loss: 0.2039
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2606.4 words/s - loss: 0.2329
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2557.5 words/s - loss: 0.2225
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2487.6 words/s - loss: 0.2227
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2448.5 words/s - loss: 0.1955
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2398.9 words/s - loss: 0.2228
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.284 w/ 92 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3116.5 words/s - loss: 0.2122
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3047.0 words/s - loss: 0.2353
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2929.7 words/s - loss: 0.2241
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2783.2 words/s - loss: 0.1895
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2741.5 words/s - loss: 0.2216
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2668.7 words/s - loss: 0.2206
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2624.5 words/s - loss: 0.2334
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2622.8 words/s - loss: 0.2279
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2313.9 words/s - loss: 0.2188
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2123.7 words/s - loss: 0.2506
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.319 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3192.3 words/s - loss: 0.2109
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3167.2 words/s - loss: 0.2258
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2761.2 words/s - loss: 0.2109
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2713.6 words/s - loss: 0.2185
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2664.8 words/s - loss: 0.2059
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2458.6 words/s - loss: 0.2198
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2387.4 words/s - loss: 0.2031
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2397.0 words/s - loss: 0.2120
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2373.1 words/s - loss: 0.2391
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2077.3 words/s - loss: 0.2472
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.333 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.295 w/ 92 queries
INFO:__main__:removing file tmp/mbert_esfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_9.txt
INFO:__main__:0.33326842766436704
INFO:__main__:0.2952511811945241
INFO:__main__:best MAP: 0.314
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 752908.74it/s]
100%|██████████| 5000/5000 [00:00<00:00, 753152.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 701435.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 699773.77it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 706420.99it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 718227.34it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 718030.61it/s]100%|██████████| 5000/5000 [00:00<00:00, 750645.00it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 693617.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 680319.21it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 265.24it/s] 20%|██        | 37/185 [00:00<00:00, 269.37it/s]100%|██████████| 185/185 [00:00<00:00, 1266.67it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1287.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22384.14it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 23314.99it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 315.63it/s]100%|██████████| 185/185 [00:00<00:00, 1495.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 253.77it/s]100%|██████████| 185/185 [00:00<00:00, 23122.54it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1215.61it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 284.21it/s]100%|██████████| 185/185 [00:00<00:00, 23186.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1353.61it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
 20%|██        | 37/185 [00:00<00:00, 226.95it/s]  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1092.04it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23587.86it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 241.35it/s] 20%|██        | 37/185 [00:00<00:00, 223.46it/s] 20%|██        | 37/185 [00:00<00:00, 176.16it/s]100%|██████████| 185/185 [00:00<00:00, 23524.93it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
 20%|██        | 37/185 [00:00<00:00, 184.31it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1158.57it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1075.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
100%|██████████| 185/185 [00:00<00:00, 853.10it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 892.25it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23853.25it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
100%|██████████| 185/185 [00:00<00:00, 22719.71it/s]INFO:__main__:f2 has 92 queries ...

INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 21635.80it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22501.63it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2947.2 words/s - loss: 0.3430
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2842.5 words/s - loss: 0.3478
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2810.0 words/s - loss: 0.3677
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2845.9 words/s - loss: 0.3810
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2801.3 words/s - loss: 0.3584
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2738.4 words/s - loss: 0.3971
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2675.3 words/s - loss: 0.3668
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2497.5 words/s - loss: 0.3921
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2555.0 words/s - loss: 0.3388
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2196.1 words/s - loss: 0.3666
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3297.8 words/s - loss: 0.2814
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2834.8 words/s - loss: 0.2759
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2767.2 words/s - loss: 0.2448
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2823.5 words/s - loss: 0.2559
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2668.1 words/s - loss: 0.2639
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2721.1 words/s - loss: 0.2641
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2480.1 words/s - loss: 0.2915
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2597.8 words/s - loss: 0.2638
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2501.1 words/s - loss: 0.2656
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2805.3 words/s - loss: 0.2402
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3090.2 words/s - loss: 0.2644
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2925.1 words/s - loss: 0.2508
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2541.9 words/s - loss: 0.2632
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2561.6 words/s - loss: 0.2213
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2445.7 words/s - loss: 0.2251
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2660.4 words/s - loss: 0.2341
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2563.2 words/s - loss: 0.2408
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2407.6 words/s - loss: 0.2056
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2692.1 words/s - loss: 0.2460
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2458.7 words/s - loss: 0.2405
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2913.8 words/s - loss: 0.2230
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2742.6 words/s - loss: 0.2330
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3118.1 words/s - loss: 0.2162
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2747.1 words/s - loss: 0.2369
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2548.7 words/s - loss: 0.2573
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3214.9 words/s - loss: 0.2050
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2676.5 words/s - loss: 0.2380
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2800.0 words/s - loss: 0.2433
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2222.5 words/s - loss: 0.2442
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2683.4 words/s - loss: 0.2190
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2655.2 words/s - loss: 0.2279
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2613.1 words/s - loss: 0.2306
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2316.5 words/s - loss: 0.2279
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2542.1 words/s - loss: 0.1961
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3044.8 words/s - loss: 0.2001
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2845.3 words/s - loss: 0.2439
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2860.2 words/s - loss: 0.2255
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2327.9 words/s - loss: 0.2178
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2548.8 words/s - loss: 0.1774
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2256.7 words/s - loss: 0.1922
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2728.3 words/s - loss: 0.2048
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3334.7 words/s - loss: 0.2225
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2815.5 words/s - loss: 0.2343
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2658.0 words/s - loss: 0.1999
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2667.5 words/s - loss: 0.2081
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3044.8 words/s - loss: 0.1979
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2781.9 words/s - loss: 0.2246
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2425.7 words/s - loss: 0.2020
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2766.8 words/s - loss: 0.1773
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2694.9 words/s - loss: 0.2220
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3222.8 words/s - loss: 0.2159
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3062.6 words/s - loss: 0.2288
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3046.6 words/s - loss: 0.1963
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2918.6 words/s - loss: 0.1798
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 2887.4 words/s - loss: 0.2207
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2710.2 words/s - loss: 0.2142
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2498.1 words/s - loss: 0.2345
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2525.8 words/s - loss: 0.1769
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2439.5 words/s - loss: 0.1991
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2077.9 words/s - loss: 0.2103
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3117.6 words/s - loss: 0.2735
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2932.6 words/s - loss: 0.2106
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2959.9 words/s - loss: 0.2006
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2912.4 words/s - loss: 0.2150
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2739.5 words/s - loss: 0.2170
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2705.2 words/s - loss: 0.1988
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2649.8 words/s - loss: 0.1910
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2459.1 words/s - loss: 0.2166
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2448.9 words/s - loss: 0.1717
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2262.3 words/s - loss: 0.2023
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.446 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.376 w/ 92 queries
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3438.6 words/s - loss: 0.2131
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3202.1 words/s - loss: 0.1911
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3082.3 words/s - loss: 0.2115
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2735.5 words/s - loss: 0.2366
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2531.6 words/s - loss: 0.2023
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2595.0 words/s - loss: 0.2152
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2539.6 words/s - loss: 0.1983
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2460.3 words/s - loss: 0.2021
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2366.6 words/s - loss: 0.2035
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2346.3 words/s - loss: 0.2180
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3121.9 words/s - loss: 0.1951
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2853.4 words/s - loss: 0.1929
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2797.2 words/s - loss: 0.1733
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2673.6 words/s - loss: 0.1944
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2710.8 words/s - loss: 0.2188
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2721.1 words/s - loss: 0.2012
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2660.4 words/s - loss: 0.1905
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2700.3 words/s - loss: 0.1663
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2593.4 words/s - loss: 0.1808
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2267.7 words/s - loss: 0.2211
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.435 w/ 93 queries
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.374 w/ 92 queries
INFO:__main__:removing file tmp/mbert_esfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_9.txt
INFO:__main__:0.42382405403304035
INFO:__main__:0.3763434479017755
INFO:__main__:best MAP: 0.400
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 714702.65it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 728203.06it/s]
  0%|          | 0/5000 [00:00<?, ?it/s]INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 742880.62it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 658963.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 733193.02it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 734862.99it/s]100%|██████████| 5000/5000 [00:00<00:00, 737058.31it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 661332.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 659751.47it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 656591.11it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 224.80it/s]100%|██████████| 185/185 [00:00<00:00, 1083.43it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 330.97it/s]100%|██████████| 185/185 [00:00<00:00, 1568.52it/s]
100%|██████████| 185/185 [00:00<00:00, 24668.45it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 262.81it/s]100%|██████████| 185/185 [00:00<00:00, 24000.07it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1255.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23266.06it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 228.76it/s] 20%|██        | 37/185 [00:00<00:00, 255.20it/s]100%|██████████| 185/185 [00:00<00:00, 1101.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
100%|██████████| 185/185 [00:00<00:00, 1221.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 252.93it/s] 20%|██        | 37/185 [00:00<00:00, 219.80it/s]100%|██████████| 185/185 [00:00<00:00, 1212.43it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23708.22it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 23087.45it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1056.61it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 23788.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 21757.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 207.45it/s]100%|██████████| 185/185 [00:00<00:00, 1001.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23234.01it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
 20%|██        | 37/185 [00:00<00:00, 223.30it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1075.01it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23138.40it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 200.19it/s]100%|██████████| 185/185 [00:00<00:00, 965.09it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22201.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2891.1 words/s - loss: 0.3808
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2877.5 words/s - loss: 0.3773
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2845.4 words/s - loss: 0.3902
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2766.1 words/s - loss: 0.3763
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2830.9 words/s - loss: 0.3621
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2585.7 words/s - loss: 0.3858
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2582.5 words/s - loss: 0.3960
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2561.6 words/s - loss: 0.3585
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2465.4 words/s - loss: 0.3561
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2300.9 words/s - loss: 0.3683
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2853.0 words/s - loss: 0.2684
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2705.0 words/s - loss: 0.2515
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2638.7 words/s - loss: 0.2545
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3047.8 words/s - loss: 0.2429
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2807.3 words/s - loss: 0.2691
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2544.5 words/s - loss: 0.2782
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2800.0 words/s - loss: 0.2407
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2436.0 words/s - loss: 0.2395
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2742.1 words/s - loss: 0.2570
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2854.4 words/s - loss: 0.2772
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3271.8 words/s - loss: 0.2181
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2852.1 words/s - loss: 0.2446
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2698.6 words/s - loss: 0.2551
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2801.2 words/s - loss: 0.2390
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2905.1 words/s - loss: 0.2169
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3116.4 words/s - loss: 0.2407
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2483.7 words/s - loss: 0.2149
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2357.1 words/s - loss: 0.2585
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2936.9 words/s - loss: 0.2259
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2311.7 words/s - loss: 0.2153
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3369.6 words/s - loss: 0.2486
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3143.4 words/s - loss: 0.2368
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3037.5 words/s - loss: 0.2161
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2505.3 words/s - loss: 0.2137
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2754.4 words/s - loss: 0.2248
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2759.7 words/s - loss: 0.2299
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2507.2 words/s - loss: 0.2255
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2380.2 words/s - loss: 0.2315
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2503.8 words/s - loss: 0.2350
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2767.1 words/s - loss: 0.2464
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3091.0 words/s - loss: 0.2425
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2937.2 words/s - loss: 0.1879
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2967.9 words/s - loss: 0.2141
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2854.8 words/s - loss: 0.2031
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2634.1 words/s - loss: 0.1737
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2695.1 words/s - loss: 0.2230
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2594.6 words/s - loss: 0.2380
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2820.0 words/s - loss: 0.2246
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2436.8 words/s - loss: 0.2279
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2403.8 words/s - loss: 0.2110
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2841.4 words/s - loss: 0.2007
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2224.9 words/s - loss: 0.2407
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2911.7 words/s - loss: 0.2491
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2657.6 words/s - loss: 0.2044
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2441.2 words/s - loss: 0.2237
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2659.3 words/s - loss: 0.2419
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2481.5 words/s - loss: 0.2061
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3087.6 words/s - loss: 0.2146
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2878.5 words/s - loss: 0.1906
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2633.3 words/s - loss: 0.2089
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.443 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3236.5 words/s - loss: 0.2033
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3059.6 words/s - loss: 0.1973
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3035.4 words/s - loss: 0.2254
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2877.0 words/s - loss: 0.2324
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2766.5 words/s - loss: 0.2181
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2646.8 words/s - loss: 0.2215
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2695.2 words/s - loss: 0.1898
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2608.2 words/s - loss: 0.2256
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2577.4 words/s - loss: 0.1999
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2289.0 words/s - loss: 0.1990
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.448 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3664.7 words/s - loss: 0.1855
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3338.3 words/s - loss: 0.2053
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2893.9 words/s - loss: 0.2181
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2769.1 words/s - loss: 0.2198
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2849.7 words/s - loss: 0.1851
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2708.4 words/s - loss: 0.1869
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2642.9 words/s - loss: 0.1764
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2673.8 words/s - loss: 0.2165
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2576.8 words/s - loss: 0.1838
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2480.2 words/s - loss: 0.2042
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.386 w/ 92 queries
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2800.5 words/s - loss: 0.2126
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2711.1 words/s - loss: 0.1993
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2764.1 words/s - loss: 0.1874
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2711.8 words/s - loss: 0.1986
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2658.6 words/s - loss: 0.1999
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2698.3 words/s - loss: 0.2158
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2697.2 words/s - loss: 0.1839
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2397.3 words/s - loss: 0.1947
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2470.2 words/s - loss: 0.2118
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2242.8 words/s - loss: 0.2009
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3348.4 words/s - loss: 0.2129
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2990.8 words/s - loss: 0.1761
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2894.5 words/s - loss: 0.2035
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2773.9 words/s - loss: 0.1866
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2681.1 words/s - loss: 0.1822
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2509.8 words/s - loss: 0.2021
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2590.5 words/s - loss: 0.1682
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2511.2 words/s - loss: 0.1857
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2494.6 words/s - loss: 0.1925
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2429.7 words/s - loss: 0.2078
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:removing file tmp/mbert_esfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_9.txt
Traceback (most recent call last):
  File "finetune-search.py", line 580, in <module>
    clir.run()
  File "finetune-search.py", line 377, in run
    logger.info(self.f1_maps)
NameError: name 'f1_maps' is not defined
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', '--dataset', 'mix', '--source_lang', 'es', '--target_lang', 'fr', '--batch_size', '16', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '4', '--seed', '611']' returned non-zero exit status 1.

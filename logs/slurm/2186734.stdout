11: The current process just got forked. Disabling parallelism to avoid deadlocks...
11: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
15: The current process just got forked. Disabling parallelism to avoid deadlocks...
15: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 9: The current process just got forked. Disabling parallelism to avoid deadlocks...
 9: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
10: The current process just got forked. Disabling parallelism to avoid deadlocks...
10: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 8: The current process just got forked. Disabling parallelism to avoid deadlocks...
 8: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
12: The current process just got forked. Disabling parallelism to avoid deadlocks...
12: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
13: The current process just got forked. Disabling parallelism to avoid deadlocks...
13: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
14: The current process just got forked. Disabling parallelism to avoid deadlocks...
14: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 3: The current process just got forked. Disabling parallelism to avoid deadlocks...
 3: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 4: The current process just got forked. Disabling parallelism to avoid deadlocks...
 4: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 6: The current process just got forked. Disabling parallelism to avoid deadlocks...
 6: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 1: The current process just got forked. Disabling parallelism to avoid deadlocks...
 1: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 0: The current process just got forked. Disabling parallelism to avoid deadlocks...
 0: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 2: The current process just got forked. Disabling parallelism to avoid deadlocks...
 2: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 5: The current process just got forked. Disabling parallelism to avoid deadlocks...
 5: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 7: The current process just got forked. Disabling parallelism to avoid deadlocks...
 7: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 5: The current process just got forked. Disabling parallelism to avoid deadlocks...
 5: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
 5: The current process just got forked. Disabling parallelism to avoid deadlocks...
 5: To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
13: SLURM job: True
13: 13 - SLURM_JOB_ID: 2186734
13: 13 - SLURM_JOB_NODELIST: dgx-[1-2]
13: 13 - SLURM_JOB_NUM_NODES: 2
13: 13 - SLURM_NTASKS: 16
13: 13 - SLURM_TASKS_PER_NODE: 8(x2)
13: 13 - SLURM_MEM_PER_NODE: None
13: 13 - SLURM_MEM_PER_CPU: None
13: 13 - SLURM_NODEID: 1
13: 13 - SLURM_PROCID: 13
13: 13 - SLURM_LOCALID: 5
13: 13 - SLURM_TASK_PID: 56877
13: 13 - Master address: dgx-1
13: 13 - Master port   : 12223
13: 13 - Number of nodes: 2
13: 13 - Node ID        : 1
13: 13 - Local rank     : 5
13: 13 - Global rank    : 13
13: 13 - World size     : 16
13: 13 - GPUs per node  : 8
13: 13 - Master         : False
13: 13 - Multi-node     : True
13: 13 - Multi-GPU      : True
13: 13 - Hostname       : dgx-2.svail.baidu.com
13: Initializing PyTorch distributed ...
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
13: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: SLURM job: True
 6: 6 - SLURM_JOB_ID: 2186734
 6: 6 - SLURM_JOB_NODELIST: dgx-[1-2]
 6: 6 - SLURM_JOB_NUM_NODES: 2
 6: 6 - SLURM_NTASKS: 16
 6: 6 - SLURM_TASKS_PER_NODE: 8(x2)
 6: 6 - SLURM_MEM_PER_NODE: None
 6: 6 - SLURM_MEM_PER_CPU: None
 6: 6 - SLURM_NODEID: 0
 6: 6 - SLURM_PROCID: 6
 6: 6 - SLURM_LOCALID: 6
 6: 6 - SLURM_TASK_PID: 13803
 6: 6 - Master address: dgx-1
 6: 6 - Master port   : 12223
 6: 6 - Number of nodes: 2
 6: 6 - Node ID        : 0
 6: 6 - Local rank     : 6
 6: 6 - Global rank    : 6
 6: 6 - World size     : 16
 6: 6 - GPUs per node  : 8
 6: 6 - Master         : False
 6: 6 - Multi-node     : True
 6: 6 - Multi-GPU      : True
 6: 6 - Hostname       : dgx-1.svail.baidu.com
 6: Initializing PyTorch distributed ...
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: SLURM job: True
11: 11 - SLURM_JOB_ID: 2186734
11: 11 - SLURM_JOB_NODELIST: dgx-[1-2]
11: 11 - SLURM_JOB_NUM_NODES: 2
11: 11 - SLURM_NTASKS: 16
11: 11 - SLURM_TASKS_PER_NODE: 8(x2)
11: 11 - SLURM_MEM_PER_NODE: None
11: 11 - SLURM_MEM_PER_CPU: None
11: 11 - SLURM_NODEID: 1
11: 11 - SLURM_PROCID: 11
11: 11 - SLURM_LOCALID: 3
11: 11 - SLURM_TASK_PID: 56875
11: 11 - Master address: dgx-1
11: 11 - Master port   : 12223
11: 11 - Number of nodes: 2
11: 11 - Node ID        : 1
11: 11 - Local rank     : 3
11: 11 - Global rank    : 11
11: 11 - World size     : 16
11: 11 - GPUs per node  : 8
11: 11 - Master         : False
11: 11 - Multi-node     : True
11: 11 - Multi-GPU      : True
11: 11 - Hostname       : dgx-2.svail.baidu.com
11: Initializing PyTorch distributed ...
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
11: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 6: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: SLURM job: True
12: 12 - SLURM_JOB_ID: 2186734
12: 12 - SLURM_JOB_NODELIST: dgx-[1-2]
12: 12 - SLURM_JOB_NUM_NODES: 2
12: 12 - SLURM_NTASKS: 16
12: 12 - SLURM_TASKS_PER_NODE: 8(x2)
12: 12 - SLURM_MEM_PER_NODE: None
12: 12 - SLURM_MEM_PER_CPU: None
12: 12 - SLURM_NODEID: 1
12: 12 - SLURM_PROCID: 12
12: 12 - SLURM_LOCALID: 4
12: 12 - SLURM_TASK_PID: 56876
12: 12 - Master address: dgx-1
12: 12 - Master port   : 12223
12: 12 - Number of nodes: 2
12: 12 - Node ID        : 1
12: 12 - Local rank     : 4
12: 12 - Global rank    : 12
12: 12 - World size     : 16
12: 12 - GPUs per node  : 8
12: 12 - Master         : False
12: 12 - Multi-node     : True
12: 12 - Multi-GPU      : True
12: 12 - Hostname       : dgx-2.svail.baidu.com
12: Initializing PyTorch distributed ...
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
12: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: SLURM job: True
 8: 8 - SLURM_JOB_ID: 2186734
 8: 8 - SLURM_JOB_NODELIST: dgx-[1-2]
 8: 8 - SLURM_JOB_NUM_NODES: 2
 8: 8 - SLURM_NTASKS: 16
 8: 8 - SLURM_TASKS_PER_NODE: 8(x2)
 8: 8 - SLURM_MEM_PER_NODE: None
 8: 8 - SLURM_MEM_PER_CPU: None
 8: 8 - SLURM_NODEID: 1
 8: 8 - SLURM_PROCID: 8
 8: 8 - SLURM_LOCALID: 0
 8: 8 - SLURM_TASK_PID: 56872
 8: 8 - Master address: dgx-1
 8: 8 - Master port   : 12223
 8: 8 - Number of nodes: 2
 8: 8 - Node ID        : 1
 8: 8 - Local rank     : 0
 8: 8 - Global rank    : 8
 8: 8 - World size     : 16
 8: 8 - GPUs per node  : 8
 8: 8 - Master         : False
 8: 8 - Multi-node     : True
 8: 8 - Multi-GPU      : True
 8: 8 - Hostname       : dgx-2.svail.baidu.com
 8: Initializing PyTorch distributed ...
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 8: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: SLURM job: True
10: 10 - SLURM_JOB_ID: 2186734
10: 10 - SLURM_JOB_NODELIST: dgx-[1-2]
10: 10 - SLURM_JOB_NUM_NODES: 2
10: 10 - SLURM_NTASKS: 16
10: 10 - SLURM_TASKS_PER_NODE: 8(x2)
10: 10 - SLURM_MEM_PER_NODE: None
10: 10 - SLURM_MEM_PER_CPU: None
10: 10 - SLURM_NODEID: 1
10: 10 - SLURM_PROCID: 10
10: 10 - SLURM_LOCALID: 2
10: 10 - SLURM_TASK_PID: 56874
10: 10 - Master address: dgx-1
10: 10 - Master port   : 12223
10: 10 - Number of nodes: 2
10: 10 - Node ID        : 1
10: 10 - Local rank     : 2
10: 10 - Global rank    : 10
10: 10 - World size     : 16
10: 10 - GPUs per node  : 8
10: 10 - Master         : False
10: 10 - Multi-node     : True
10: 10 - Multi-GPU      : True
10: 10 - Hostname       : dgx-2.svail.baidu.com
10: Initializing PyTorch distributed ...
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
10: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: SLURM job: True
 3: 3 - SLURM_JOB_ID: 2186734
 3: 3 - SLURM_JOB_NODELIST: dgx-[1-2]
 3: 3 - SLURM_JOB_NUM_NODES: 2
 3: 3 - SLURM_NTASKS: 16
 3: 3 - SLURM_TASKS_PER_NODE: 8(x2)
 3: 3 - SLURM_MEM_PER_NODE: None
 3: 3 - SLURM_MEM_PER_CPU: None
 3: 3 - SLURM_NODEID: 0
 3: 3 - SLURM_PROCID: 3
 3: 3 - SLURM_LOCALID: 3
 3: 3 - SLURM_TASK_PID: 13800
 3: 3 - Master address: dgx-1
 3: 3 - Master port   : 12223
 3: 3 - Number of nodes: 2
 3: 3 - Node ID        : 0
 3: 3 - Local rank     : 3
 3: 3 - Global rank    : 3
 3: 3 - World size     : 16
 3: 3 - GPUs per node  : 8
 3: 3 - Master         : False
 3: 3 - Multi-node     : True
 3: 3 - Multi-GPU      : True
 3: 3 - Hostname       : dgx-1.svail.baidu.com
 3: Initializing PyTorch distributed ...
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: SLURM job: True
14: 14 - SLURM_JOB_ID: 2186734
14: 14 - SLURM_JOB_NODELIST: dgx-[1-2]
14: 14 - SLURM_JOB_NUM_NODES: 2
14: 14 - SLURM_NTASKS: 16
14: 14 - SLURM_TASKS_PER_NODE: 8(x2)
14: 14 - SLURM_MEM_PER_NODE: None
14: 14 - SLURM_MEM_PER_CPU: None
14: 14 - SLURM_NODEID: 1
14: 14 - SLURM_PROCID: 14
14: 14 - SLURM_LOCALID: 6
14: 14 - SLURM_TASK_PID: 56878
14: 14 - Master address: dgx-1
14: 14 - Master port   : 12223
14: 14 - Number of nodes: 2
14: 14 - Node ID        : 1
14: 14 - Local rank     : 6
14: 14 - Global rank    : 14
14: 14 - World size     : 16
14: 14 - GPUs per node  : 8
14: 14 - Master         : False
14: 14 - Multi-node     : True
14: 14 - Multi-GPU      : True
14: 14 - Hostname       : dgx-2.svail.baidu.com
14: Initializing PyTorch distributed ...
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
14: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 3: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: SLURM job: True
 4: 4 - SLURM_JOB_ID: 2186734
 4: 4 - SLURM_JOB_NODELIST: dgx-[1-2]
 4: 4 - SLURM_JOB_NUM_NODES: 2
 4: 4 - SLURM_NTASKS: 16
 4: 4 - SLURM_TASKS_PER_NODE: 8(x2)
 4: 4 - SLURM_MEM_PER_NODE: None
 4: 4 - SLURM_MEM_PER_CPU: None
 4: 4 - SLURM_NODEID: 0
 4: 4 - SLURM_PROCID: 4
 4: 4 - SLURM_LOCALID: 4
 4: 4 - SLURM_TASK_PID: 13801
 4: 4 - Master address: dgx-1
 4: 4 - Master port   : 12223
 4: 4 - Number of nodes: 2
 4: 4 - Node ID        : 0
 4: 4 - Local rank     : 4
 4: 4 - Global rank    : 4
 4: 4 - World size     : 16
 4: 4 - GPUs per node  : 8
 4: 4 - Master         : False
 4: 4 - Multi-node     : True
 4: 4 - Multi-GPU      : True
 4: 4 - Hostname       : dgx-1.svail.baidu.com
 4: Initializing PyTorch distributed ...
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 4: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: SLURM job: True
 9: 9 - SLURM_JOB_ID: 2186734
 9: 9 - SLURM_JOB_NODELIST: dgx-[1-2]
 9: 9 - SLURM_JOB_NUM_NODES: 2
 9: 9 - SLURM_NTASKS: 16
 9: 9 - SLURM_TASKS_PER_NODE: 8(x2)
 9: 9 - SLURM_MEM_PER_NODE: None
 9: 9 - SLURM_MEM_PER_CPU: None
 9: 9 - SLURM_NODEID: 1
 9: 9 - SLURM_PROCID: 9
 9: 9 - SLURM_LOCALID: 1
 9: 9 - SLURM_TASK_PID: 56873
 9: 9 - Master address: dgx-1
 9: 9 - Master port   : 12223
 9: 9 - Number of nodes: 2
 9: 9 - Node ID        : 1
 9: 9 - Local rank     : 1
 9: 9 - Global rank    : 9
 9: 9 - World size     : 16
 9: 9 - GPUs per node  : 8
 9: 9 - Master         : False
 9: 9 - Multi-node     : True
 9: 9 - Multi-GPU      : True
 9: 9 - Hostname       : dgx-2.svail.baidu.com
 9: Initializing PyTorch distributed ...
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 9: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: SLURM job: True
 1: 1 - SLURM_JOB_ID: 2186734
 1: 1 - SLURM_JOB_NODELIST: dgx-[1-2]
 1: 1 - SLURM_JOB_NUM_NODES: 2
 1: 1 - SLURM_NTASKS: 16
 1: 1 - SLURM_TASKS_PER_NODE: 8(x2)
 1: 1 - SLURM_MEM_PER_NODE: None
 1: 1 - SLURM_MEM_PER_CPU: None
 1: 1 - SLURM_NODEID: 0
 1: 1 - SLURM_PROCID: 1
 1: 1 - SLURM_LOCALID: 1
 1: 1 - SLURM_TASK_PID: 13798
 1: 1 - Master address: dgx-1
 1: 1 - Master port   : 12223
 1: 1 - Number of nodes: 2
 1: 1 - Node ID        : 0
 1: 1 - Local rank     : 1
 1: 1 - Global rank    : 1
 1: 1 - World size     : 16
 1: 1 - GPUs per node  : 8
 1: 1 - Master         : False
 1: 1 - Multi-node     : True
 1: 1 - Multi-GPU      : True
 1: 1 - Hostname       : dgx-1.svail.baidu.com
 1: Initializing PyTorch distributed ...
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 1: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: SLURM job: True
 5: 5 - SLURM_JOB_ID: 2186734
 5: 5 - SLURM_JOB_NODELIST: dgx-[1-2]
 5: 5 - SLURM_JOB_NUM_NODES: 2
 5: 5 - SLURM_NTASKS: 16
 5: 5 - SLURM_TASKS_PER_NODE: 8(x2)
 5: 5 - SLURM_MEM_PER_NODE: None
 5: 5 - SLURM_MEM_PER_CPU: None
 5: 5 - SLURM_NODEID: 0
 5: 5 - SLURM_PROCID: 5
 5: 5 - SLURM_LOCALID: 5
 5: 5 - SLURM_TASK_PID: 13802
 5: 5 - Master address: dgx-1
 5: 5 - Master port   : 12223
 5: 5 - Number of nodes: 2
 5: 5 - Node ID        : 0
 5: 5 - Local rank     : 5
 5: 5 - Global rank    : 5
 5: 5 - World size     : 16
 5: 5 - GPUs per node  : 8
 5: 5 - Master         : False
 5: 5 - Multi-node     : True
 5: 5 - Multi-GPU      : True
 5: 5 - Hostname       : dgx-1.svail.baidu.com
 5: Initializing PyTorch distributed ...
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 5: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: SLURM job: True
 2: 2 - SLURM_JOB_ID: 2186734
 2: 2 - SLURM_JOB_NODELIST: dgx-[1-2]
 2: 2 - SLURM_JOB_NUM_NODES: 2
 2: 2 - SLURM_NTASKS: 16
 2: 2 - SLURM_TASKS_PER_NODE: 8(x2)
 2: 2 - SLURM_MEM_PER_NODE: None
 2: 2 - SLURM_MEM_PER_CPU: None
 2: 2 - SLURM_NODEID: 0
 2: 2 - SLURM_PROCID: 2
 2: 2 - SLURM_LOCALID: 2
 2: 2 - SLURM_TASK_PID: 13799
 2: 2 - Master address: dgx-1
 2: 2 - Master port   : 12223
 2: 2 - Number of nodes: 2
 2: 2 - Node ID        : 0
 2: 2 - Local rank     : 2
 2: 2 - Global rank    : 2
 2: 2 - World size     : 16
 2: 2 - GPUs per node  : 8
 2: 2 - Master         : False
 2: 2 - Multi-node     : True
 2: 2 - Multi-GPU      : True
 2: 2 - Hostname       : dgx-1.svail.baidu.com
 2: Initializing PyTorch distributed ...
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 2: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: SLURM job: True
15: 15 - SLURM_JOB_ID: 2186734
15: 15 - SLURM_JOB_NODELIST: dgx-[1-2]
15: 15 - SLURM_JOB_NUM_NODES: 2
15: 15 - SLURM_NTASKS: 16
15: 15 - SLURM_TASKS_PER_NODE: 8(x2)
15: 15 - SLURM_MEM_PER_NODE: None
15: 15 - SLURM_MEM_PER_CPU: None
15: 15 - SLURM_NODEID: 1
15: 15 - SLURM_PROCID: 15
15: 15 - SLURM_LOCALID: 7
15: 15 - SLURM_TASK_PID: 56879
15: 15 - Master address: dgx-1
15: 15 - Master port   : 12223
15: 15 - Number of nodes: 2
15: 15 - Node ID        : 1
15: 15 - Local rank     : 7
15: 15 - Global rank    : 15
15: 15 - World size     : 16
15: 15 - GPUs per node  : 8
15: 15 - Master         : False
15: 15 - Multi-node     : True
15: 15 - Multi-GPU      : True
15: 15 - Hostname       : dgx-2.svail.baidu.com
15: Initializing PyTorch distributed ...
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
15: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: SLURM job: True
 7: 7 - SLURM_JOB_ID: 2186734
 7: 7 - SLURM_JOB_NODELIST: dgx-[1-2]
 7: 7 - SLURM_JOB_NUM_NODES: 2
 7: 7 - SLURM_NTASKS: 16
 7: 7 - SLURM_TASKS_PER_NODE: 8(x2)
 7: 7 - SLURM_MEM_PER_NODE: None
 7: 7 - SLURM_MEM_PER_CPU: None
 7: 7 - SLURM_NODEID: 0
 7: 7 - SLURM_PROCID: 7
 7: 7 - SLURM_LOCALID: 7
 7: 7 - SLURM_TASK_PID: 13804
 7: 7 - Master address: dgx-1
 7: 7 - Master port   : 12223
 7: 7 - Number of nodes: 2
 7: 7 - Node ID        : 0
 7: 7 - Local rank     : 7
 7: 7 - Global rank    : 7
 7: 7 - World size     : 16
 7: 7 - GPUs per node  : 8
 7: 7 - Master         : False
 7: 7 - Multi-node     : True
 7: 7 - Multi-GPU      : True
 7: 7 - Hostname       : dgx-1.svail.baidu.com
 7: Initializing PyTorch distributed ...
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 7: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: SLURM job: True
 0: 0 - SLURM_JOB_ID: 2186734
 0: 0 - SLURM_JOB_NODELIST: dgx-[1-2]
 0: 0 - SLURM_JOB_NUM_NODES: 2
 0: 0 - SLURM_NTASKS: 16
 0: 0 - SLURM_TASKS_PER_NODE: 8(x2)
 0: 0 - SLURM_MEM_PER_NODE: None
 0: 0 - SLURM_MEM_PER_CPU: None
 0: 0 - SLURM_NODEID: 0
 0: 0 - SLURM_PROCID: 0
 0: 0 - SLURM_LOCALID: 0
 0: 0 - SLURM_TASK_PID: 13797
 0: 0 - Master address: dgx-1
 0: 0 - Master port   : 12223
 0: 0 - Number of nodes: 2
 0: 0 - Node ID        : 0
 0: 0 - Local rank     : 0
 0: 0 - Global rank    : 0
 0: 0 - World size     : 16
 0: 0 - GPUs per node  : 8
 0: 0 - Master         : True
 0: 0 - Multi-node     : True
 0: 0 - Multi-GPU      : True
 0: 0 - Hostname       : dgx-1.svail.baidu.com
 0: Initializing PyTorch distributed ...
 0: Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.
 0: 
 0: Defaults for this optimization level are:
 0: enabled                : True
 0: opt_level              : O2
 0: cast_model_type        : torch.float16
 0: patch_torch_functions  : False
 0: keep_batchnorm_fp32    : True
 0: master_weights         : True
 0: loss_scale             : dynamic
 0: Processing user overrides (additional kwargs that are not None)...
 0: After processing overrides, optimization options are:
 0: enabled                : True
 0: opt_level              : O2
 0: cast_model_type        : torch.float16
 0: patch_torch_functions  : False
 0: keep_batchnorm_fp32    : True
 0: master_weights         : True
 0: loss_scale             : dynamic
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
 0: Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

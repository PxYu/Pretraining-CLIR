INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 800103.77it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 765160.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 578317.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 148.35it/s]100%|██████████| 192/192 [00:00<00:00, 1557.38it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19364.38it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  9%|▉         | 17/192 [00:00<00:01, 93.59it/s]100%|██████████| 192/192 [00:00<00:00, 1009.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20061.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  9%|▉         | 17/192 [00:00<00:01, 113.31it/s]100%|██████████| 192/192 [00:00<00:00, 1209.83it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19976.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 702022.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 704901.35it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 741305.05it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 651592.98it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 702916.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 691832.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 606990.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 129.01it/s]100%|██████████| 192/192 [00:00<00:00, 1368.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19988.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 108.59it/s]100%|██████████| 192/192 [00:00<00:00, 1162.90it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20262.85it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 130.71it/s]100%|██████████| 192/192 [00:00<00:00, 1383.04it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19735.97it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 141.42it/s]100%|██████████| 192/192 [00:00<00:00, 1485.77it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19359.26it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 139.82it/s]100%|██████████| 192/192 [00:00<00:00, 1474.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19716.16it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 127.13it/s]100%|██████████| 192/192 [00:00<00:00, 1348.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20071.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 130.27it/s]100%|██████████| 192/192 [00:00<00:00, 1377.15it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19265.70it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3636.8 words/s - loss: 0.3747
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3595.1 words/s - loss: 0.3824
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3419.8 words/s - loss: 0.3664
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3212.8 words/s - loss: 0.3673
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3217.0 words/s - loss: 0.3729
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3062.1 words/s - loss: 0.3955
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3125.4 words/s - loss: 0.3481
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2980.6 words/s - loss: 0.3625
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2977.0 words/s - loss: 0.3636
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2827.7 words/s - loss: 0.3571
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3365.2 words/s - loss: 0.2765
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3540.9 words/s - loss: 0.2593
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3038.9 words/s - loss: 0.2450
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3152.8 words/s - loss: 0.2706
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2880.1 words/s - loss: 0.2525
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2640.0 words/s - loss: 0.2491
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2792.7 words/s - loss: 0.2440
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2778.9 words/s - loss: 0.2628
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2550.4 words/s - loss: 0.2314
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2667.1 words/s - loss: 0.2497
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3249.5 words/s - loss: 0.2206
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3193.1 words/s - loss: 0.1923
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3603.8 words/s - loss: 0.2500
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3783.5 words/s - loss: 0.1931
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3491.8 words/s - loss: 0.2150
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3215.0 words/s - loss: 0.1987
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2394.4 words/s - loss: 0.2695
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3224.6 words/s - loss: 0.2322
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2488.9 words/s - loss: 0.1994
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2442.1 words/s - loss: 0.2324
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3240.7 words/s - loss: 0.2173
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3174.5 words/s - loss: 0.2412
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3220.1 words/s - loss: 0.1544
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3289.5 words/s - loss: 0.1808
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3778.1 words/s - loss: 0.2037
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3280.3 words/s - loss: 0.1823
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3053.9 words/s - loss: 0.1788
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2988.9 words/s - loss: 0.2187
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2853.3 words/s - loss: 0.1682
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3806.7 words/s - loss: 0.1801
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2792.4 words/s - loss: 0.1798
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2779.0 words/s - loss: 0.1513
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2971.2 words/s - loss: 0.2023
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2933.2 words/s - loss: 0.1512
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3660.4 words/s - loss: 0.1747
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2696.9 words/s - loss: 0.1985
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3017.8 words/s - loss: 0.1283
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3124.0 words/s - loss: 0.1670
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2776.0 words/s - loss: 0.1572
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3099.4 words/s - loss: 0.1872
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2860.9 words/s - loss: 0.1570
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2897.8 words/s - loss: 0.1790
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3112.8 words/s - loss: 0.1325
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3210.5 words/s - loss: 0.1690
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3161.6 words/s - loss: 0.1045
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3230.4 words/s - loss: 0.1569
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2842.9 words/s - loss: 0.1470
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3216.6 words/s - loss: 0.1540
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3182.5 words/s - loss: 0.1604
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3001.3 words/s - loss: 0.2205
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.331 w/ 96 queries
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3194.6 words/s - loss: 0.1467
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3309.3 words/s - loss: 0.1266
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3113.6 words/s - loss: 0.1669
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3133.7 words/s - loss: 0.1689
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2993.7 words/s - loss: 0.2016
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3005.2 words/s - loss: 0.1580
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3038.6 words/s - loss: 0.1405
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2930.6 words/s - loss: 0.1143
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2879.8 words/s - loss: 0.1417
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2774.5 words/s - loss: 0.1255
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.348 w/ 96 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3904.1 words/s - loss: 0.1513
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3783.2 words/s - loss: 0.1555
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3632.6 words/s - loss: 0.1339
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3363.0 words/s - loss: 0.1211
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3346.3 words/s - loss: 0.1275
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3054.3 words/s - loss: 0.1712
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2999.9 words/s - loss: 0.1368
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2879.7 words/s - loss: 0.1391
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2768.8 words/s - loss: 0.1449
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2333.2 words/s - loss: 0.1303
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.361 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.339 w/ 96 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esde_f2_6_7.txt
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 4191.4 words/s - loss: 0.1302
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3517.5 words/s - loss: 0.1574
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3347.7 words/s - loss: 0.1441
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3286.5 words/s - loss: 0.1154
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3085.6 words/s - loss: 0.1609
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3137.1 words/s - loss: 0.1328
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3060.8 words/s - loss: 0.1426
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2998.7 words/s - loss: 0.1340
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2712.5 words/s - loss: 0.1310
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2447.3 words/s - loss: 0.1525
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.350 w/ 96 queries
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esde_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3942.9 words/s - loss: 0.1349
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3801.6 words/s - loss: 0.1446
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3597.9 words/s - loss: 0.1459
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3176.9 words/s - loss: 0.1338
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3217.2 words/s - loss: 0.1251
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3143.8 words/s - loss: 0.1108
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2815.0 words/s - loss: 0.1128
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2788.5 words/s - loss: 0.1230
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2756.8 words/s - loss: 0.1230
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2379.9 words/s - loss: 0.1213
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.352 w/ 96 queries
INFO:__main__:removing file tmp/mbert_esde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_9.txt
INFO:__main__:0.36009809972611845
INFO:__main__:0.3390199143932697
INFO:__main__:best MAP: 0.350
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 787603.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 137.84it/s]100%|██████████| 192/192 [00:00<00:00, 1380.91it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19967.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 695872.85it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684225.77it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 699773.77it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 677812.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 696450.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 720324.24it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 688968.76it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 668265.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 687838.89it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 137.57it/s]100%|██████████| 192/192 [00:00<00:00, 1377.75it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19601.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 128.31it/s]100%|██████████| 192/192 [00:00<00:00, 1289.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 127.92it/s]100%|██████████| 192/192 [00:00<00:00, 19533.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1286.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19339.73it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 135.55it/s]  9%|▉         | 18/192 [00:00<00:01, 140.01it/s]100%|██████████| 192/192 [00:00<00:00, 1358.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1399.76it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 131.85it/s]100%|██████████| 192/192 [00:00<00:00, 19758.24it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1324.82it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
100%|██████████| 192/192 [00:00<00:00, 19635.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19863.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 108.39it/s]100%|██████████| 192/192 [00:00<00:00, 1100.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20013.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 134.39it/s]100%|██████████| 192/192 [00:00<00:00, 1346.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19613.87it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 111.76it/s]100%|██████████| 192/192 [00:00<00:00, 1131.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19399.36it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3663.5 words/s - loss: 0.2854
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3331.7 words/s - loss: 0.2580
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3306.9 words/s - loss: 0.2702
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3186.4 words/s - loss: 0.2633
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3205.9 words/s - loss: 0.2808
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3153.7 words/s - loss: 0.2602
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2985.1 words/s - loss: 0.2676
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2636.5 words/s - loss: 0.2690
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2617.2 words/s - loss: 0.2676
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2564.7 words/s - loss: 0.2410
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3373.1 words/s - loss: 0.1738
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3480.3 words/s - loss: 0.1624
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3152.0 words/s - loss: 0.1453
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2979.1 words/s - loss: 0.1504
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3041.1 words/s - loss: 0.1631
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2919.6 words/s - loss: 0.1522
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3417.6 words/s - loss: 0.1718
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3506.7 words/s - loss: 0.1270
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3260.1 words/s - loss: 0.1479
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2634.5 words/s - loss: 0.1706
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3356.8 words/s - loss: 0.1705
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3800.2 words/s - loss: 0.1247
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 4064.6 words/s - loss: 0.1309
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3125.3 words/s - loss: 0.1264
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3519.3 words/s - loss: 0.1469
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3607.0 words/s - loss: 0.1329
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3184.4 words/s - loss: 0.1216
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2944.0 words/s - loss: 0.1367
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2806.4 words/s - loss: 0.1320
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2662.4 words/s - loss: 0.1660
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3488.4 words/s - loss: 0.1513
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3232.9 words/s - loss: 0.1325
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3869.0 words/s - loss: 0.1305
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3974.7 words/s - loss: 0.1151
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3382.6 words/s - loss: 0.1100
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2745.2 words/s - loss: 0.1391
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2830.4 words/s - loss: 0.1231
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2964.8 words/s - loss: 0.1442
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3276.6 words/s - loss: 0.1317
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2655.8 words/s - loss: 0.1060
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3479.5 words/s - loss: 0.1283
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2863.5 words/s - loss: 0.1219
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3070.4 words/s - loss: 0.1069
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2770.2 words/s - loss: 0.1310
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2703.1 words/s - loss: 0.1351
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2655.9 words/s - loss: 0.1226
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3774.0 words/s - loss: 0.1282
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2484.5 words/s - loss: 0.1175
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2546.1 words/s - loss: 0.1214
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3315.7 words/s - loss: 0.1041
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3387.8 words/s - loss: 0.1007
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3159.6 words/s - loss: 0.1294
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3493.1 words/s - loss: 0.1108
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2821.6 words/s - loss: 0.1038
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3903.9 words/s - loss: 0.1194
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3289.7 words/s - loss: 0.1176
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2913.3 words/s - loss: 0.0863
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2735.9 words/s - loss: 0.1476
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3694.4 words/s - loss: 0.1171
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2517.3 words/s - loss: 0.0994
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3631.1 words/s - loss: 0.1269
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3500.9 words/s - loss: 0.1240
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3304.3 words/s - loss: 0.1305
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3147.7 words/s - loss: 0.1140
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3092.0 words/s - loss: 0.0980
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2931.3 words/s - loss: 0.0962
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2847.2 words/s - loss: 0.1357
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2865.2 words/s - loss: 0.1266
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2878.4 words/s - loss: 0.0983
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2738.9 words/s - loss: 0.1080
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.423 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.437 w/ 96 queries
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 4431.2 words/s - loss: 0.1159
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3955.2 words/s - loss: 0.0972
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3876.3 words/s - loss: 0.1234
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3358.6 words/s - loss: 0.1214
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3165.8 words/s - loss: 0.1117
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3009.2 words/s - loss: 0.1085
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2929.1 words/s - loss: 0.1180
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2921.2 words/s - loss: 0.1147
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2880.7 words/s - loss: 0.0830
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2675.1 words/s - loss: 0.1129
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3642.0 words/s - loss: 0.1169
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3408.6 words/s - loss: 0.1017
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3105.7 words/s - loss: 0.0947
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3180.1 words/s - loss: 0.1071
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2944.2 words/s - loss: 0.1212
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2939.8 words/s - loss: 0.1027
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2928.2 words/s - loss: 0.1324
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2872.4 words/s - loss: 0.1168
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2874.8 words/s - loss: 0.1005
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2682.4 words/s - loss: 0.1185
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esde_f2_9_8.txt
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esde_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4306.0 words/s - loss: 0.1031
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3721.8 words/s - loss: 0.0813
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3266.9 words/s - loss: 0.0876
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3260.7 words/s - loss: 0.1024
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3097.0 words/s - loss: 0.1470
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3208.5 words/s - loss: 0.1013
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3095.2 words/s - loss: 0.1056
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3009.7 words/s - loss: 0.1003
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2717.8 words/s - loss: 0.1110
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2690.4 words/s - loss: 0.0917
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.436 w/ 96 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.439 w/ 96 queries
INFO:__main__:removing file tmp/mbert_esde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_9.txt
INFO:__main__:0.42811165914730753
INFO:__main__:0.43930273174075934
INFO:__main__:best MAP: 0.434
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 687681.01it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 707111.74it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 690625.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 589866.40it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 335565.80it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 701670.24it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 702798.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 672659.97it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 597564.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 688584.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 133.65it/s]100%|██████████| 192/192 [00:00<00:00, 1339.59it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 112.82it/s]100%|██████████| 192/192 [00:00<00:00, 19727.26it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1143.30it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19628.21it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 111.80it/s]  9%|▉         | 18/192 [00:00<00:01, 112.00it/s]100%|██████████| 192/192 [00:00<00:00, 1131.87it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 106.37it/s]100%|██████████| 192/192 [00:00<00:00, 1131.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 119.19it/s]100%|██████████| 192/192 [00:00<00:00, 19422.76it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1080.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 102.91it/s]100%|██████████| 192/192 [00:00<00:00, 1204.23it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19167.12it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19647.37it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1044.78it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 20097.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 18684.60it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 114.04it/s]100%|██████████| 192/192 [00:00<00:00, 1154.00it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19736.93it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 118.26it/s]100%|██████████| 192/192 [00:00<00:00, 1192.36it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18864.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 117.76it/s]100%|██████████| 192/192 [00:00<00:00, 1188.76it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19665.12it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3995.5 words/s - loss: 0.2547
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3590.1 words/s - loss: 0.2361
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3199.8 words/s - loss: 0.2901
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3233.5 words/s - loss: 0.2583
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3026.3 words/s - loss: 0.2724
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3026.1 words/s - loss: 0.2813
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2972.7 words/s - loss: 0.2664
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2766.5 words/s - loss: 0.2386
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2690.0 words/s - loss: 0.2571
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2662.9 words/s - loss: 0.2566
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3344.3 words/s - loss: 0.1516
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3393.3 words/s - loss: 0.1784
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3426.7 words/s - loss: 0.1751
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3818.6 words/s - loss: 0.1489
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3366.6 words/s - loss: 0.1534
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3230.9 words/s - loss: 0.1343
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3530.5 words/s - loss: 0.1251
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2812.9 words/s - loss: 0.1594
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2678.4 words/s - loss: 0.1720
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2744.0 words/s - loss: 0.1488
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3375.9 words/s - loss: 0.1451
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3146.5 words/s - loss: 0.1421
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3513.5 words/s - loss: 0.1458
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3383.3 words/s - loss: 0.1429
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3418.9 words/s - loss: 0.1313
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2994.7 words/s - loss: 0.1261
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2975.5 words/s - loss: 0.1439
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3174.4 words/s - loss: 0.1433
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2831.0 words/s - loss: 0.1559
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3281.7 words/s - loss: 0.1281
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3626.4 words/s - loss: 0.1243
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4128.4 words/s - loss: 0.1130
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3394.2 words/s - loss: 0.1149
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3882.9 words/s - loss: 0.1249
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3582.8 words/s - loss: 0.1068
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3921.0 words/s - loss: 0.1405
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2725.2 words/s - loss: 0.1279
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2919.7 words/s - loss: 0.1316
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3439.1 words/s - loss: 0.1336
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3038.7 words/s - loss: 0.1098
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3633.1 words/s - loss: 0.1591
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3909.4 words/s - loss: 0.1164
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3667.4 words/s - loss: 0.1119
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3048.7 words/s - loss: 0.1078
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3506.7 words/s - loss: 0.0992
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2841.5 words/s - loss: 0.1069
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2936.9 words/s - loss: 0.1238
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3082.2 words/s - loss: 0.1306
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3967.7 words/s - loss: 0.1303
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2691.2 words/s - loss: 0.1488
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2448.9 words/s - loss: 0.1208
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2972.7 words/s - loss: 0.1312
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2274.4 words/s - loss: 0.1137
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2578.5 words/s - loss: 0.1278
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3211.9 words/s - loss: 0.0952
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3229.5 words/s - loss: 0.1449
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3531.1 words/s - loss: 0.1409
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2701.2 words/s - loss: 0.1000
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3313.3 words/s - loss: 0.1467
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2789.7 words/s - loss: 0.1157
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.428 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 4325.3 words/s - loss: 0.1141
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3779.8 words/s - loss: 0.1049
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3437.1 words/s - loss: 0.1120
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3218.4 words/s - loss: 0.1034
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3151.5 words/s - loss: 0.1053
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3147.4 words/s - loss: 0.1200
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2957.5 words/s - loss: 0.1229
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2837.3 words/s - loss: 0.0924
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2758.5 words/s - loss: 0.0872
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2594.5 words/s - loss: 0.0877
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3702.9 words/s - loss: 0.0978
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3265.2 words/s - loss: 0.0983
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3293.1 words/s - loss: 0.1152
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3312.8 words/s - loss: 0.1098
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3086.1 words/s - loss: 0.1047
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3011.6 words/s - loss: 0.1201
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2926.8 words/s - loss: 0.1050
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2956.8 words/s - loss: 0.0968
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2909.9 words/s - loss: 0.1230
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2845.5 words/s - loss: 0.0970
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.435 w/ 96 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.455 w/ 96 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4029.0 words/s - loss: 0.1173
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3818.9 words/s - loss: 0.1150
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3446.1 words/s - loss: 0.1108
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3294.1 words/s - loss: 0.0904
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3178.8 words/s - loss: 0.1007
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3176.3 words/s - loss: 0.1070
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2963.2 words/s - loss: 0.0937
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2924.2 words/s - loss: 0.1017
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2615.9 words/s - loss: 0.0971
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2464.2 words/s - loss: 0.0997
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.418 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_8.txt
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esde_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3946.5 words/s - loss: 0.1012
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3791.7 words/s - loss: 0.0907
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3636.9 words/s - loss: 0.1081
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3501.8 words/s - loss: 0.0851
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3120.2 words/s - loss: 0.1036
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3240.9 words/s - loss: 0.0938
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3155.9 words/s - loss: 0.1084
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3067.8 words/s - loss: 0.0997
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2955.8 words/s - loss: 0.1002
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2830.5 words/s - loss: 0.1239
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.421 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.442 w/ 96 queries
INFO:__main__:removing file tmp/mbert_esde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_9.txt
Traceback (most recent call last):
  File "finetune-search.py", line 580, in <module>
    clir.run()
  File "finetune-search.py", line 377, in run
    logger.info(self.f1_maps)
NameError: name 'f1_maps' is not defined
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', '--dataset', 'mix', '--source_lang', 'es', '--target_lang', 'de', '--batch_size', '16', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '4', '--seed', '611']' returned non-zero exit status 1.

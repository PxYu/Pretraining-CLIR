INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 863843.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 914588.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 858468.21it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 858960.48it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 817603.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 819904.61it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 879751.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 815029.34it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 44%|████▍     | 69/156 [00:00<00:00, 687.95it/s]100%|██████████| 156/156 [00:00<00:00, 1486.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19663.75it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1673.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20497.84it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:01, 132.31it/s]100%|██████████| 156/156 [00:00<00:00, 865.05it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19686.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 23%|██▎       | 36/156 [00:00<00:00, 359.94it/s]100%|██████████| 156/156 [00:00<00:00, 1468.35it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20562.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 133.06it/s]100%|██████████| 156/156 [00:00<00:00, 869.89it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19979.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 135.89it/s]100%|██████████| 156/156 [00:00<00:00, 887.25it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20400.06it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 51%|█████▏    | 80/156 [00:00<00:00, 799.72it/s]100%|██████████| 156/156 [00:00<00:00, 1499.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 21167.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 221.15it/s]100%|██████████| 156/156 [00:00<00:00, 1414.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 21706.91it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 5341.2 words/s - loss: 0.5228
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4920.3 words/s - loss: 0.5077
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4490.7 words/s - loss: 0.5014
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4471.1 words/s - loss: 0.5171
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4452.5 words/s - loss: 0.4881
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4318.3 words/s - loss: 0.4699
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 4162.5 words/s - loss: 0.5048
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4210.3 words/s - loss: 0.5032
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4354.3 words/s - loss: 0.2737
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 5084.1 words/s - loss: 0.2831
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4699.9 words/s - loss: 0.2544
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4775.8 words/s - loss: 0.2742
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4579.5 words/s - loss: 0.2693
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3775.9 words/s - loss: 0.2660
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4359.8 words/s - loss: 0.2896
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4409.7 words/s - loss: 0.2716
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 4955.7 words/s - loss: 0.2439
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4908.3 words/s - loss: 0.2572
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4160.7 words/s - loss: 0.2481
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 3975.6 words/s - loss: 0.2350
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4652.8 words/s - loss: 0.2360
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4601.5 words/s - loss: 0.2507
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4210.1 words/s - loss: 0.2546
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4230.8 words/s - loss: 0.2442
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 5092.7 words/s - loss: 0.2585
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4746.3 words/s - loss: 0.2402
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4190.0 words/s - loss: 0.2023
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4575.6 words/s - loss: 0.2409
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4216.2 words/s - loss: 0.2090
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4712.6 words/s - loss: 0.2126
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4746.1 words/s - loss: 0.2228
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4010.8 words/s - loss: 0.2122
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4687.0 words/s - loss: 0.2207
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 4962.0 words/s - loss: 0.2247
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4194.0 words/s - loss: 0.2231
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4135.7 words/s - loss: 0.2114
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4521.5 words/s - loss: 0.1996
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4490.9 words/s - loss: 0.1818
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4176.5 words/s - loss: 0.2331
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 4194.8 words/s - loss: 0.2200
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4581.3 words/s - loss: 0.2099
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4319.4 words/s - loss: 0.1966
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4581.2 words/s - loss: 0.2127
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4813.1 words/s - loss: 0.1936
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 4702.3 words/s - loss: 0.2196
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 4606.3 words/s - loss: 0.2031
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3807.6 words/s - loss: 0.2017
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 4200.2 words/s - loss: 0.1814
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.456 w/ 78 queries
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 5348.1 words/s - loss: 0.1875
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 4998.8 words/s - loss: 0.1977
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4593.8 words/s - loss: 0.1983
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4417.6 words/s - loss: 0.1988
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4341.3 words/s - loss: 0.2287
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4197.5 words/s - loss: 0.1967
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3876.8 words/s - loss: 0.1733
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3594.8 words/s - loss: 0.1991
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.420 w/ 78 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4912.4 words/s - loss: 0.1832
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 4854.6 words/s - loss: 0.1872
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4721.8 words/s - loss: 0.2223
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 4576.5 words/s - loss: 0.2060
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 4483.5 words/s - loss: 0.1663
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4152.8 words/s - loss: 0.1953
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 4210.8 words/s - loss: 0.1933
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3803.2 words/s - loss: 0.2014
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.425 w/ 78 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 5397.0 words/s - loss: 0.1926
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 5329.0 words/s - loss: 0.1889
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4787.7 words/s - loss: 0.1993
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 4843.6 words/s - loss: 0.1717
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4540.8 words/s - loss: 0.2143
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4338.1 words/s - loss: 0.1844
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4296.5 words/s - loss: 0.1684
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4248.1 words/s - loss: 0.2043
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.353 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 5175.5 words/s - loss: 0.1869
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 4684.3 words/s - loss: 0.2021
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4592.5 words/s - loss: 0.1810
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4632.4 words/s - loss: 0.1751
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4366.3 words/s - loss: 0.1941
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4335.3 words/s - loss: 0.1814
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 3963.6 words/s - loss: 0.1941
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3897.9 words/s - loss: 0.1975
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.358 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.438 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:[0.3904838879257476, 0.3631748841224912, 0.3626635145822434, 0.3533196313724206, 0.3584579685628268]
INFO:__main__:[0.45605325086698856, 0.41982496011454473, 0.4254654177872357, 0.4309494557943282, 0.4383167073031717]
INFO:__main__:0.3904838879257476
INFO:__main__:0.45605325086698856
INFO:__main__:best MAP: 0.423
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 920166.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 782840.72it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 824028.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 809211.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 835319.05it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 803383.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 865483.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 642293.34it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 29%|██▉       | 45/156 [00:00<00:00, 449.93it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1475.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19930.29it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 215.98it/s] 17%|█▋        | 27/156 [00:00<00:00, 193.84it/s]100%|██████████| 156/156 [00:00<00:00, 1187.44it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1071.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18712.79it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
100%|██████████| 156/156 [00:00<00:00, 18901.99it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 137.02it/s]100%|██████████| 156/156 [00:00<00:00, 767.59it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19529.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 138.97it/s]100%|██████████| 156/156 [00:00<00:00, 778.46it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20038.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 145.04it/s]100%|██████████| 156/156 [00:00<00:00, 810.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19617.18it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 226.70it/s]100%|██████████| 156/156 [00:00<00:00, 1242.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19267.12it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 225.75it/s]100%|██████████| 156/156 [00:00<00:00, 1240.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19854.69it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 5260.0 words/s - loss: 0.2858
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4608.8 words/s - loss: 0.2996
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4559.9 words/s - loss: 0.2996
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4487.6 words/s - loss: 0.3174
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 4408.3 words/s - loss: 0.2936
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4310.8 words/s - loss: 0.2866
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4335.7 words/s - loss: 0.3039
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4191.3 words/s - loss: 0.2706
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4535.8 words/s - loss: 0.2075
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4714.4 words/s - loss: 0.2330
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4691.0 words/s - loss: 0.1863
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4521.1 words/s - loss: 0.2420
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4230.6 words/s - loss: 0.2266
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4707.0 words/s - loss: 0.2327
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4380.0 words/s - loss: 0.2156
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 4126.1 words/s - loss: 0.2414
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4884.0 words/s - loss: 0.2067
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4905.8 words/s - loss: 0.2011
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4616.4 words/s - loss: 0.1871
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4944.9 words/s - loss: 0.2187
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4267.0 words/s - loss: 0.2018
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4555.8 words/s - loss: 0.2079
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4268.5 words/s - loss: 0.2315
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3551.3 words/s - loss: 0.2004
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4573.5 words/s - loss: 0.1987
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 4833.3 words/s - loss: 0.2078
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 5234.4 words/s - loss: 0.1838
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4841.5 words/s - loss: 0.1757
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4886.7 words/s - loss: 0.2025
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4447.1 words/s - loss: 0.1885
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4404.8 words/s - loss: 0.1998
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4338.3 words/s - loss: 0.1743
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 5101.5 words/s - loss: 0.1714
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4645.4 words/s - loss: 0.1789
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4491.6 words/s - loss: 0.1942
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4110.7 words/s - loss: 0.1777
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 4332.4 words/s - loss: 0.1818
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4666.3 words/s - loss: 0.1515
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4473.8 words/s - loss: 0.1692
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4734.3 words/s - loss: 0.1786
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4326.6 words/s - loss: 0.1759
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4614.0 words/s - loss: 0.1788
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4766.7 words/s - loss: 0.1851
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 4408.3 words/s - loss: 0.1924
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 4286.5 words/s - loss: 0.1644
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4790.5 words/s - loss: 0.1634
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 4571.1 words/s - loss: 0.1686
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4106.5 words/s - loss: 0.1611
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.507 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 5100.7 words/s - loss: 0.1850
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 4946.1 words/s - loss: 0.1637
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 4591.1 words/s - loss: 0.1719
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4503.5 words/s - loss: 0.1606
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4407.0 words/s - loss: 0.1854
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4343.0 words/s - loss: 0.1573
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4321.0 words/s - loss: 0.1757
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4322.9 words/s - loss: 0.1465
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 5310.5 words/s - loss: 0.1519
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 4924.9 words/s - loss: 0.1630
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4960.4 words/s - loss: 0.1987
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4693.6 words/s - loss: 0.1767
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 4431.9 words/s - loss: 0.1916
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4292.9 words/s - loss: 0.1483
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 4225.8 words/s - loss: 0.1559
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3945.5 words/s - loss: 0.1708
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 5433.6 words/s - loss: 0.1581
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4635.8 words/s - loss: 0.1732
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4550.5 words/s - loss: 0.1535
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4342.1 words/s - loss: 0.1591
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4347.9 words/s - loss: 0.1431
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 4118.0 words/s - loss: 0.1520
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 3986.4 words/s - loss: 0.1736
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3933.4 words/s - loss: 0.1463
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.497 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4983.7 words/s - loss: 0.1535
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4709.8 words/s - loss: 0.1519
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4605.3 words/s - loss: 0.1767
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4507.6 words/s - loss: 0.1751
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4477.7 words/s - loss: 0.1576
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4410.5 words/s - loss: 0.1582
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4212.0 words/s - loss: 0.1331
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3858.9 words/s - loss: 0.1785
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.493 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.552 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:[0.5066381180469589, 0.5019888326250997, 0.5093231712220087, 0.4970006588439074, 0.49314837994947214]
INFO:__main__:[0.556236468868148, 0.5565676011180437, 0.5571892143990815, 0.5485851843828601, 0.552046341878912]
INFO:__main__:0.5093231712220087
INFO:__main__:0.5571892143990815
INFO:__main__:best MAP: 0.533
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 917830.98it/s]
100%|██████████| 5000/5000 [00:00<00:00, 899447.59it/s]INFO:root:Number of positive query-document pairs in [train] set: 5000

INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 824514.25it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 818528.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 908959.78it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 880416.46it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 786658.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 744040.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 206.04it/s] 17%|█▋        | 27/156 [00:00<00:00, 202.95it/s] 17%|█▋        | 27/156 [00:00<00:00, 200.24it/s] 17%|█▋        | 27/156 [00:00<00:00, 200.18it/s]100%|██████████| 156/156 [00:00<00:00, 1135.25it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1119.32it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1104.99it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 1103.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 246.86it/s]100%|██████████| 156/156 [00:00<00:00, 18297.30it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18320.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18945.78it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18635.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1337.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 17560.22it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 266.37it/s]100%|██████████| 156/156 [00:00<00:00, 1447.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19351.46it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:01, 123.33it/s]100%|██████████| 156/156 [00:00<00:00, 691.37it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:01, 127.45it/s]100%|██████████| 156/156 [00:00<00:00, 19031.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 715.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 19621.30it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 5116.7 words/s - loss: 0.3182
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4828.1 words/s - loss: 0.3059
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4770.4 words/s - loss: 0.3077
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4493.7 words/s - loss: 0.3118
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4507.2 words/s - loss: 0.3305
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4287.6 words/s - loss: 0.3016
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4114.3 words/s - loss: 0.3070
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 4138.5 words/s - loss: 0.3150
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 5092.1 words/s - loss: 0.2310
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4984.1 words/s - loss: 0.2286
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4977.1 words/s - loss: 0.2176
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 5085.6 words/s - loss: 0.2364
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4551.1 words/s - loss: 0.2118
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4696.7 words/s - loss: 0.1970
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4004.1 words/s - loss: 0.1970
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4095.9 words/s - loss: 0.2117
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4726.0 words/s - loss: 0.2075
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4435.3 words/s - loss: 0.2024
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4642.3 words/s - loss: 0.1984
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4358.3 words/s - loss: 0.2066
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4367.2 words/s - loss: 0.1918
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4689.7 words/s - loss: 0.2015
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3926.8 words/s - loss: 0.2249
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4261.4 words/s - loss: 0.1807
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4431.2 words/s - loss: 0.1845
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4630.7 words/s - loss: 0.2022
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4620.3 words/s - loss: 0.1806
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 5028.3 words/s - loss: 0.1773
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4659.8 words/s - loss: 0.1748
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4333.1 words/s - loss: 0.1933
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 4499.0 words/s - loss: 0.1929
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3943.0 words/s - loss: 0.1738
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 5096.7 words/s - loss: 0.1648
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4668.3 words/s - loss: 0.1795
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 4955.5 words/s - loss: 0.1897
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4083.5 words/s - loss: 0.1728
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4392.5 words/s - loss: 0.1847
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4398.8 words/s - loss: 0.1946
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4043.1 words/s - loss: 0.1750
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4803.2 words/s - loss: 0.1817
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 5860.3 words/s - loss: 0.1770
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4484.7 words/s - loss: 0.1950
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 4851.9 words/s - loss: 0.1572
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4844.5 words/s - loss: 0.1526
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 5158.6 words/s - loss: 0.1922
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 4402.6 words/s - loss: 0.1540
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4662.9 words/s - loss: 0.1593
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4062.0 words/s - loss: 0.1664
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.501 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 5090.1 words/s - loss: 0.1504
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4914.4 words/s - loss: 0.1533
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4811.2 words/s - loss: 0.1947
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4572.2 words/s - loss: 0.1685
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4334.5 words/s - loss: 0.1796
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4272.7 words/s - loss: 0.1597
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3743.2 words/s - loss: 0.1662
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3714.9 words/s - loss: 0.1647
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 5176.2 words/s - loss: 0.1498
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 5159.7 words/s - loss: 0.1737
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 5018.4 words/s - loss: 0.1570
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 4999.2 words/s - loss: 0.1481
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4855.2 words/s - loss: 0.1593
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 4479.7 words/s - loss: 0.1534
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 4296.7 words/s - loss: 0.1535
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 4124.5 words/s - loss: 0.1432
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.550 w/ 78 queries
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4761.7 words/s - loss: 0.1691
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4854.0 words/s - loss: 0.1714
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 4728.4 words/s - loss: 0.1568
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 4639.0 words/s - loss: 0.1466
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4394.9 words/s - loss: 0.1589
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4396.5 words/s - loss: 0.1614
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4244.6 words/s - loss: 0.1811
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4091.1 words/s - loss: 0.1514
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4975.1 words/s - loss: 0.1190
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 4918.5 words/s - loss: 0.1771
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4798.3 words/s - loss: 0.1675
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4524.0 words/s - loss: 0.1591
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4540.7 words/s - loss: 0.1384
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4501.0 words/s - loss: 0.1453
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4289.5 words/s - loss: 0.1465
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4152.8 words/s - loss: 0.1455
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.498 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:[0.501362767631093, 0.4985338109535893, 0.49163602448813204, 0.5019223883355683, 0.49815110154407743]
INFO:__main__:[0.5517707269165211, 0.5527784223544953, 0.5498575050328921, 0.5512375616791143, 0.555684081524283]
INFO:__main__:0.49815110154407743
INFO:__main__:0.5512375616791143
INFO:__main__:best MAP: 0.525

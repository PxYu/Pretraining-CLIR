INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 715751.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 919884.20it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 892177.32it/s]
100%|██████████| 5000/5000 [00:00<00:00, 878977.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 845216.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 835751.80it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 815980.70it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 807559.78it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 218.43it/s]100%|██████████| 156/156 [00:00<00:00, 1394.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20330.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 203.69it/s] 15%|█▍        | 23/156 [00:00<00:00, 201.03it/s] 15%|█▍        | 23/156 [00:00<00:00, 200.61it/s]100%|██████████| 156/156 [00:00<00:00, 1303.59it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1287.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 1284.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 188.44it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19405.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 1207.22it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19560.29it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18988.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18513.71it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 136.05it/s]100%|██████████| 156/156 [00:00<00:00, 887.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19893.93it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:01, 127.87it/s]100%|██████████| 156/156 [00:00<00:00, 835.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19637.20it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:01, 130.90it/s]100%|██████████| 156/156 [00:00<00:00, 855.77it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20094.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4967.1 words/s - loss: 0.3446
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4919.2 words/s - loss: 0.3607
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4906.6 words/s - loss: 0.3659
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4418.2 words/s - loss: 0.3874
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4296.2 words/s - loss: 0.3675
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4115.8 words/s - loss: 0.3852
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 3964.6 words/s - loss: 0.3771
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3820.9 words/s - loss: 0.3560
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4565.3 words/s - loss: 0.2529
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4482.5 words/s - loss: 0.2607
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4413.2 words/s - loss: 0.2523
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4780.7 words/s - loss: 0.2583
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4128.9 words/s - loss: 0.2637
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 4391.7 words/s - loss: 0.2877
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4332.6 words/s - loss: 0.2459
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4086.3 words/s - loss: 0.2576
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4455.2 words/s - loss: 0.2178
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4404.0 words/s - loss: 0.2183
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4493.7 words/s - loss: 0.2372
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4499.2 words/s - loss: 0.2490
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4532.2 words/s - loss: 0.2030
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4693.9 words/s - loss: 0.2354
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4797.6 words/s - loss: 0.2402
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3731.6 words/s - loss: 0.2109
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4950.6 words/s - loss: 0.2475
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4928.8 words/s - loss: 0.2578
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 4299.8 words/s - loss: 0.2489
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 5185.1 words/s - loss: 0.2366
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4665.3 words/s - loss: 0.2264
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4258.7 words/s - loss: 0.2134
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 3742.2 words/s - loss: 0.1869
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4303.0 words/s - loss: 0.1926
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 4838.0 words/s - loss: 0.1979
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4927.7 words/s - loss: 0.1876
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 5376.2 words/s - loss: 0.2046
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3701.5 words/s - loss: 0.2232
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4548.1 words/s - loss: 0.1963
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4585.3 words/s - loss: 0.1833
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4558.2 words/s - loss: 0.2245
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4190.8 words/s - loss: 0.1946
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4473.1 words/s - loss: 0.1848
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 4883.6 words/s - loss: 0.1879
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4204.6 words/s - loss: 0.1981
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4909.8 words/s - loss: 0.1786
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4042.5 words/s - loss: 0.2407
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 4401.9 words/s - loss: 0.2042
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 4257.3 words/s - loss: 0.1808
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4378.4 words/s - loss: 0.1726
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.419 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 5409.1 words/s - loss: 0.1960
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4396.5 words/s - loss: 0.1844
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4483.1 words/s - loss: 0.1903
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4457.0 words/s - loss: 0.1910
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4382.8 words/s - loss: 0.1865
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4151.2 words/s - loss: 0.1884
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3859.9 words/s - loss: 0.1823
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3824.5 words/s - loss: 0.1765
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 4944.4 words/s - loss: 0.1694
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4779.2 words/s - loss: 0.1719
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4685.4 words/s - loss: 0.2229
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 4569.5 words/s - loss: 0.1928
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 4527.1 words/s - loss: 0.1808
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4510.8 words/s - loss: 0.1594
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 4415.3 words/s - loss: 0.1648
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3985.9 words/s - loss: 0.2054
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.430 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.502 w/ 78 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 5519.6 words/s - loss: 0.1990
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 5109.5 words/s - loss: 0.1841
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4701.8 words/s - loss: 0.1560
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4741.5 words/s - loss: 0.1589
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4245.2 words/s - loss: 0.1618
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 4322.3 words/s - loss: 0.1720
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4281.8 words/s - loss: 0.1958
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4017.0 words/s - loss: 0.1577
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.404 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4675.6 words/s - loss: 0.1833
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 4622.9 words/s - loss: 0.1759
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4387.5 words/s - loss: 0.1572
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4401.9 words/s - loss: 0.1815
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4302.6 words/s - loss: 0.1661
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4468.8 words/s - loss: 0.1546
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4361.9 words/s - loss: 0.1769
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 3797.6 words/s - loss: 0.1558
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.413 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:[0.4185783059324841, 0.4106108814173852, 0.42954439400044436, 0.40411466416871084, 0.41296372343050747]
INFO:__main__:[0.4906277312106741, 0.484017718048206, 0.5024753075167495, 0.47363987187479295, 0.49072281366194176]
INFO:__main__:0.42954439400044436
INFO:__main__:0.5024753075167495
INFO:__main__:best MAP: 0.466
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 881267.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 896486.98it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 813417.11it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 882119.96it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 849565.32it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 883457.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 878498.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 765188.46it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 37%|███▋      | 57/156 [00:00<00:00, 569.46it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1481.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19727.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 227.87it/s]100%|██████████| 156/156 [00:00<00:00, 1250.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19569.66it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 227.09it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1246.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19383.56it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 141.87it/s]100%|██████████| 156/156 [00:00<00:00, 792.89it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19340.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 216.11it/s]100%|██████████| 156/156 [00:00<00:00, 1188.32it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 133.82it/s]100%|██████████| 156/156 [00:00<00:00, 749.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19362.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 219.05it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1205.10it/s]
100%|██████████| 156/156 [00:00<00:00, 19189.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 19174.52it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 134.45it/s]100%|██████████| 156/156 [00:00<00:00, 752.93it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19702.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 5038.0 words/s - loss: 0.2873
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4887.3 words/s - loss: 0.3145
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4753.3 words/s - loss: 0.3344
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4592.8 words/s - loss: 0.3264
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4289.1 words/s - loss: 0.2798
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3941.0 words/s - loss: 0.3046
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3795.3 words/s - loss: 0.3068
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 3820.7 words/s - loss: 0.2843
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4822.3 words/s - loss: 0.2403
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4473.9 words/s - loss: 0.2129
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4924.5 words/s - loss: 0.2464
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4210.2 words/s - loss: 0.2288
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3927.3 words/s - loss: 0.2255
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4398.5 words/s - loss: 0.2374
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4627.0 words/s - loss: 0.2035
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3978.3 words/s - loss: 0.2614
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4363.5 words/s - loss: 0.1969
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4897.7 words/s - loss: 0.2018
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4448.6 words/s - loss: 0.1822
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 4269.2 words/s - loss: 0.2078
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 3998.4 words/s - loss: 0.2209
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4630.7 words/s - loss: 0.2257
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4479.0 words/s - loss: 0.2139
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4217.7 words/s - loss: 0.2175
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4736.0 words/s - loss: 0.2200
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4784.9 words/s - loss: 0.1997
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4479.0 words/s - loss: 0.2127
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 5141.9 words/s - loss: 0.1875
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 4782.1 words/s - loss: 0.2280
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4588.5 words/s - loss: 0.2022
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4198.4 words/s - loss: 0.1886
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4206.8 words/s - loss: 0.1952
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4620.1 words/s - loss: 0.1839
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 4561.8 words/s - loss: 0.1669
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4190.8 words/s - loss: 0.1865
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 4592.2 words/s - loss: 0.1520
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4167.9 words/s - loss: 0.1834
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4068.7 words/s - loss: 0.1857
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4448.6 words/s - loss: 0.1954
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4111.6 words/s - loss: 0.1643
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4758.4 words/s - loss: 0.2141
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4559.5 words/s - loss: 0.1714
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 4343.2 words/s - loss: 0.1879
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4800.2 words/s - loss: 0.1854
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 4516.8 words/s - loss: 0.1703
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 4034.9 words/s - loss: 0.1509
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4715.2 words/s - loss: 0.1456
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 5204.2 words/s - loss: 0.1752
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 5707.9 words/s - loss: 0.1827
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 5000.2 words/s - loss: 0.1936
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4723.0 words/s - loss: 0.1645
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4460.7 words/s - loss: 0.1659
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4462.3 words/s - loss: 0.1621
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4192.6 words/s - loss: 0.1594
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4090.3 words/s - loss: 0.1898
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 4058.4 words/s - loss: 0.1611
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 5283.2 words/s - loss: 0.2058
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 4965.9 words/s - loss: 0.1437
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4827.2 words/s - loss: 0.1489
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 4187.8 words/s - loss: 0.1682
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 4130.8 words/s - loss: 0.1909
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3932.0 words/s - loss: 0.1649
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3966.3 words/s - loss: 0.1640
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3749.9 words/s - loss: 0.1623
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 5181.7 words/s - loss: 0.1801
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4694.2 words/s - loss: 0.1543
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4710.5 words/s - loss: 0.1474
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4489.6 words/s - loss: 0.1887
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4218.0 words/s - loss: 0.1444
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 4220.4 words/s - loss: 0.1612
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3797.4 words/s - loss: 0.1209
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 3535.9 words/s - loss: 0.1609
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 5032.3 words/s - loss: 0.1753
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 5078.8 words/s - loss: 0.1727
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4637.6 words/s - loss: 0.1610
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4482.2 words/s - loss: 0.1663
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4482.3 words/s - loss: 0.1450
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 4372.7 words/s - loss: 0.1949
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4349.3 words/s - loss: 0.1331
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4219.7 words/s - loss: 0.1530
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.491 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.558 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:[0.48646895835487397, 0.48589036373309935, 0.47756276688374866, 0.4881651621649402, 0.49101485161772107]
INFO:__main__:[0.5509966643657089, 0.554730329101506, 0.5373841701388595, 0.5543225727575881, 0.5584667971179477]
INFO:__main__:0.49101485161772107
INFO:__main__:0.5584667971179477
INFO:__main__:best MAP: 0.525
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 908723.46it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 831972.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 855666.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 885584.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 815917.21it/s]
100%|██████████| 5000/5000 [00:00<00:00, 885472.05it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 818912.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 826724.48it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 260.13it/s]100%|██████████| 156/156 [00:00<00:00, 1409.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
 17%|█▋        | 27/156 [00:00<00:00, 212.10it/s]  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 208.87it/s]100%|██████████| 156/156 [00:00<00:00, 1167.66it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1147.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17678.84it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18787.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18331.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 233.25it/s]100%|██████████| 156/156 [00:00<00:00, 1273.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18870.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 132.96it/s]100%|██████████| 156/156 [00:00<00:00, 741.32it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19089.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:01, 123.94it/s] 17%|█▋        | 27/156 [00:00<00:00, 213.23it/s]100%|██████████| 156/156 [00:00<00:00, 696.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1174.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19438.26it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 19824.62it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 132.84it/s]100%|██████████| 156/156 [00:00<00:00, 744.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19574.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4892.8 words/s - loss: 0.2926
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4808.2 words/s - loss: 0.3049
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4673.7 words/s - loss: 0.3186
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4610.1 words/s - loss: 0.2948
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4598.1 words/s - loss: 0.2996
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4463.7 words/s - loss: 0.3120
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4407.5 words/s - loss: 0.2998
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3780.8 words/s - loss: 0.3048
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4883.1 words/s - loss: 0.2248
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4703.7 words/s - loss: 0.2224
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4873.4 words/s - loss: 0.2077
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4204.6 words/s - loss: 0.2179
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4380.9 words/s - loss: 0.2210
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 4337.5 words/s - loss: 0.2314
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4170.3 words/s - loss: 0.2148
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3796.1 words/s - loss: 0.2305
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4943.9 words/s - loss: 0.2088
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4428.7 words/s - loss: 0.1954
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4306.6 words/s - loss: 0.2039
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4606.1 words/s - loss: 0.1904
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4616.7 words/s - loss: 0.1978
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4541.9 words/s - loss: 0.2028
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3998.5 words/s - loss: 0.2008
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3786.2 words/s - loss: 0.1922
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4898.2 words/s - loss: 0.1918
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4588.4 words/s - loss: 0.2096
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4636.3 words/s - loss: 0.1971
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4646.4 words/s - loss: 0.2025
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 4187.6 words/s - loss: 0.2042
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4312.8 words/s - loss: 0.2027
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4547.3 words/s - loss: 0.1818
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4623.0 words/s - loss: 0.1811
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4411.1 words/s - loss: 0.2035
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 5109.8 words/s - loss: 0.1777
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4959.7 words/s - loss: 0.1928
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4434.3 words/s - loss: 0.1813
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4351.8 words/s - loss: 0.1619
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4043.6 words/s - loss: 0.1623
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4154.9 words/s - loss: 0.1834
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 4208.4 words/s - loss: 0.1719
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4753.8 words/s - loss: 0.1701
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4950.4 words/s - loss: 0.1986
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3908.2 words/s - loss: 0.1838
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4319.3 words/s - loss: 0.2204
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4675.3 words/s - loss: 0.1623
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 4615.8 words/s - loss: 0.2113
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4725.7 words/s - loss: 0.1608
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3762.1 words/s - loss: 0.1497
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.503 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 4801.2 words/s - loss: 0.1771
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4619.1 words/s - loss: 0.1623
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4487.3 words/s - loss: 0.1878
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 4083.8 words/s - loss: 0.1684
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 4182.3 words/s - loss: 0.1618
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3954.0 words/s - loss: 0.1542
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3900.9 words/s - loss: 0.1893
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3916.6 words/s - loss: 0.1619
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 5124.9 words/s - loss: 0.1562
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 4935.7 words/s - loss: 0.1531
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4778.9 words/s - loss: 0.1897
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4643.7 words/s - loss: 0.1478
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4413.6 words/s - loss: 0.1438
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 4249.6 words/s - loss: 0.1715
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3922.1 words/s - loss: 0.1632
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3765.3 words/s - loss: 0.1567
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 4867.9 words/s - loss: 0.1832
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4886.9 words/s - loss: 0.1715
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4319.7 words/s - loss: 0.1363
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4371.5 words/s - loss: 0.1587
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4209.4 words/s - loss: 0.1552
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4046.3 words/s - loss: 0.1439
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 3806.1 words/s - loss: 0.1567
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3809.3 words/s - loss: 0.1632
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 5449.5 words/s - loss: 0.1606
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4897.7 words/s - loss: 0.1644
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4910.2 words/s - loss: 0.1661
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 4383.9 words/s - loss: 0.1778
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4163.0 words/s - loss: 0.1754
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4103.2 words/s - loss: 0.1683
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4144.5 words/s - loss: 0.1508
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4043.1 words/s - loss: 0.1516
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.556 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:[0.5029491691853382, 0.48922322193953166, 0.48178502054805744, 0.4651735425559956, 0.4827645867839455]
INFO:__main__:[0.5610472934722452, 0.5572920817451303, 0.5567591837120693, 0.5364857621487081, 0.5557460628106264]
INFO:__main__:0.5029491691853382
INFO:__main__:0.5610472934722452
INFO:__main__:best MAP: 0.532

INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 866520.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
100%|██████████| 5000/5000 [00:00<00:00, 811716.98it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 833990.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 744542.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 734631.31it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 854759.32it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 2155.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 21679.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 826594.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 705019.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 159.68it/s]100%|██████████| 192/192 [00:00<00:00, 1677.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 21861.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 145.65it/s]100%|██████████| 192/192 [00:00<00:00, 1538.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 21239.78it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 132.32it/s]100%|██████████| 192/192 [00:00<00:00, 1404.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20867.18it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 106.06it/s]  9%|▉         | 17/192 [00:00<00:01, 105.09it/s]100%|██████████| 192/192 [00:00<00:00, 1140.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1129.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 21193.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 20837.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 133.37it/s]100%|██████████| 192/192 [00:00<00:00, 1419.30it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 21301.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 129.91it/s]100%|██████████| 192/192 [00:00<00:00, 1384.04it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 22364.03it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4034.5 words/s - loss: 0.3433
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3815.4 words/s - loss: 0.3818
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 3752.8 words/s - loss: 0.3355
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3764.8 words/s - loss: 0.3592
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 3504.2 words/s - loss: 0.3529
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3422.4 words/s - loss: 0.3549
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3111.7 words/s - loss: 0.3580
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 2734.6 words/s - loss: 0.3608
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4125.8 words/s - loss: 0.2423
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3716.4 words/s - loss: 0.2269
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 3798.7 words/s - loss: 0.2348
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3751.7 words/s - loss: 0.2232
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3361.4 words/s - loss: 0.2360
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 3426.7 words/s - loss: 0.2317
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3459.6 words/s - loss: 0.2173
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 2915.0 words/s - loss: 0.2543
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4031.1 words/s - loss: 0.1962
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3235.6 words/s - loss: 0.2391
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 3403.7 words/s - loss: 0.2136
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 3266.3 words/s - loss: 0.1974
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3352.4 words/s - loss: 0.2028
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3257.7 words/s - loss: 0.2286
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3502.6 words/s - loss: 0.1731
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4673.8 words/s - loss: 0.1943
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3528.4 words/s - loss: 0.2095
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4316.3 words/s - loss: 0.1809
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3529.4 words/s - loss: 0.1765
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4402.6 words/s - loss: 0.1655
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3398.8 words/s - loss: 0.2025
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 3200.2 words/s - loss: 0.1870
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3118.0 words/s - loss: 0.1980
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3608.4 words/s - loss: 0.1850
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3402.7 words/s - loss: 0.1990
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 3598.1 words/s - loss: 0.1771
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3576.4 words/s - loss: 0.1790
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 3736.6 words/s - loss: 0.1910
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 3823.6 words/s - loss: 0.1510
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3147.3 words/s - loss: 0.1637
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3200.1 words/s - loss: 0.1642
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4003.1 words/s - loss: 0.1727
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4568.9 words/s - loss: 0.1416
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3026.6 words/s - loss: 0.1527
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 2973.4 words/s - loss: 0.1478
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3212.9 words/s - loss: 0.1650
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3598.1 words/s - loss: 0.1616
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 3534.3 words/s - loss: 0.1407
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3201.7 words/s - loss: 0.1499
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3933.0 words/s - loss: 0.1468
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.351 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_frde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 4328.0 words/s - loss: 0.1336
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3749.2 words/s - loss: 0.1853
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3472.2 words/s - loss: 0.1485
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3448.1 words/s - loss: 0.1430
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3459.2 words/s - loss: 0.1537
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3362.0 words/s - loss: 0.1324
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3317.2 words/s - loss: 0.1323
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3021.9 words/s - loss: 0.1535
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.345 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_frde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4653.4 words/s - loss: 0.1374
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 3983.8 words/s - loss: 0.1417
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3929.3 words/s - loss: 0.1677
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 3703.3 words/s - loss: 0.1275
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3636.7 words/s - loss: 0.1384
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3629.0 words/s - loss: 0.1436
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3233.8 words/s - loss: 0.1588
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 2842.5 words/s - loss: 0.1213
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.357 w/ 96 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_frde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4972.3 words/s - loss: 0.1564
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3786.1 words/s - loss: 0.1265
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3651.7 words/s - loss: 0.1292
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3633.7 words/s - loss: 0.1297
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3442.0 words/s - loss: 0.1208
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3283.2 words/s - loss: 0.1575
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3093.8 words/s - loss: 0.1326
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3000.3 words/s - loss: 0.1388
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.356 w/ 96 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.373 w/ 96 queries
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_frde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4089.0 words/s - loss: 0.1308
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4178.3 words/s - loss: 0.1277
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 3700.6 words/s - loss: 0.1138
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 3705.7 words/s - loss: 0.1213
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 3551.9 words/s - loss: 0.1325
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3644.0 words/s - loss: 0.1269
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 3506.2 words/s - loss: 0.1246
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3388.2 words/s - loss: 0.1526
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.376 w/ 96 queries
INFO:__main__:removing file tmp/mbert_frde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_9.txt
INFO:__main__:[0.35102533420816817, 0.3446077675029852, 0.3416007862229724, 0.3563793510094621, 0.3601206581648853]
INFO:__main__:[0.3554305274727884, 0.37044954984891615, 0.35655866715595613, 0.3729726166684886, 0.37551559009612356]
INFO:__main__:0.3601206581648853
INFO:__main__:0.37551559009612356
INFO:__main__:best MAP: 0.368
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 805636.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 817922.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 712759.41it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 792904.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 822122.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 849840.74it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 795822.71it/s]
100%|██████████| 5000/5000 [00:00<00:00, 813164.79it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 133.76it/s]100%|██████████| 192/192 [00:00<00:00, 1346.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 96.75it/s]100%|██████████| 192/192 [00:00<00:00, 20792.29it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 989.83it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 133.73it/s]  9%|▉         | 18/192 [00:00<00:01, 132.30it/s]100%|██████████| 192/192 [00:00<00:00, 20656.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1348.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1333.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 20884.50it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 20506.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 87.19it/s]  9%|▉         | 18/192 [00:00<00:02, 86.94it/s]100%|██████████| 192/192 [00:00<00:00, 895.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 893.43it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 132.29it/s]100%|██████████| 192/192 [00:00<00:00, 21072.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 21004.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1328.00it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19481.96it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:02, 84.87it/s]100%|██████████| 192/192 [00:00<00:00, 872.45it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20850.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3685.4 words/s - loss: 0.2512
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3694.5 words/s - loss: 0.2286
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 3633.8 words/s - loss: 0.2315
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 3429.2 words/s - loss: 0.2298
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3366.0 words/s - loss: 0.2228
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3266.2 words/s - loss: 0.2821
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 3093.6 words/s - loss: 0.2358
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 2998.1 words/s - loss: 0.2332
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4313.3 words/s - loss: 0.1721
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4016.0 words/s - loss: 0.1257
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 3597.4 words/s - loss: 0.1465
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3707.4 words/s - loss: 0.1655
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3558.1 words/s - loss: 0.1581
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 3121.7 words/s - loss: 0.1690
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3538.5 words/s - loss: 0.1521
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3119.6 words/s - loss: 0.1625
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3561.8 words/s - loss: 0.1427
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4317.7 words/s - loss: 0.1517
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3972.9 words/s - loss: 0.1606
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 3852.9 words/s - loss: 0.1588
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3641.7 words/s - loss: 0.1429
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 3794.1 words/s - loss: 0.1474
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3525.7 words/s - loss: 0.1329
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 2903.6 words/s - loss: 0.1557
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4786.5 words/s - loss: 0.1390
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4000.9 words/s - loss: 0.1381
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 3752.8 words/s - loss: 0.1454
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 3898.6 words/s - loss: 0.1353
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3597.1 words/s - loss: 0.1535
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 2860.3 words/s - loss: 0.1738
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4372.4 words/s - loss: 0.1268
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4103.4 words/s - loss: 0.1426
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 2803.9 words/s - loss: 0.1392
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3540.5 words/s - loss: 0.1272
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4291.6 words/s - loss: 0.1339
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 3570.2 words/s - loss: 0.1363
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3307.8 words/s - loss: 0.1148
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 2931.7 words/s - loss: 0.1146
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3086.2 words/s - loss: 0.1185
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3745.5 words/s - loss: 0.1205
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4393.3 words/s - loss: 0.1327
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3597.7 words/s - loss: 0.1283
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 3238.1 words/s - loss: 0.1243
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3603.8 words/s - loss: 0.1277
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3323.2 words/s - loss: 0.1242
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3584.1 words/s - loss: 0.1007
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 3049.9 words/s - loss: 0.1376
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 2831.5 words/s - loss: 0.1236
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.406 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_frde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3904.8 words/s - loss: 0.1182
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3790.8 words/s - loss: 0.1193
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3730.0 words/s - loss: 0.1151
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3483.9 words/s - loss: 0.1215
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3475.0 words/s - loss: 0.1315
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3472.3 words/s - loss: 0.1366
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3458.1 words/s - loss: 0.0982
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3177.0 words/s - loss: 0.1121
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_frde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3861.2 words/s - loss: 0.0976
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3575.3 words/s - loss: 0.1131
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3553.6 words/s - loss: 0.1107
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 3444.1 words/s - loss: 0.1274
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 3426.2 words/s - loss: 0.1044
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3357.5 words/s - loss: 0.1195
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 3285.9 words/s - loss: 0.1134
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3221.0 words/s - loss: 0.0942
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.439 w/ 96 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_frde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4382.2 words/s - loss: 0.1241
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4013.5 words/s - loss: 0.0984
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3880.6 words/s - loss: 0.1171
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3855.2 words/s - loss: 0.0976
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3512.5 words/s - loss: 0.1009
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3431.9 words/s - loss: 0.1095
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3138.3 words/s - loss: 0.1003
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3026.6 words/s - loss: 0.0917
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.432 w/ 96 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_frde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 3771.3 words/s - loss: 0.1163
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 3672.4 words/s - loss: 0.1229
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3695.4 words/s - loss: 0.0993
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3514.0 words/s - loss: 0.1345
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3610.5 words/s - loss: 0.1048
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3254.2 words/s - loss: 0.1404
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 2963.0 words/s - loss: 0.1056
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 2852.4 words/s - loss: 0.1070
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.434 w/ 96 queries
INFO:__main__:removing file tmp/mbert_frde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_9.txt
INFO:__main__:[0.4060304071731455, 0.40751600357535933, 0.40750305708859075, 0.4034136331977029, 0.4074905431191054]
INFO:__main__:[0.4298739630184893, 0.4459219928767131, 0.4385865002832207, 0.43239408232515686, 0.43432958181653153]
INFO:__main__:0.40751600357535933
INFO:__main__:0.4459219928767131
INFO:__main__:best MAP: 0.427
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 875235.59it/s]
100%|██████████| 5000/5000 [00:00<00:00, 853368.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 846103.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 832864.18it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 812125.62it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 705185.78it/s]100%|██████████| 5000/5000 [00:00<00:00, 701787.64it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 777414.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s] 21%|██▏       | 41/192 [00:00<00:00, 409.87it/s]  9%|▉         | 18/192 [00:00<00:01, 136.44it/s]100%|██████████| 192/192 [00:00<00:00, 1794.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 131.06it/s]100%|██████████| 192/192 [00:00<00:00, 1372.36it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 126.47it/s]100%|██████████| 192/192 [00:00<00:00, 21543.20it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1320.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 20939.89it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1270.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 20770.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19127.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 88.03it/s]100%|██████████| 192/192 [00:00<00:00, 904.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 21265.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 121.59it/s]100%|██████████| 192/192 [00:00<00:00, 1228.24it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19789.80it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 89.54it/s]  9%|▉         | 18/192 [00:00<00:01, 89.52it/s]100%|██████████| 192/192 [00:00<00:00, 919.31it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
100%|██████████| 192/192 [00:00<00:00, 918.63it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 21138.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 20914.88it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 5144.2 words/s - loss: 0.2388
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4254.8 words/s - loss: 0.2601
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4031.1 words/s - loss: 0.2467
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3668.1 words/s - loss: 0.2410
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3489.7 words/s - loss: 0.2466
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3436.4 words/s - loss: 0.2502
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3370.4 words/s - loss: 0.2542
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3360.6 words/s - loss: 0.2129
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 3458.6 words/s - loss: 0.1257
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3384.1 words/s - loss: 0.1456
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 3699.9 words/s - loss: 0.1603
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3258.1 words/s - loss: 0.1810
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3592.4 words/s - loss: 0.1778
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3520.4 words/s - loss: 0.1698
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 3329.7 words/s - loss: 0.1671
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 2798.5 words/s - loss: 0.1582
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4045.3 words/s - loss: 0.1430
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3821.0 words/s - loss: 0.1516
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3932.9 words/s - loss: 0.1465
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 3106.6 words/s - loss: 0.1464
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4395.5 words/s - loss: 0.1595
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3585.7 words/s - loss: 0.1364
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3252.5 words/s - loss: 0.1343
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 3559.9 words/s - loss: 0.1536
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 3730.7 words/s - loss: 0.1561
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3460.2 words/s - loss: 0.1353
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 3201.8 words/s - loss: 0.1518
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3422.8 words/s - loss: 0.1447
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3019.4 words/s - loss: 0.1483
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4516.9 words/s - loss: 0.1194
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 3202.6 words/s - loss: 0.1277
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3330.8 words/s - loss: 0.1385
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3730.7 words/s - loss: 0.1273
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3734.8 words/s - loss: 0.1121
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 3265.3 words/s - loss: 0.1314
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3659.3 words/s - loss: 0.1221
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3245.9 words/s - loss: 0.1316
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3406.8 words/s - loss: 0.1226
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 2952.3 words/s - loss: 0.1327
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 3387.4 words/s - loss: 0.1311
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 3936.7 words/s - loss: 0.1327
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3473.8 words/s - loss: 0.1216
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3790.7 words/s - loss: 0.1162
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3725.0 words/s - loss: 0.1190
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3874.8 words/s - loss: 0.1134
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3101.6 words/s - loss: 0.1399
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3030.8 words/s - loss: 0.0972
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 3056.2 words/s - loss: 0.1215
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.407 w/ 96 queries
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_frde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3970.9 words/s - loss: 0.1029
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3942.3 words/s - loss: 0.1175
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3876.3 words/s - loss: 0.1415
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3448.9 words/s - loss: 0.1011
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3316.1 words/s - loss: 0.1261
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3280.1 words/s - loss: 0.1028
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 2944.8 words/s - loss: 0.1043
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 2743.1 words/s - loss: 0.1154
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_frde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4156.7 words/s - loss: 0.1236
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3869.0 words/s - loss: 0.1051
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3731.4 words/s - loss: 0.1222
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 3599.6 words/s - loss: 0.0921
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3558.3 words/s - loss: 0.1114
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3383.3 words/s - loss: 0.1066
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3258.3 words/s - loss: 0.1124
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 3248.2 words/s - loss: 0.1220
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.413 w/ 96 queries
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.457 w/ 96 queries
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_frde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4237.2 words/s - loss: 0.1222
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3786.4 words/s - loss: 0.1094
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 3648.4 words/s - loss: 0.1177
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3352.2 words/s - loss: 0.1236
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3310.1 words/s - loss: 0.0973
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3337.4 words/s - loss: 0.1268
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3078.8 words/s - loss: 0.1225
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 2309.5 words/s - loss: 0.0958
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.453 w/ 96 queries
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_frde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4286.7 words/s - loss: 0.1341
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4061.4 words/s - loss: 0.0952
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3849.1 words/s - loss: 0.0970
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 3549.6 words/s - loss: 0.1041
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3328.4 words/s - loss: 0.1383
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3316.6 words/s - loss: 0.1164
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 3215.7 words/s - loss: 0.1171
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 2832.5 words/s - loss: 0.1031
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.398 w/ 96 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_frde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_frde_f1_2_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:removing file tmp/mbert_frde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_frde_f2_5_9.txt
INFO:__main__:[0.40729407070445234, 0.4053752381801234, 0.41329260758243086, 0.40299580203653246, 0.39830710835611133]
INFO:__main__:[0.4516304055660581, 0.4463747340684008, 0.4572561229707061, 0.45270505699995295, 0.43764030217649735]
INFO:__main__:0.41329260758243086
INFO:__main__:0.4572561229707061
INFO:__main__:best MAP: 0.435

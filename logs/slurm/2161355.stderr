INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 677374.68it/s]
100%|██████████| 5000/5000 [00:00<00:00, 687500.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 685209.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 762767.15it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 724504.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 767147.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 775058.02it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 664076.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 683779.59it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 673957.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 177.57it/s] 15%|█▍        | 23/156 [00:00<00:00, 176.02it/s] 15%|█▍        | 23/156 [00:00<00:00, 175.32it/s]100%|██████████| 156/156 [00:00<00:00, 1139.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1130.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1125.67it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18401.24it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18446.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18248.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 15%|█▍        | 23/156 [00:00<00:00, 152.04it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 148.80it/s]100%|██████████| 156/156 [00:00<00:00, 983.65it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 146.59it/s] 15%|█▍        | 23/156 [00:00<00:00, 146.98it/s] 15%|█▍        | 23/156 [00:00<00:00, 141.05it/s]100%|██████████| 156/156 [00:00<00:00, 964.35it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 140.67it/s]100%|██████████| 156/156 [00:00<00:00, 953.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 948.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 18665.28it/s]
  0%|          | 0/156 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f2] set: 29534
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 143.38it/s]100%|██████████| 156/156 [00:00<00:00, 916.63it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 914.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 18668.48it/s]
  0%|          | 0/156 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18937.55it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 931.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 17476.27it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18447.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18779.93it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
100%|██████████| 156/156 [00:00<00:00, 18290.14it/s]
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 4567.2 words/s - loss: 0.4117
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4501.0 words/s - loss: 0.4146
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 4028.8 words/s - loss: 0.4093
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4039.0 words/s - loss: 0.3856
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3931.0 words/s - loss: 0.4079
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3847.4 words/s - loss: 0.4249
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3880.7 words/s - loss: 0.3717
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3643.5 words/s - loss: 0.4095
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3448.0 words/s - loss: 0.3911
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3382.2 words/s - loss: 0.4059
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 4065.8 words/s - loss: 0.2731
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4305.7 words/s - loss: 0.2963
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3518.7 words/s - loss: 0.2493
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3786.5 words/s - loss: 0.2941
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3655.9 words/s - loss: 0.2668
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4142.3 words/s - loss: 0.3046
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3834.8 words/s - loss: 0.2856
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3517.8 words/s - loss: 0.2576
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3887.8 words/s - loss: 0.2411
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3586.4 words/s - loss: 0.2749
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3891.0 words/s - loss: 0.2764
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 4298.8 words/s - loss: 0.2559
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3620.5 words/s - loss: 0.2301
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 4055.9 words/s - loss: 0.2344
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3799.3 words/s - loss: 0.2330
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3438.0 words/s - loss: 0.2338
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3476.7 words/s - loss: 0.2323
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4142.1 words/s - loss: 0.2221
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3701.5 words/s - loss: 0.2521
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3560.6 words/s - loss: 0.2335
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 4443.4 words/s - loss: 0.2351
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4172.7 words/s - loss: 0.2012
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 4049.6 words/s - loss: 0.2222
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 4042.1 words/s - loss: 0.2182
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3745.4 words/s - loss: 0.2236
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4099.0 words/s - loss: 0.2487
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4149.9 words/s - loss: 0.2088
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4152.9 words/s - loss: 0.2098
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3926.8 words/s - loss: 0.2012
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3409.9 words/s - loss: 0.2183
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4273.3 words/s - loss: 0.2334
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4202.1 words/s - loss: 0.2217
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4496.7 words/s - loss: 0.1963
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3428.2 words/s - loss: 0.2139
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3826.4 words/s - loss: 0.2348
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3658.9 words/s - loss: 0.2024
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 4169.7 words/s - loss: 0.2240
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3492.6 words/s - loss: 0.2341
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3956.9 words/s - loss: 0.1925
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3396.3 words/s - loss: 0.1893
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 4148.3 words/s - loss: 0.2051
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 4078.7 words/s - loss: 0.2087
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4082.8 words/s - loss: 0.2204
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3577.6 words/s - loss: 0.2188
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 4033.3 words/s - loss: 0.1784
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3492.3 words/s - loss: 0.1963
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3749.3 words/s - loss: 0.2055
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3961.9 words/s - loss: 0.1971
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3851.7 words/s - loss: 0.1778
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3783.2 words/s - loss: 0.1924
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.443 w/ 78 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4551.3 words/s - loss: 0.1904
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 4084.2 words/s - loss: 0.1995
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4070.5 words/s - loss: 0.1888
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4020.9 words/s - loss: 0.1730
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 4015.8 words/s - loss: 0.2029
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3972.4 words/s - loss: 0.2068
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3790.3 words/s - loss: 0.2043
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4000.3 words/s - loss: 0.1604
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3772.1 words/s - loss: 0.1928
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3670.4 words/s - loss: 0.1845
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.366 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4552.8 words/s - loss: 0.1907
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4467.9 words/s - loss: 0.1874
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4356.2 words/s - loss: 0.2068
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 4101.8 words/s - loss: 0.2000
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 4036.5 words/s - loss: 0.1659
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3961.3 words/s - loss: 0.1823
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3789.6 words/s - loss: 0.1938
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3829.9 words/s - loss: 0.1762
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3690.0 words/s - loss: 0.1898
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3557.4 words/s - loss: 0.1685
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.361 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.451 w/ 78 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 4375.3 words/s - loss: 0.1646
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4298.7 words/s - loss: 0.1824
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3882.2 words/s - loss: 0.1821
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3974.8 words/s - loss: 0.1669
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3994.7 words/s - loss: 0.1862
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3874.6 words/s - loss: 0.1851
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3757.3 words/s - loss: 0.1792
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3664.8 words/s - loss: 0.1630
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3654.3 words/s - loss: 0.1820
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3438.3 words/s - loss: 0.1794
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.385 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.452 w/ 78 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4682.3 words/s - loss: 0.1840
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 4451.8 words/s - loss: 0.1623
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4416.3 words/s - loss: 0.1874
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4234.1 words/s - loss: 0.1771
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4211.9 words/s - loss: 0.1775
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3701.9 words/s - loss: 0.1632
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3674.8 words/s - loss: 0.1747
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3720.2 words/s - loss: 0.1737
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3401.0 words/s - loss: 0.1611
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3615.8 words/s - loss: 0.1701
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.381 w/ 78 queries
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.435 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:[0.37350417890342047, 0.3655900739494269, 0.3613222147818642, 0.38529128068393054, 0.3806698879984306]
INFO:__main__:[0.4426492837397838, 0.4470833183680821, 0.4505998863130004, 0.45240983776881083, 0.4346298750015694]
INFO:__main__:0.38529128068393054
INFO:__main__:0.45240983776881083
INFO:__main__:best MAP: 0.419
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 715190.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 704569.80it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 681424.49it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 722383.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 745468.51it/s]100%|██████████| 5000/5000 [00:00<00:00, 705921.64it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 685321.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684605.49it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 645655.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 648169.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 183.84it/s]100%|██████████| 156/156 [00:00<00:00, 1013.81it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 184.99it/s]100%|██████████| 156/156 [00:00<00:00, 16919.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1021.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18053.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 183.20it/s]100%|██████████| 156/156 [00:00<00:00, 1011.55it/s]
 17%|█▋        | 27/156 [00:00<00:00, 199.10it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1096.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17646.89it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18406.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 188.40it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1039.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17875.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 183.36it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1011.31it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17537.63it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 179.45it/s] 17%|█▋        | 27/156 [00:00<00:00, 178.30it/s]100%|██████████| 156/156 [00:00<00:00, 991.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 984.83it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 188.67it/s]100%|██████████| 156/156 [00:00<00:00, 17955.36it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 17696.53it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1040.52it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18079.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 208.75it/s]100%|██████████| 156/156 [00:00<00:00, 1145.13it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17953.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 4513.0 words/s - loss: 0.2775
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4268.4 words/s - loss: 0.2872
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 4113.6 words/s - loss: 0.2980
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 4119.5 words/s - loss: 0.2948
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4192.9 words/s - loss: 0.2882
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4063.5 words/s - loss: 0.3056
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3852.7 words/s - loss: 0.2961
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3831.4 words/s - loss: 0.2867
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3577.3 words/s - loss: 0.2859
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3199.9 words/s - loss: 0.2740
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4495.8 words/s - loss: 0.2295
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4235.1 words/s - loss: 0.2240
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 4450.4 words/s - loss: 0.2174
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 4140.1 words/s - loss: 0.2204
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 4118.5 words/s - loss: 0.2280
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3853.2 words/s - loss: 0.2258
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3794.9 words/s - loss: 0.2562
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3561.5 words/s - loss: 0.2004
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4083.5 words/s - loss: 0.2133
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3263.9 words/s - loss: 0.2191
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 5012.4 words/s - loss: 0.2216
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 4057.9 words/s - loss: 0.1931
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3836.6 words/s - loss: 0.2162
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 4084.6 words/s - loss: 0.2061
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3847.2 words/s - loss: 0.1607
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3684.2 words/s - loss: 0.1825
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3784.3 words/s - loss: 0.1974
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3817.8 words/s - loss: 0.2144
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3525.8 words/s - loss: 0.2044
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3234.2 words/s - loss: 0.2107
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4520.0 words/s - loss: 0.1821
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3757.5 words/s - loss: 0.1718
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4013.6 words/s - loss: 0.2191
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3648.3 words/s - loss: 0.1932
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3870.3 words/s - loss: 0.1891
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3550.5 words/s - loss: 0.1855
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3654.5 words/s - loss: 0.1827
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4231.7 words/s - loss: 0.1946
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3371.9 words/s - loss: 0.1781
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3690.5 words/s - loss: 0.1783
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4740.1 words/s - loss: 0.1855
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3638.9 words/s - loss: 0.1938
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4392.5 words/s - loss: 0.1973
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 4450.7 words/s - loss: 0.1761
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3322.7 words/s - loss: 0.1899
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4016.7 words/s - loss: 0.1739
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3852.6 words/s - loss: 0.1677
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4523.4 words/s - loss: 0.1620
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3623.0 words/s - loss: 0.2010
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4417.5 words/s - loss: 0.1652
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3456.7 words/s - loss: 0.1948
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 4190.0 words/s - loss: 0.1564
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3997.4 words/s - loss: 0.1748
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3549.8 words/s - loss: 0.2005
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 4152.3 words/s - loss: 0.1815
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3978.9 words/s - loss: 0.1588
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4189.1 words/s - loss: 0.1909
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3997.1 words/s - loss: 0.1613
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3284.5 words/s - loss: 0.1822
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 4102.7 words/s - loss: 0.1551
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.469 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.530 w/ 78 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4689.0 words/s - loss: 0.1584
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 4385.3 words/s - loss: 0.1854
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 4156.6 words/s - loss: 0.1501
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 4158.5 words/s - loss: 0.1810
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4188.6 words/s - loss: 0.1725
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4072.0 words/s - loss: 0.1413
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3971.5 words/s - loss: 0.1711
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3886.3 words/s - loss: 0.1771
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3747.5 words/s - loss: 0.1858
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3518.3 words/s - loss: 0.1975
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4824.1 words/s - loss: 0.1798
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4651.5 words/s - loss: 0.1837
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 4338.7 words/s - loss: 0.1690
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4256.6 words/s - loss: 0.1753
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4022.9 words/s - loss: 0.1887
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3781.9 words/s - loss: 0.1700
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3565.0 words/s - loss: 0.1265
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3521.8 words/s - loss: 0.1719
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3511.4 words/s - loss: 0.1549
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3191.7 words/s - loss: 0.1645
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 4285.0 words/s - loss: 0.1665
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4323.1 words/s - loss: 0.2018
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4243.0 words/s - loss: 0.1425
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 4169.1 words/s - loss: 0.1596
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 4088.9 words/s - loss: 0.1732
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 4115.9 words/s - loss: 0.1558
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3895.1 words/s - loss: 0.1546
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3717.7 words/s - loss: 0.1776
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3610.8 words/s - loss: 0.1617
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3573.3 words/s - loss: 0.1581
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.547 w/ 78 queries
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 4325.6 words/s - loss: 0.1564
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4277.2 words/s - loss: 0.1694
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4121.5 words/s - loss: 0.1372
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3948.6 words/s - loss: 0.1567
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3675.2 words/s - loss: 0.1660
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3748.7 words/s - loss: 0.1720
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3415.0 words/s - loss: 0.1550
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3491.5 words/s - loss: 0.1476
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3206.6 words/s - loss: 0.1371
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3174.4 words/s - loss: 0.1596
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.483 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.554 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:[0.4685714510037214, 0.47542602014990987, 0.49237708650901585, 0.4941810114337615, 0.48349393300319404]
INFO:__main__:[0.5301700468791706, 0.5470276936567436, 0.5541668649771709, 0.5473322848729429, 0.5536132228097144]
INFO:__main__:0.49237708650901585
INFO:__main__:0.5473322848729429
INFO:__main__:best MAP: 0.520
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 747061.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 707135.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 733962.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 738902.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 696982.95it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 734476.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 723729.85it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 471588.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 690011.52it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 636561.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 204.16it/s]100%|██████████| 156/156 [00:00<00:00, 1120.52it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17598.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 201.54it/s]100%|██████████| 156/156 [00:00<00:00, 1105.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17576.73it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 207.61it/s] 17%|█▋        | 27/156 [00:00<00:00, 207.43it/s]100%|██████████| 156/156 [00:00<00:00, 1140.72it/s]
100%|██████████| 156/156 [00:00<00:00, 1139.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17924.38it/s]
100%|██████████| 156/156 [00:00<00:00, 17865.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 171.28it/s]100%|██████████| 156/156 [00:00<00:00, 948.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18043.50it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 201.66it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1109.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18345.01it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 200.38it/s]100%|██████████| 156/156 [00:00<00:00, 1101.63it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18291.67it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 156.10it/s]100%|██████████| 156/156 [00:00<00:00, 867.97it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 8778.93it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 207.03it/s]100%|██████████| 156/156 [00:00<00:00, 1137.84it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18318.81it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 189.76it/s]100%|██████████| 156/156 [00:00<00:00, 1044.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17111.10it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 4520.0 words/s - loss: 0.2895
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 4075.1 words/s - loss: 0.2905
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3961.9 words/s - loss: 0.3265
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3814.9 words/s - loss: 0.3084
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3756.8 words/s - loss: 0.3029
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3911.6 words/s - loss: 0.2922
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3699.0 words/s - loss: 0.2901
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3625.6 words/s - loss: 0.2988
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3503.9 words/s - loss: 0.3137
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3454.5 words/s - loss: 0.2665
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3917.9 words/s - loss: 0.2350
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4214.2 words/s - loss: 0.2170
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4551.3 words/s - loss: 0.2136
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 4723.9 words/s - loss: 0.1975
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 4239.3 words/s - loss: 0.2114
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 4126.8 words/s - loss: 0.2273
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 4018.6 words/s - loss: 0.2134
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3999.2 words/s - loss: 0.2261
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4192.9 words/s - loss: 0.2519
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3219.6 words/s - loss: 0.2179
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4485.8 words/s - loss: 0.1871
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4376.2 words/s - loss: 0.1930
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3733.8 words/s - loss: 0.1929
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3884.5 words/s - loss: 0.2052
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3659.3 words/s - loss: 0.1938
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3701.5 words/s - loss: 0.1991
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3812.6 words/s - loss: 0.2004
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3410.2 words/s - loss: 0.2260
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3301.1 words/s - loss: 0.1908
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3532.5 words/s - loss: 0.1986
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4207.0 words/s - loss: 0.1713
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4091.4 words/s - loss: 0.1813
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4153.3 words/s - loss: 0.2093
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 4234.2 words/s - loss: 0.1993
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3762.1 words/s - loss: 0.1685
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3988.8 words/s - loss: 0.1821
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4076.5 words/s - loss: 0.2014
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3483.2 words/s - loss: 0.1820
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3273.7 words/s - loss: 0.1721
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3596.9 words/s - loss: 0.2013
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3785.3 words/s - loss: 0.2010
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4520.9 words/s - loss: 0.1893
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 4087.9 words/s - loss: 0.1932
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4061.6 words/s - loss: 0.1698
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3872.9 words/s - loss: 0.1639
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3725.8 words/s - loss: 0.1734
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4313.6 words/s - loss: 0.1538
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3892.1 words/s - loss: 0.1969
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4015.7 words/s - loss: 0.1684
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3768.8 words/s - loss: 0.1842
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 4200.3 words/s - loss: 0.1916
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3958.7 words/s - loss: 0.1674
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3943.6 words/s - loss: 0.1536
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4342.9 words/s - loss: 0.1884
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 4459.0 words/s - loss: 0.1642
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3479.4 words/s - loss: 0.1982
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3635.3 words/s - loss: 0.1321
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3464.8 words/s - loss: 0.1788
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4070.7 words/s - loss: 0.1830
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3944.4 words/s - loss: 0.1609
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.494 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 4526.1 words/s - loss: 0.1857
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4275.4 words/s - loss: 0.1359
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 4196.2 words/s - loss: 0.1583
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3972.7 words/s - loss: 0.1522
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4038.5 words/s - loss: 0.1562
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3801.3 words/s - loss: 0.1825
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3730.8 words/s - loss: 0.1711
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3678.3 words/s - loss: 0.1602
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3627.4 words/s - loss: 0.1680
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3329.0 words/s - loss: 0.1518
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.506 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4960.7 words/s - loss: 0.1408
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4357.6 words/s - loss: 0.1654
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4320.5 words/s - loss: 0.1754
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4182.3 words/s - loss: 0.1826
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4033.9 words/s - loss: 0.1696
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3576.7 words/s - loss: 0.1830
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3565.5 words/s - loss: 0.1889
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3393.3 words/s - loss: 0.1414
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3426.1 words/s - loss: 0.1568
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3220.7 words/s - loss: 0.1437
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.488 w/ 78 queries
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 4382.6 words/s - loss: 0.1856
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 4103.6 words/s - loss: 0.1641
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 4054.2 words/s - loss: 0.1524
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4099.0 words/s - loss: 0.1511
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4129.7 words/s - loss: 0.1662
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 4052.2 words/s - loss: 0.1601
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4103.6 words/s - loss: 0.1582
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3870.1 words/s - loss: 0.1441
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3531.0 words/s - loss: 0.1663
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3459.6 words/s - loss: 0.1571
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4714.5 words/s - loss: 0.1453
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 4315.0 words/s - loss: 0.1485
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 4127.2 words/s - loss: 0.1494
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4068.5 words/s - loss: 0.1358
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3981.8 words/s - loss: 0.1295
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3900.1 words/s - loss: 0.1627
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3804.4 words/s - loss: 0.1611
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3666.5 words/s - loss: 0.1488
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3664.7 words/s - loss: 0.1551
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2934.2 words/s - loss: 0.1377
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.551 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:[0.49423423847139475, 0.506494597388343, 0.48836245615990054, 0.48851696735563305, 0.4896079882375139]
INFO:__main__:[0.5554985130045261, 0.5622688440993305, 0.5454754877200294, 0.5493657872130868, 0.5513382396493242]
INFO:__main__:0.506494597388343
INFO:__main__:0.5622688440993305
INFO:__main__:best MAP: 0.534

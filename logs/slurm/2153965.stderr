INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 774771.69it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 793744.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 175.98it/s]100%|██████████| 176/176 [00:00<00:00, 1372.91it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16566.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 586665.17it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 12%|█▏        | 21/176 [00:00<00:00, 185.06it/s]100%|██████████| 176/176 [00:00<00:00, 1439.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16664.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
 12%|█▏        | 21/176 [00:00<00:01, 129.65it/s]100%|██████████| 176/176 [00:00<00:00, 1031.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 13746.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 666037.41it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 669631.52it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 638985.98it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 702751.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 689149.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 678360.67it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 674108.65it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 135.45it/s]100%|██████████| 176/176 [00:00<00:00, 1071.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 155.74it/s]100%|██████████| 176/176 [00:00<00:00, 1222.23it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15594.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:00, 165.62it/s]100%|██████████| 176/176 [00:00<00:00, 15887.17it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:01, 150.80it/s]100%|██████████| 176/176 [00:00<00:00, 1297.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1187.78it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16556.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16105.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 155.60it/s]100%|██████████| 176/176 [00:00<00:00, 1223.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 156.33it/s]100%|██████████| 176/176 [00:00<00:00, 16427.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
100%|██████████| 176/176 [00:00<00:00, 1225.70it/s]
INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15699.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:00, 156.23it/s]100%|██████████| 176/176 [00:00<00:00, 1227.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16211.29it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3934.9 words/s - loss: 0.4011
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3778.9 words/s - loss: 0.4113
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3731.2 words/s - loss: 0.3895
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3703.7 words/s - loss: 0.3894
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3539.3 words/s - loss: 0.3996
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3497.8 words/s - loss: 0.3751
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3435.8 words/s - loss: 0.3915
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3388.6 words/s - loss: 0.3758
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3289.8 words/s - loss: 0.3762
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3232.3 words/s - loss: 0.3769
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 4139.6 words/s - loss: 0.2670
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3675.4 words/s - loss: 0.2685
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3673.4 words/s - loss: 0.2628
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3616.7 words/s - loss: 0.2311
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3615.1 words/s - loss: 0.2712
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3511.0 words/s - loss: 0.2760
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3542.6 words/s - loss: 0.2726
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3337.8 words/s - loss: 0.2568
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3504.5 words/s - loss: 0.2629
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3081.5 words/s - loss: 0.2754
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3869.3 words/s - loss: 0.1957
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4101.7 words/s - loss: 0.1901
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3609.0 words/s - loss: 0.2688
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3498.8 words/s - loss: 0.1880
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3780.0 words/s - loss: 0.2260
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3756.2 words/s - loss: 0.2175
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3779.0 words/s - loss: 0.2020
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3381.5 words/s - loss: 0.2107
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3626.7 words/s - loss: 0.2318
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3392.4 words/s - loss: 0.2388
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3766.2 words/s - loss: 0.1995
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3647.1 words/s - loss: 0.1754
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3551.0 words/s - loss: 0.2006
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3610.2 words/s - loss: 0.1679
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3967.8 words/s - loss: 0.2179
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3470.1 words/s - loss: 0.1691
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3377.7 words/s - loss: 0.1672
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3271.5 words/s - loss: 0.1863
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3475.1 words/s - loss: 0.2023
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3604.7 words/s - loss: 0.1819
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3843.2 words/s - loss: 0.1829
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3291.9 words/s - loss: 0.1510
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3649.3 words/s - loss: 0.1921
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 4015.3 words/s - loss: 0.1703
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3755.7 words/s - loss: 0.1767
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4079.5 words/s - loss: 0.1858
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3422.7 words/s - loss: 0.1589
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3506.8 words/s - loss: 0.1325
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3849.2 words/s - loss: 0.2041
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3152.1 words/s - loss: 0.1508
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3644.3 words/s - loss: 0.1496
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3777.2 words/s - loss: 0.1523
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3302.0 words/s - loss: 0.1634
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3308.1 words/s - loss: 0.1431
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3633.2 words/s - loss: 0.1389
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3549.0 words/s - loss: 0.1396
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3452.8 words/s - loss: 0.1664
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4304.3 words/s - loss: 0.1500
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3784.6 words/s - loss: 0.1536
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3390.5 words/s - loss: 0.1878
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.372 w/ 88 queries
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3719.5 words/s - loss: 0.1647
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3724.8 words/s - loss: 0.1199
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3660.4 words/s - loss: 0.1435
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3602.4 words/s - loss: 0.1626
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3538.4 words/s - loss: 0.1734
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3549.5 words/s - loss: 0.1290
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3489.8 words/s - loss: 0.1530
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3409.7 words/s - loss: 0.1463
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3149.0 words/s - loss: 0.1617
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3069.2 words/s - loss: 0.1438
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.369 w/ 88 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.329 w/ 88 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fren_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4180.1 words/s - loss: 0.1243
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3718.7 words/s - loss: 0.1252
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3688.0 words/s - loss: 0.1788
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3649.1 words/s - loss: 0.1167
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3626.6 words/s - loss: 0.1325
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3488.3 words/s - loss: 0.1439
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3363.9 words/s - loss: 0.1380
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3370.8 words/s - loss: 0.1445
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3349.9 words/s - loss: 0.1324
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3219.7 words/s - loss: 0.1545
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.297 w/ 88 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fren_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 4454.6 words/s - loss: 0.1287
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4163.5 words/s - loss: 0.1258
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3657.5 words/s - loss: 0.1407
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3697.2 words/s - loss: 0.1489
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3639.0 words/s - loss: 0.1472
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3381.4 words/s - loss: 0.1428
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3335.8 words/s - loss: 0.1331
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3296.8 words/s - loss: 0.1038
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3294.7 words/s - loss: 0.1104
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3214.3 words/s - loss: 0.1465
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.373 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fren_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3588.4 words/s - loss: 0.1336
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3616.2 words/s - loss: 0.1269
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3522.7 words/s - loss: 0.1399
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3432.6 words/s - loss: 0.1283
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3509.1 words/s - loss: 0.1455
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3474.8 words/s - loss: 0.1474
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3436.4 words/s - loss: 0.1205
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3245.5 words/s - loss: 0.1168
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3131.7 words/s - loss: 0.1186
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3057.9 words/s - loss: 0.1402
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.377 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.320 w/ 88 queries
INFO:__main__:removing file tmp/mbert_fren_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_9.txt
INFO:__main__:0.36879465632845393
INFO:__main__:0.3198689182627159
INFO:__main__:best MAP: 0.344
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 659606.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 664370.53it/s]100%|██████████| 5000/5000 [00:00<00:00, 661812.67it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 693777.95it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 671582.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 660978.32it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 685164.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 668862.67it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 694030.51it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 645198.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 178.25it/s]100%|██████████| 176/176 [00:00<00:00, 1275.20it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16120.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 151.76it/s] 13%|█▎        | 23/176 [00:00<00:01, 149.18it/s]100%|██████████| 176/176 [00:00<00:00, 1094.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1079.13it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 14977.53it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 15620.91it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 152.51it/s] 13%|█▎        | 23/176 [00:00<00:01, 134.17it/s]100%|██████████| 176/176 [00:00<00:00, 1102.85it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 976.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 203.01it/s]100%|██████████| 176/176 [00:00<00:00, 15822.81it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 15920.07it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 1442.32it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 126.43it/s]100%|██████████| 176/176 [00:00<00:00, 16387.27it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 922.61it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 132.15it/s] 13%|█▎        | 23/176 [00:00<00:01, 148.97it/s]100%|██████████| 176/176 [00:00<00:00, 16489.40it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 960.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1078.13it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15386.17it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 16014.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 116.32it/s]100%|██████████| 176/176 [00:00<00:00, 851.31it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16016.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3690.2 words/s - loss: 0.2985
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3710.3 words/s - loss: 0.2456
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3615.5 words/s - loss: 0.3091
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3550.9 words/s - loss: 0.2513
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3619.2 words/s - loss: 0.2458
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3487.2 words/s - loss: 0.2444
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3353.8 words/s - loss: 0.2457
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3289.9 words/s - loss: 0.2635
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3292.0 words/s - loss: 0.2851
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2971.6 words/s - loss: 0.2516
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3941.0 words/s - loss: 0.1661
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3623.0 words/s - loss: 0.1744
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3752.1 words/s - loss: 0.2197
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3439.1 words/s - loss: 0.1491
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3393.3 words/s - loss: 0.1841
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3731.5 words/s - loss: 0.1514
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3374.9 words/s - loss: 0.1870
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3399.9 words/s - loss: 0.1713
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3283.6 words/s - loss: 0.1747
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3380.9 words/s - loss: 0.1694
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3874.6 words/s - loss: 0.1476
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3593.8 words/s - loss: 0.1504
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3452.7 words/s - loss: 0.1853
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3426.2 words/s - loss: 0.1459
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3525.6 words/s - loss: 0.1514
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3562.8 words/s - loss: 0.1189
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3591.8 words/s - loss: 0.1711
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3277.2 words/s - loss: 0.1626
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3421.0 words/s - loss: 0.1584
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3218.7 words/s - loss: 0.1541
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3255.0 words/s - loss: 0.1699
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3702.0 words/s - loss: 0.1249
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3498.1 words/s - loss: 0.1522
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3443.3 words/s - loss: 0.1669
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3377.2 words/s - loss: 0.1591
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3275.8 words/s - loss: 0.1490
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3305.3 words/s - loss: 0.1688
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3375.1 words/s - loss: 0.1597
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3589.2 words/s - loss: 0.1500
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3626.5 words/s - loss: 0.1648
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3733.6 words/s - loss: 0.1275
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3465.8 words/s - loss: 0.1412
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3355.9 words/s - loss: 0.1533
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3700.1 words/s - loss: 0.1494
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3779.8 words/s - loss: 0.1427
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3663.9 words/s - loss: 0.1565
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3477.0 words/s - loss: 0.1395
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3665.0 words/s - loss: 0.1542
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3708.0 words/s - loss: 0.1268
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3184.0 words/s - loss: 0.1468
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3659.6 words/s - loss: 0.1531
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3597.9 words/s - loss: 0.1492
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3192.8 words/s - loss: 0.1264
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3512.8 words/s - loss: 0.1334
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3334.7 words/s - loss: 0.1316
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3546.9 words/s - loss: 0.1539
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3908.6 words/s - loss: 0.1418
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3522.0 words/s - loss: 0.1218
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3530.6 words/s - loss: 0.1396
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3511.4 words/s - loss: 0.1362
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.419 w/ 88 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3982.8 words/s - loss: 0.1185
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3963.6 words/s - loss: 0.1373
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3796.6 words/s - loss: 0.1478
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3734.9 words/s - loss: 0.1390
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3658.1 words/s - loss: 0.1318
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3598.6 words/s - loss: 0.1276
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3519.1 words/s - loss: 0.1189
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3385.4 words/s - loss: 0.1155
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3248.6 words/s - loss: 0.1477
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3351.2 words/s - loss: 0.1318
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fren_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4001.5 words/s - loss: 0.1290
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3893.8 words/s - loss: 0.1532
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3768.3 words/s - loss: 0.1143
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3584.9 words/s - loss: 0.1261
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3538.7 words/s - loss: 0.1293
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3506.0 words/s - loss: 0.0994
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3451.2 words/s - loss: 0.1428
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3298.5 words/s - loss: 0.1269
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3261.1 words/s - loss: 0.1251
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3137.5 words/s - loss: 0.1389
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.410 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fren_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4143.1 words/s - loss: 0.1145
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3752.1 words/s - loss: 0.1055
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3776.3 words/s - loss: 0.1330
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3755.8 words/s - loss: 0.1294
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3699.5 words/s - loss: 0.1019
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3645.4 words/s - loss: 0.1423
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3622.0 words/s - loss: 0.1003
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3408.8 words/s - loss: 0.0945
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3315.4 words/s - loss: 0.1195
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3139.1 words/s - loss: 0.1081
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.418 w/ 88 queries
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.357 w/ 88 queries
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fren_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3964.0 words/s - loss: 0.1160
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3803.3 words/s - loss: 0.1282
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3679.6 words/s - loss: 0.1214
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3681.9 words/s - loss: 0.1378
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3637.8 words/s - loss: 0.1432
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3508.2 words/s - loss: 0.1153
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3512.0 words/s - loss: 0.1332
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3463.5 words/s - loss: 0.1189
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3313.9 words/s - loss: 0.1278
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3204.2 words/s - loss: 0.1389
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.405 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.351 w/ 88 queries
INFO:__main__:removing file tmp/mbert_fren_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_9.txt
INFO:__main__:0.41832933792067845
INFO:__main__:0.35289908804422665
INFO:__main__:best MAP: 0.386
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 613363.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 651512.01it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 618939.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 754018.62it/s]
100%|██████████| 5000/5000 [00:00<00:00, 734065.60it/s]INFO:root:Number of positive query-document pairs in [train] set: 5000

INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 685612.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 645297.39it/s]
100%|██████████| 5000/5000 [00:00<00:00, 675955.52it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 656262.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 394327.51it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 153.50it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
100%|██████████| 176/176 [00:00<00:00, 1108.64it/s]
  0%|          | 0/176 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15570.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 164.48it/s]100%|██████████| 176/176 [00:00<00:00, 1181.23it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15701.32it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 163.53it/s] 13%|█▎        | 23/176 [00:00<00:00, 167.29it/s]100%|██████████| 176/176 [00:00<00:00, 1178.96it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1203.58it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16625.32it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
 13%|█▎        | 23/176 [00:00<00:01, 146.64it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16343.37it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1062.14it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 145.34it/s]100%|██████████| 176/176 [00:00<00:00, 1053.36it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16012.96it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
 13%|█▎        | 23/176 [00:00<00:01, 135.25it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 131.72it/s] 13%|█▎        | 23/176 [00:00<00:01, 129.16it/s]100%|██████████| 176/176 [00:00<00:00, 16323.14it/s]
100%|██████████| 176/176 [00:00<00:00, 984.46it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 961.05it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 942.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 152.17it/s]100%|██████████| 176/176 [00:00<00:00, 16492.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16598.41it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1101.89it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
100%|██████████| 176/176 [00:00<00:00, 16018.87it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16220.91it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4146.6 words/s - loss: 0.2777
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3814.2 words/s - loss: 0.2384
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3723.1 words/s - loss: 0.2834
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3580.8 words/s - loss: 0.2516
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3536.1 words/s - loss: 0.2552
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3562.7 words/s - loss: 0.2956
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3468.1 words/s - loss: 0.2589
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3505.8 words/s - loss: 0.2565
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3494.0 words/s - loss: 0.2465
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3436.8 words/s - loss: 0.2746
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3934.3 words/s - loss: 0.1710
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3834.1 words/s - loss: 0.1466
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3653.1 words/s - loss: 0.1842
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3471.2 words/s - loss: 0.2179
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3569.2 words/s - loss: 0.1993
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3370.0 words/s - loss: 0.1758
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3188.2 words/s - loss: 0.1610
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3584.0 words/s - loss: 0.1856
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3363.8 words/s - loss: 0.1733
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3169.7 words/s - loss: 0.1778
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3925.2 words/s - loss: 0.1624
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3945.6 words/s - loss: 0.1298
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3755.4 words/s - loss: 0.1856
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3737.7 words/s - loss: 0.1512
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3829.0 words/s - loss: 0.1797
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3577.3 words/s - loss: 0.1412
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3364.9 words/s - loss: 0.1607
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3248.6 words/s - loss: 0.1597
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3912.5 words/s - loss: 0.1817
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3207.1 words/s - loss: 0.1385
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3895.9 words/s - loss: 0.1369
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3505.0 words/s - loss: 0.1188
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3561.3 words/s - loss: 0.1464
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3573.8 words/s - loss: 0.1295
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3695.7 words/s - loss: 0.1639
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 4013.4 words/s - loss: 0.1533
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3685.9 words/s - loss: 0.1122
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3489.8 words/s - loss: 0.1659
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2955.9 words/s - loss: 0.1444
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3545.6 words/s - loss: 0.1652
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3461.2 words/s - loss: 0.1139
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3569.8 words/s - loss: 0.1689
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3846.4 words/s - loss: 0.1469
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3493.9 words/s - loss: 0.1382
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3355.6 words/s - loss: 0.1206
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3163.0 words/s - loss: 0.1271
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3639.9 words/s - loss: 0.1584
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3553.5 words/s - loss: 0.1406
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3327.6 words/s - loss: 0.1176
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3273.9 words/s - loss: 0.1339
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3797.3 words/s - loss: 0.1218
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3522.2 words/s - loss: 0.1425
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3825.4 words/s - loss: 0.1171
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3914.1 words/s - loss: 0.1362
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3591.7 words/s - loss: 0.1366
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3563.3 words/s - loss: 0.1413
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3611.1 words/s - loss: 0.1212
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3475.3 words/s - loss: 0.1366
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3899.3 words/s - loss: 0.1345
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3416.9 words/s - loss: 0.1231
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fren_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3916.7 words/s - loss: 0.1098
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3767.7 words/s - loss: 0.1482
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3720.1 words/s - loss: 0.1232
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3558.1 words/s - loss: 0.1618
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3508.0 words/s - loss: 0.1260
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3461.3 words/s - loss: 0.1500
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3388.9 words/s - loss: 0.1058
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3422.2 words/s - loss: 0.1262
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3305.7 words/s - loss: 0.1434
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2970.4 words/s - loss: 0.1608
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.423 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fren_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 4125.0 words/s - loss: 0.1417
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3758.8 words/s - loss: 0.1033
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3775.5 words/s - loss: 0.1451
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3684.1 words/s - loss: 0.1503
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3561.0 words/s - loss: 0.1340
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3421.4 words/s - loss: 0.1375
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3459.4 words/s - loss: 0.1129
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3394.6 words/s - loss: 0.1290
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3360.0 words/s - loss: 0.1303
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3111.0 words/s - loss: 0.1209
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.360 w/ 88 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fren_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3900.5 words/s - loss: 0.1117
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3778.8 words/s - loss: 0.1283
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3551.9 words/s - loss: 0.1106
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3455.4 words/s - loss: 0.1331
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3460.1 words/s - loss: 0.1016
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3450.8 words/s - loss: 0.1036
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3412.8 words/s - loss: 0.1253
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3365.2 words/s - loss: 0.1205
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3381.2 words/s - loss: 0.1138
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3375.0 words/s - loss: 0.1274
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fren_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fren_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fren_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
slurmstepd-asimov-217: error: *** JOB 2153965 ON asimov-217 CANCELLED AT 2020-08-01T19:42:55 ***

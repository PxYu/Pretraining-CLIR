INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 694651.21it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684829.05it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684337.41it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 671798.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 576758.61it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 569012.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 686532.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 674563.99it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 622374.17it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 614244.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 138.12it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1457.47it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
100%|██████████| 192/192 [00:00<00:00, 19447.62it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 136.31it/s]100%|██████████| 192/192 [00:00<00:00, 1439.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19423.23it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 125.88it/s]  9%|▉         | 17/192 [00:00<00:01, 134.37it/s]100%|██████████| 192/192 [00:00<00:00, 1333.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1418.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19146.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
  9%|▉         | 17/192 [00:00<00:01, 117.26it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19588.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1248.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19307.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 118.12it/s]100%|██████████| 192/192 [00:00<00:00, 1257.85it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19561.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 136.18it/s]100%|██████████| 192/192 [00:00<00:00, 1439.01it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19711.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 121.12it/s]100%|██████████| 192/192 [00:00<00:00, 1287.35it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19757.27it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 136.57it/s]100%|██████████| 192/192 [00:00<00:00, 1445.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 16559.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 136.98it/s]100%|██████████| 192/192 [00:00<00:00, 1421.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20290.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3578.5 words/s - loss: 0.4164
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3514.3 words/s - loss: 0.3938
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3400.1 words/s - loss: 0.4329
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3346.9 words/s - loss: 0.4032
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3168.6 words/s - loss: 0.4251
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3077.1 words/s - loss: 0.4105
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2950.0 words/s - loss: 0.4014
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2893.0 words/s - loss: 0.4035
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2630.6 words/s - loss: 0.4146
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2539.0 words/s - loss: 0.4091
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3856.5 words/s - loss: 0.2171
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3951.1 words/s - loss: 0.2301
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3429.9 words/s - loss: 0.2018
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2962.9 words/s - loss: 0.2474
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2999.3 words/s - loss: 0.2103
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3261.2 words/s - loss: 0.2560
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3371.4 words/s - loss: 0.2207
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3139.2 words/s - loss: 0.2344
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2802.5 words/s - loss: 0.2194
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2395.5 words/s - loss: 0.2480
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3299.9 words/s - loss: 0.1879
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2979.1 words/s - loss: 0.2140
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3101.7 words/s - loss: 0.1994
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3137.5 words/s - loss: 0.2113
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3014.3 words/s - loss: 0.2123
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3049.2 words/s - loss: 0.2003
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2678.4 words/s - loss: 0.2064
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3323.5 words/s - loss: 0.1739
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3166.0 words/s - loss: 0.2034
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2402.1 words/s - loss: 0.1728
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3000.8 words/s - loss: 0.2084
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2835.3 words/s - loss: 0.1869
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2963.0 words/s - loss: 0.1863
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2965.7 words/s - loss: 0.1951
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2442.7 words/s - loss: 0.1463
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3047.0 words/s - loss: 0.1992
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3004.6 words/s - loss: 0.2107
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3029.7 words/s - loss: 0.1765
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3175.7 words/s - loss: 0.1934
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3515.0 words/s - loss: 0.1916
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2823.8 words/s - loss: 0.1869
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3936.3 words/s - loss: 0.1786
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3256.7 words/s - loss: 0.1858
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3091.7 words/s - loss: 0.1595
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3453.2 words/s - loss: 0.1533
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3154.8 words/s - loss: 0.1612
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3038.9 words/s - loss: 0.1665
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3318.5 words/s - loss: 0.1584
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3465.0 words/s - loss: 0.1678
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3485.6 words/s - loss: 0.1334
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3605.5 words/s - loss: 0.1585
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3310.0 words/s - loss: 0.1639
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2750.9 words/s - loss: 0.2058
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2916.0 words/s - loss: 0.1786
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3142.1 words/s - loss: 0.1479
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3388.6 words/s - loss: 0.1773
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2771.8 words/s - loss: 0.1636
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2960.7 words/s - loss: 0.1809
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2593.7 words/s - loss: 0.1552
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2530.4 words/s - loss: 0.1684
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.363 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.389 w/ 96 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_ende_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4042.6 words/s - loss: 0.1531
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3526.6 words/s - loss: 0.1510
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3508.6 words/s - loss: 0.1422
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3439.6 words/s - loss: 0.1545
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3401.6 words/s - loss: 0.1637
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3311.3 words/s - loss: 0.1612
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3032.8 words/s - loss: 0.1586
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2983.9 words/s - loss: 0.1565
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2822.6 words/s - loss: 0.1227
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2603.4 words/s - loss: 0.1656
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.379 w/ 96 queries
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.387 w/ 96 queries
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_ende_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3620.6 words/s - loss: 0.1374
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3225.7 words/s - loss: 0.1696
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3183.3 words/s - loss: 0.1612
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3164.0 words/s - loss: 0.1422
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3040.7 words/s - loss: 0.1369
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3062.3 words/s - loss: 0.1621
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2942.0 words/s - loss: 0.1408
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2868.2 words/s - loss: 0.1675
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2769.8 words/s - loss: 0.1545
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2748.4 words/s - loss: 0.1174
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.376 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.381 w/ 96 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_ende_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3173.8 words/s - loss: 0.1146
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3207.2 words/s - loss: 0.1598
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3058.1 words/s - loss: 0.1445
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3084.3 words/s - loss: 0.1428
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2994.4 words/s - loss: 0.1331
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2948.3 words/s - loss: 0.1345
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2913.6 words/s - loss: 0.1177
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2798.3 words/s - loss: 0.1678
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2761.1 words/s - loss: 0.1300
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2476.7 words/s - loss: 0.1620
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.388 w/ 96 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.396 w/ 96 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_ende_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3613.1 words/s - loss: 0.1318
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3622.8 words/s - loss: 0.1424
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3214.5 words/s - loss: 0.1195
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3251.3 words/s - loss: 0.1355
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3110.3 words/s - loss: 0.1293
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3086.8 words/s - loss: 0.1334
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2917.7 words/s - loss: 0.1129
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2872.0 words/s - loss: 0.1270
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2838.2 words/s - loss: 0.1448
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2416.1 words/s - loss: 0.1485
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.360 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.370 w/ 96 queries
INFO:__main__:removing file tmp/mbert_ende_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_9.txt
INFO:__main__:[0.3626423469231788, 0.3794339531581263, 0.37585869698856317, 0.38805074595430306, 0.3604696436492934]
INFO:__main__:[0.3893457540286003, 0.38675057467959845, 0.3810522464978631, 0.39601755024403285, 0.37015308432188004]
INFO:__main__:0.38805074595430306
INFO:__main__:0.39601755024403285
INFO:__main__:best MAP: 0.392
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 798762.90it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 143.19it/s]100%|██████████| 192/192 [00:00<00:00, 1429.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19330.45it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 681004.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 707469.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 669952.40it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 695180.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684940.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 704569.80it/s]100%|██████████| 5000/5000 [00:00<00:00, 709072.22it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 626146.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 643160.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 129.34it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1298.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19992.21it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 158.89it/s]100%|██████████| 192/192 [00:00<00:00, 1575.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19503.67it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 130.93it/s]100%|██████████| 192/192 [00:00<00:00, 1310.20it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18155.93it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 103.61it/s]100%|██████████| 192/192 [00:00<00:00, 1053.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19731.13it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 122.92it/s]100%|██████████| 192/192 [00:00<00:00, 1237.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19550.54it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 129.08it/s]  9%|▉         | 18/192 [00:00<00:01, 127.92it/s]100%|██████████| 192/192 [00:00<00:00, 1296.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1284.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19517.37it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 19486.20it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 100.80it/s]100%|██████████| 192/192 [00:00<00:00, 1026.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19525.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 106.05it/s]100%|██████████| 192/192 [00:00<00:00, 1075.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19360.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3760.4 words/s - loss: 0.2749
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3601.6 words/s - loss: 0.2332
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3516.0 words/s - loss: 0.2266
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3231.0 words/s - loss: 0.2455
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2966.0 words/s - loss: 0.1882
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2738.5 words/s - loss: 0.2383
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2679.6 words/s - loss: 0.2452
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2585.7 words/s - loss: 0.2250
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2428.0 words/s - loss: 0.2171
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2422.6 words/s - loss: 0.2512
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3591.4 words/s - loss: 0.1788
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3437.3 words/s - loss: 0.1676
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3002.8 words/s - loss: 0.1801
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3584.6 words/s - loss: 0.1670
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2694.9 words/s - loss: 0.1511
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3431.3 words/s - loss: 0.1716
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3448.7 words/s - loss: 0.1622
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3371.8 words/s - loss: 0.1462
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3281.7 words/s - loss: 0.1631
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2392.8 words/s - loss: 0.1455
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3002.3 words/s - loss: 0.1704
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3602.4 words/s - loss: 0.1449
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3069.2 words/s - loss: 0.1723
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3660.4 words/s - loss: 0.1498
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3164.3 words/s - loss: 0.1647
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3059.3 words/s - loss: 0.1423
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3173.1 words/s - loss: 0.1371
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2982.0 words/s - loss: 0.1702
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2354.0 words/s - loss: 0.1567
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3312.3 words/s - loss: 0.1741
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3618.9 words/s - loss: 0.1288
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3666.6 words/s - loss: 0.1462
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2742.8 words/s - loss: 0.1153
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3110.0 words/s - loss: 0.1315
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3139.9 words/s - loss: 0.1514
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2874.5 words/s - loss: 0.1594
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2769.7 words/s - loss: 0.1341
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2854.4 words/s - loss: 0.1294
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 4348.6 words/s - loss: 0.1270
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2956.7 words/s - loss: 0.1165
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3277.8 words/s - loss: 0.1171
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3285.9 words/s - loss: 0.1314
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2989.3 words/s - loss: 0.1242
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3168.3 words/s - loss: 0.1438
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2952.9 words/s - loss: 0.1605
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 4107.9 words/s - loss: 0.1209
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3086.6 words/s - loss: 0.1435
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2760.5 words/s - loss: 0.1283
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3192.3 words/s - loss: 0.1249
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3263.0 words/s - loss: 0.1244
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3499.0 words/s - loss: 0.1245
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3433.8 words/s - loss: 0.1607
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2841.7 words/s - loss: 0.1218
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3114.5 words/s - loss: 0.0975
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2628.0 words/s - loss: 0.1206
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3412.1 words/s - loss: 0.1447
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3101.3 words/s - loss: 0.1318
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2680.9 words/s - loss: 0.1362
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3053.0 words/s - loss: 0.1262
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3883.4 words/s - loss: 0.1166
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.443 w/ 96 queries
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_ende_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4039.8 words/s - loss: 0.0897
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3573.0 words/s - loss: 0.1014
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3212.0 words/s - loss: 0.1222
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3039.6 words/s - loss: 0.1189
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3065.6 words/s - loss: 0.1090
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2923.1 words/s - loss: 0.1225
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2883.5 words/s - loss: 0.1173
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2558.9 words/s - loss: 0.1267
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2557.0 words/s - loss: 0.1123
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2520.1 words/s - loss: 0.1206
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.450 w/ 96 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_ende_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3797.8 words/s - loss: 0.1084
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3724.3 words/s - loss: 0.1280
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3505.3 words/s - loss: 0.1208
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3216.6 words/s - loss: 0.1216
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3023.7 words/s - loss: 0.1044
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2938.8 words/s - loss: 0.1086
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2862.1 words/s - loss: 0.1227
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2903.5 words/s - loss: 0.1282
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2705.9 words/s - loss: 0.1062
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2624.7 words/s - loss: 0.1065
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.409 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_ende_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 4065.8 words/s - loss: 0.1115
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3913.2 words/s - loss: 0.1052
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3920.7 words/s - loss: 0.1101
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3369.5 words/s - loss: 0.1341
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3313.2 words/s - loss: 0.1128
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3250.8 words/s - loss: 0.0983
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3104.7 words/s - loss: 0.1156
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2849.0 words/s - loss: 0.1050
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2886.2 words/s - loss: 0.1281
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2838.6 words/s - loss: 0.1007
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_ende_f2_8_8.txt
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_ende_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3165.9 words/s - loss: 0.1053
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3092.7 words/s - loss: 0.1002
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3144.5 words/s - loss: 0.1274
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3167.6 words/s - loss: 0.1116
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3124.2 words/s - loss: 0.1111
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3048.4 words/s - loss: 0.1207
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3062.3 words/s - loss: 0.0977
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2844.9 words/s - loss: 0.1162
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2723.3 words/s - loss: 0.1210
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2766.8 words/s - loss: 0.0999
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.397 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.426 w/ 96 queries
INFO:__main__:removing file tmp/mbert_ende_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_9.txt
INFO:__main__:[0.41116543694529417, 0.42366563532670537, 0.40935976543731706, 0.4239490728633369, 0.39691359668144627]
INFO:__main__:[0.4425645084606625, 0.4501869738493253, 0.4463052699409141, 0.4446489932011512, 0.42613249405729065]
INFO:__main__:0.42366563532670537
INFO:__main__:0.4446489932011512
INFO:__main__:best MAP: 0.434
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 724780.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 715898.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 701388.63it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 685097.51it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 697748.20it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 748154.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 426380.40it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 679063.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 581766.53it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 696103.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 137.47it/s]100%|██████████| 192/192 [00:00<00:00, 1376.06it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 127.05it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19832.20it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1277.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19258.79it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 113.47it/s]  9%|▉         | 18/192 [00:00<00:01, 128.06it/s]100%|██████████| 192/192 [00:00<00:00, 1148.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
100%|██████████| 192/192 [00:00<00:00, 1287.96it/s]
  0%|          | 0/192 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19484.79it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 19301.72it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 103.64it/s]  9%|▉         | 18/192 [00:00<00:01, 117.22it/s]100%|██████████| 192/192 [00:00<00:00, 1053.44it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1183.20it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 109.04it/s]100%|██████████| 192/192 [00:00<00:00, 19485.73it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 19799.05it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 1106.38it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 112.10it/s]  9%|▉         | 18/192 [00:00<00:01, 99.51it/s]100%|██████████| 192/192 [00:00<00:00, 19665.60it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1132.14it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
100%|██████████| 192/192 [00:00<00:00, 1011.34it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19152.07it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 18546.05it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 122.90it/s]100%|██████████| 192/192 [00:00<00:00, 1238.93it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19711.81it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3585.0 words/s - loss: 0.2476
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3434.8 words/s - loss: 0.2619
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3271.6 words/s - loss: 0.2507
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3142.8 words/s - loss: 0.2344
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2920.3 words/s - loss: 0.2280
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2830.3 words/s - loss: 0.2308
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2844.3 words/s - loss: 0.2265
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2855.2 words/s - loss: 0.2209
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2726.3 words/s - loss: 0.2346
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2645.6 words/s - loss: 0.2177
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 4006.2 words/s - loss: 0.1426
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3532.7 words/s - loss: 0.1595
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4202.4 words/s - loss: 0.1804
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3596.3 words/s - loss: 0.1665
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3057.4 words/s - loss: 0.1634
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3133.5 words/s - loss: 0.1519
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2645.2 words/s - loss: 0.1792
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2361.9 words/s - loss: 0.1759
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2872.9 words/s - loss: 0.1384
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2772.3 words/s - loss: 0.1689
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3261.6 words/s - loss: 0.1229
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3973.6 words/s - loss: 0.1362
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 4072.0 words/s - loss: 0.1327
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2895.0 words/s - loss: 0.1771
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3409.4 words/s - loss: 0.1597
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2576.0 words/s - loss: 0.1298
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2818.0 words/s - loss: 0.1524
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2987.0 words/s - loss: 0.1740
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2923.6 words/s - loss: 0.1283
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2640.1 words/s - loss: 0.1100
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3388.9 words/s - loss: 0.1468
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3996.7 words/s - loss: 0.1756
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2817.8 words/s - loss: 0.1344
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3420.9 words/s - loss: 0.1532
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2877.3 words/s - loss: 0.1154
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3633.2 words/s - loss: 0.1075
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3617.9 words/s - loss: 0.1248
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3056.7 words/s - loss: 0.1322
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3622.6 words/s - loss: 0.1551
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2552.9 words/s - loss: 0.1332
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3492.7 words/s - loss: 0.1353
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3294.0 words/s - loss: 0.1132
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3712.9 words/s - loss: 0.1162
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3117.8 words/s - loss: 0.1497
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3359.4 words/s - loss: 0.1095
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2526.2 words/s - loss: 0.1412
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3066.2 words/s - loss: 0.1420
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2443.8 words/s - loss: 0.1150
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3351.4 words/s - loss: 0.1279
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3509.7 words/s - loss: 0.1061
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3170.1 words/s - loss: 0.1306
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2407.5 words/s - loss: 0.1265
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3424.3 words/s - loss: 0.1151
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2856.6 words/s - loss: 0.1165
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2928.5 words/s - loss: 0.1312
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3004.4 words/s - loss: 0.1385
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3154.1 words/s - loss: 0.1169
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2610.4 words/s - loss: 0.0944
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3465.6 words/s - loss: 0.1136
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2557.3 words/s - loss: 0.1265
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.411 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.452 w/ 96 queries
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_ende_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3730.5 words/s - loss: 0.1199
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3661.2 words/s - loss: 0.1292
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3545.9 words/s - loss: 0.1241
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3471.8 words/s - loss: 0.1350
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3208.9 words/s - loss: 0.1014
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3110.8 words/s - loss: 0.1133
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3105.8 words/s - loss: 0.0988
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3060.4 words/s - loss: 0.1050
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2748.2 words/s - loss: 0.1156
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2504.1 words/s - loss: 0.1109
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.405 w/ 96 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_ende_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3987.4 words/s - loss: 0.1277
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3777.0 words/s - loss: 0.1168
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3293.8 words/s - loss: 0.1032
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3355.4 words/s - loss: 0.0998
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3217.7 words/s - loss: 0.1277
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3124.6 words/s - loss: 0.1094
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2992.3 words/s - loss: 0.1117
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2894.0 words/s - loss: 0.1209
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2840.0 words/s - loss: 0.0873
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2777.6 words/s - loss: 0.1139
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.416 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.454 w/ 96 queries
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_ende_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3879.8 words/s - loss: 0.1221
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3593.3 words/s - loss: 0.1000
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3423.7 words/s - loss: 0.1013
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3090.3 words/s - loss: 0.1017
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3049.8 words/s - loss: 0.1166
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3121.6 words/s - loss: 0.1153
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3066.4 words/s - loss: 0.1080
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2998.4 words/s - loss: 0.1411
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2918.7 words/s - loss: 0.1122
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2830.0 words/s - loss: 0.1048
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.444 w/ 96 queries
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_ende_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3407.5 words/s - loss: 0.1149
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3244.0 words/s - loss: 0.0981
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3170.6 words/s - loss: 0.1084
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3191.0 words/s - loss: 0.1069
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3085.1 words/s - loss: 0.1080
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3022.3 words/s - loss: 0.1168
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2955.0 words/s - loss: 0.0919
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2954.2 words/s - loss: 0.0936
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2920.9 words/s - loss: 0.1001
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2478.4 words/s - loss: 0.1220
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.403 w/ 96 queries
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_ende_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_ende_f1_2_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.441 w/ 96 queries
INFO:__main__:removing file tmp/mbert_ende_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_ende_f2_0_9.txt
INFO:__main__:[0.41137311766485046, 0.40487820822294046, 0.4162017721971693, 0.40843930804016776, 0.403212734836173]
INFO:__main__:[0.45216359002104395, 0.45119755547370993, 0.4542238114328813, 0.4440992548330274, 0.4410607424145823]
INFO:__main__:0.4162017721971693
INFO:__main__:0.4542238114328813
INFO:__main__:best MAP: 0.435

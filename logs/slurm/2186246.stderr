INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 596629.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1793970.92it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1833014.60it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 589783.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1733470.00it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1755379.59it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 583238.87it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 602491.38it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 1835099.75it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1866125.65it/s]
100%|██████████| 5000/5000 [00:00<00:00, 2040229.59it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:root:Number of labelled query-document pairs in [dev] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1861818.18it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 585551.31it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1794277.89it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1744573.66it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 591997.74it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1711681.36it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1709588.33it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 612575.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 600576.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1797507.50it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1790295.37it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1761868.44it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 1883218.39it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 584620.87it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1751860.33it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1760537.27it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 544177.69it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1719963.91it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1718977.05it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3787.1 words/s - loss: 0.4547
INFO:__main__:process[4]: evaluating epoch 0 on dev ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3604.1 words/s - loss: 0.4572
INFO:__main__:process[1]: evaluating epoch 0 on dev ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3565.3 words/s - loss: 0.4416
INFO:__main__:process[0]: evaluating epoch 0 on dev ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3491.1 words/s - loss: 0.4607
INFO:__main__:process[3]: evaluating epoch 0 on dev ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3493.6 words/s - loss: 0.4526
INFO:__main__:process[5]: evaluating epoch 0 on dev ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3309.3 words/s - loss: 0.4433
INFO:__main__:process[2]: evaluating epoch 0 on dev ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3253.5 words/s - loss: 0.4498
INFO:__main__:process[9]: evaluating epoch 0 on dev ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3014.5 words/s - loss: 0.4325
INFO:__main__:process[6]: evaluating epoch 0 on dev ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2917.0 words/s - loss: 0.4443
INFO:__main__:process[7]: evaluating epoch 0 on dev ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2498.5 words/s - loss: 0.4574
INFO:__main__:process[8]: evaluating epoch 0 on dev ...
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', 'bert-base-multilingual-uncased', '--dataset', 'wiki-clir', '--source_lang', 'fr', '--target_lang', 'de', '--batch_size', '8', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '3', '--seed', '611']' died with <Signals.SIGSEGV: 11>.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 572835.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1819181.12it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1874465.50it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 594464.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1879168.46it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1904597.22it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 617317.79it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1744138.39it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 580622.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1833014.60it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 1716163.67it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1691660.89it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 599066.47it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1709727.70it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1747917.99it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 547502.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1692480.03it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1685542.52it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 623836.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1760832.91it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1836224.50it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 627701.89it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1805088.66it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 561847.51it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1899766.28it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 1750690.37it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1752006.68it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 522602.61it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1696175.99it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1746753.29it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3613.3 words/s - loss: 0.2573
INFO:__main__:process[5]: evaluating epoch 0 on dev ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3551.1 words/s - loss: 0.2360
INFO:__main__:process[2]: evaluating epoch 0 on dev ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3245.1 words/s - loss: 0.2574
INFO:__main__:process[0]: evaluating epoch 0 on dev ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3233.5 words/s - loss: 0.2679
INFO:__main__:process[9]: evaluating epoch 0 on dev ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3129.2 words/s - loss: 0.2535
INFO:__main__:process[6]: evaluating epoch 0 on dev ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3101.3 words/s - loss: 0.2313
INFO:__main__:process[4]: evaluating epoch 0 on dev ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2987.2 words/s - loss: 0.2294
INFO:__main__:process[3]: evaluating epoch 0 on dev ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2780.7 words/s - loss: 0.2475
INFO:__main__:process[1]: evaluating epoch 0 on dev ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2637.4 words/s - loss: 0.2522
INFO:__main__:process[8]: evaluating epoch 0 on dev ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2672.3 words/s - loss: 0.2674
INFO:__main__:process[7]: evaluating epoch 0 on dev ...
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', '--dataset', 'wiki-clir', '--source_lang', 'fr', '--target_lang', 'de', '--batch_size', '8', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '3', '--seed', '611']' died with <Signals.SIGSEGV: 11>.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='wiki-clir', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 608293.31it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1820602.48it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1897875.11it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 626820.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1832213.87it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1757586.32it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 611058.28it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1778151.60it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1768703.72it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 631102.02it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 596493.54it/s]
  0%|          | 0/5000 [00:00<?, ?it/s]INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1976953.24it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1924875.63it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 2106843.48it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
100%|██████████| 5000/5000 [00:00<00:00, 2081953.74it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 576061.53it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1760389.49it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1834778.65it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 593841.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1755967.51it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1774090.18it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 609477.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1750105.98it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1802141.45it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 567672.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1710006.52it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1664010.16it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 534238.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1688528.18it/s]
INFO:root:Number of labelled query-document pairs in [dev] set: 0
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 1696587.65it/s]
INFO:root:Number of labelled query-document pairs in [test] set: 0
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 4242.5 words/s - loss: 0.2668
INFO:__main__:process[1]: evaluating epoch 0 on dev ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3622.0 words/s - loss: 0.2801
INFO:__main__:process[2]: evaluating epoch 0 on dev ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3579.0 words/s - loss: 0.2569
INFO:__main__:process[6]: evaluating epoch 0 on dev ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3316.7 words/s - loss: 0.2615
INFO:__main__:process[0]: evaluating epoch 0 on dev ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3187.4 words/s - loss: 0.2614
INFO:__main__:process[8]: evaluating epoch 0 on dev ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3166.1 words/s - loss: 0.2507
INFO:__main__:process[5]: evaluating epoch 0 on dev ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3028.4 words/s - loss: 0.2426
INFO:__main__:process[3]: evaluating epoch 0 on dev ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3005.3 words/s - loss: 0.2503
INFO:__main__:process[4]: evaluating epoch 0 on dev ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2927.1 words/s - loss: 0.2769
INFO:__main__:process[7]: evaluating epoch 0 on dev ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2754.9 words/s - loss: 0.2674
INFO:__main__:process[9]: evaluating epoch 0 on dev ...
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', '--dataset', 'wiki-clir', '--source_lang', 'fr', '--target_lang', 'de', '--batch_size', '8', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '3', '--seed', '611']' died with <Signals.SIGSEGV: 11>.

/var/lib/slurm-llnl/slurmd/job2186799/slurm_script: line 19: slogs/2186799.stdout: No such file or directory
/var/lib/slurm-llnl/slurmd/job2186799/slurm_script: line 20: slogs/2186799.stdout: No such file or directory
/var/lib/slurm-llnl/slurmd/job2186799/slurm_script: line 21: slogs/2186799.stdout: No such file or directory
/var/lib/slurm-llnl/slurmd/job2186799/slurm_script: line 22: slogs/2186799.stdout: No such file or directory
/var/lib/slurm-llnl/slurmd/job2186799/slurm_script: line 23: slogs/2186799.stdout: No such file or directory
 4: INFO - 08/08/20 08:28:48 - 0:00:00 - ============ Initialized logger ============
 4: INFO - 08/08/20 08:28:48 - 0:00:00 - accumulate_gradients: 1
 4:                                      amp: 2
 4:                                      batch_size: 4
 4:                                      clip_grad_norm: 5.0
 4:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 4:                                      dump_path: ./dumped/short_qlm_mix/2186799
 4:                                      epoch_size: 1000
 4:                                      exp_id: 2186799
 4:                                      exp_name: short_qlm_mix
 4:                                      fp16: True
 4:                                      global_rank: 4
 4:                                      is_master: False
 4:                                      is_slurm_job: True
 4:                                      lambda_qlm: 1.0
 4:                                      lambda_rr: 1.0
 4:                                      local_rank: 4
 4:                                      master_addr: dgx-1
 4:                                      master_port: 12223
 4:                                      max_epoch: 20
 4:                                      max_pairs: 100000
 4:                                      mlm_probability: 0.3
 4:                                      model_path: bert-base-multilingual-uncased
 4:                                      model_type: mbert
 4:                                      multi_gpu: True
 4:                                      multi_node: True
 4:                                      n_gpu_per_node: 8
 4:                                      n_nodes: 2
 4:                                      neg_val: 0
 4:                                      node_id: 0
 4:                                      num_neg: 1
 4:                                      optimizer: adam,lr=0.00001
 4:                                      qlm_mask_mode: mix
 4:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 4:                                      rr_steps: None
 4:                                      save_periodic: 0
 4:                                      slurm_debug: False
 4:                                      world_size: 16
 4: INFO - 08/08/20 08:28:48 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 4:                                      
 4: INFO - 08/08/20 08:28:48 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 4: 
 4: WARNING - 08/08/20 08:28:48 - 0:00:00 - Signal handler installed.
 8: INFO - 08/08/20 08:28:48 - 0:00:00 - ============ Initialized logger ============
 8: INFO - 08/08/20 08:28:48 - 0:00:00 - accumulate_gradients: 1
 8:                                      amp: 2
 8:                                      batch_size: 4
 8:                                      clip_grad_norm: 5.0
 8:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 8:                                      dump_path: ./dumped/short_qlm_mix/2186799
 8:                                      epoch_size: 1000
 8:                                      exp_id: 2186799
 8:                                      exp_name: short_qlm_mix
 8:                                      fp16: True
 8:                                      global_rank: 8
 8:                                      is_master: False
 8:                                      is_slurm_job: True
 8:                                      lambda_qlm: 1.0
 8:                                      lambda_rr: 1.0
 8:                                      local_rank: 0
 8:                                      master_addr: dgx-1
 8:                                      master_port: 12223
 8:                                      max_epoch: 20
 8:                                      max_pairs: 100000
 8:                                      mlm_probability: 0.3
 8:                                      model_path: bert-base-multilingual-uncased
 8:                                      model_type: mbert
 8:                                      multi_gpu: True
 8:                                      multi_node: True
 8:                                      n_gpu_per_node: 8
 8:                                      n_nodes: 2
 8:                                      neg_val: 0
 8:                                      node_id: 1
 8:                                      num_neg: 1
 8:                                      optimizer: adam,lr=0.00001
 8:                                      qlm_mask_mode: mix
 8:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 8:                                      rr_steps: None
 8:                                      save_periodic: 0
 8:                                      slurm_debug: False
 8:                                      world_size: 16
 8: INFO - 08/08/20 08:28:48 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 8:                                      
 8: INFO - 08/08/20 08:28:48 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 8: 
 8: WARNING - 08/08/20 08:28:48 - 0:00:00 - Signal handler installed.
13: INFO - 08/08/20 08:28:48 - 0:00:00 - ============ Initialized logger ============
13: INFO - 08/08/20 08:28:48 - 0:00:00 - accumulate_gradients: 1
13:                                      amp: 2
13:                                      batch_size: 4
13:                                      clip_grad_norm: 5.0
13:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
13:                                      dump_path: ./dumped/short_qlm_mix/2186799
13:                                      epoch_size: 1000
13:                                      exp_id: 2186799
13:                                      exp_name: short_qlm_mix
13:                                      fp16: True
13:                                      global_rank: 13
13:                                      is_master: False
13:                                      is_slurm_job: True
13:                                      lambda_qlm: 1.0
13:                                      lambda_rr: 1.0
13:                                      local_rank: 5
13:                                      master_addr: dgx-1
13:                                      master_port: 12223
13:                                      max_epoch: 20
13:                                      max_pairs: 100000
13:                                      mlm_probability: 0.3
13:                                      model_path: bert-base-multilingual-uncased
13:                                      model_type: mbert
13:                                      multi_gpu: True
13:                                      multi_node: True
13:                                      n_gpu_per_node: 8
13:                                      n_nodes: 2
13:                                      neg_val: 0
13:                                      node_id: 1
13:                                      num_neg: 1
13:                                      optimizer: adam,lr=0.00001
13:                                      qlm_mask_mode: mix
13:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
13:                                      rr_steps: None
13:                                      save_periodic: 0
13:                                      slurm_debug: False
13:                                      world_size: 16
13: INFO - 08/08/20 08:28:48 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
13:                                      
13: INFO - 08/08/20 08:28:48 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
13: 
13: WARNING - 08/08/20 08:28:48 - 0:00:00 - Signal handler installed.
14: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
14: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
14:                                      amp: 2
14:                                      batch_size: 4
14:                                      clip_grad_norm: 5.0
14:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
14:                                      dump_path: ./dumped/short_qlm_mix/2186799
14:                                      epoch_size: 1000
14:                                      exp_id: 2186799
14:                                      exp_name: short_qlm_mix
14:                                      fp16: True
14:                                      global_rank: 14
14:                                      is_master: False
14:                                      is_slurm_job: True
14:                                      lambda_qlm: 1.0
14:                                      lambda_rr: 1.0
14:                                      local_rank: 6
14:                                      master_addr: dgx-1
14:                                      master_port: 12223
14:                                      max_epoch: 20
14:                                      max_pairs: 100000
14:                                      mlm_probability: 0.3
14:                                      model_path: bert-base-multilingual-uncased
14:                                      model_type: mbert
14:                                      multi_gpu: True
14:                                      multi_node: True
14:                                      n_gpu_per_node: 8
14:                                      n_nodes: 2
14:                                      neg_val: 0
14:                                      node_id: 1
14:                                      num_neg: 1
14:                                      optimizer: adam,lr=0.00001
14:                                      qlm_mask_mode: mix
14:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
14:                                      rr_steps: None
14:                                      save_periodic: 0
14:                                      slurm_debug: False
14:                                      world_size: 16
14: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
14:                                      
10: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
14: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
14: 
10: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
10:                                      amp: 2
10:                                      batch_size: 4
10:                                      clip_grad_norm: 5.0
10:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
10:                                      dump_path: ./dumped/short_qlm_mix/2186799
10:                                      epoch_size: 1000
10:                                      exp_id: 2186799
10:                                      exp_name: short_qlm_mix
10:                                      fp16: True
10:                                      global_rank: 10
10:                                      is_master: False
10:                                      is_slurm_job: True
10:                                      lambda_qlm: 1.0
10:                                      lambda_rr: 1.0
10:                                      local_rank: 2
10:                                      master_addr: dgx-1
10:                                      master_port: 12223
10:                                      max_epoch: 20
10:                                      max_pairs: 100000
10:                                      mlm_probability: 0.3
10:                                      model_path: bert-base-multilingual-uncased
10:                                      model_type: mbert
10:                                      multi_gpu: True
10:                                      multi_node: True
10:                                      n_gpu_per_node: 8
10:                                      n_nodes: 2
10:                                      neg_val: 0
10:                                      node_id: 1
10:                                      num_neg: 1
10:                                      optimizer: adam,lr=0.00001
10:                                      qlm_mask_mode: mix
10:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
10:                                      rr_steps: None
10:                                      save_periodic: 0
10:                                      slurm_debug: False
10:                                      world_size: 16
14: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
10: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
10:                                      
10: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
10: 
10: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
12: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
12: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
12:                                      amp: 2
12:                                      batch_size: 4
12:                                      clip_grad_norm: 5.0
12:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
12:                                      dump_path: ./dumped/short_qlm_mix/2186799
12:                                      epoch_size: 1000
12:                                      exp_id: 2186799
12:                                      exp_name: short_qlm_mix
12:                                      fp16: True
12:                                      global_rank: 12
12:                                      is_master: False
12:                                      is_slurm_job: True
12:                                      lambda_qlm: 1.0
12:                                      lambda_rr: 1.0
12:                                      local_rank: 4
12:                                      master_addr: dgx-1
12:                                      master_port: 12223
12:                                      max_epoch: 20
12:                                      max_pairs: 100000
12:                                      mlm_probability: 0.3
12:                                      model_path: bert-base-multilingual-uncased
12:                                      model_type: mbert
12:                                      multi_gpu: True
12:                                      multi_node: True
12:                                      n_gpu_per_node: 8
12:                                      n_nodes: 2
12:                                      neg_val: 0
12:                                      node_id: 1
12:                                      num_neg: 1
12:                                      optimizer: adam,lr=0.00001
12:                                      qlm_mask_mode: mix
12:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
12:                                      rr_steps: None
12:                                      save_periodic: 0
12:                                      slurm_debug: False
12:                                      world_size: 16
11: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
12: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
12:                                      
12: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
12: 
11: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
11:                                      amp: 2
11:                                      batch_size: 4
11:                                      clip_grad_norm: 5.0
11:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
11:                                      dump_path: ./dumped/short_qlm_mix/2186799
11:                                      epoch_size: 1000
11:                                      exp_id: 2186799
11:                                      exp_name: short_qlm_mix
11:                                      fp16: True
11:                                      global_rank: 11
11:                                      is_master: False
11:                                      is_slurm_job: True
11:                                      lambda_qlm: 1.0
11:                                      lambda_rr: 1.0
11:                                      local_rank: 3
11:                                      master_addr: dgx-1
11:                                      master_port: 12223
11:                                      max_epoch: 20
11:                                      max_pairs: 100000
11:                                      mlm_probability: 0.3
11:                                      model_path: bert-base-multilingual-uncased
11:                                      model_type: mbert
11:                                      multi_gpu: True
11:                                      multi_node: True
11:                                      n_gpu_per_node: 8
11:                                      n_nodes: 2
11:                                      neg_val: 0
11:                                      node_id: 1
11:                                      num_neg: 1
11:                                      optimizer: adam,lr=0.00001
11:                                      qlm_mask_mode: mix
11:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
11:                                      rr_steps: None
11:                                      save_periodic: 0
11:                                      slurm_debug: False
11:                                      world_size: 16
12: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
11: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
11:                                      
 9: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
11: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
11: 
 9: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 9:                                      amp: 2
 9:                                      batch_size: 4
 9:                                      clip_grad_norm: 5.0
 9:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 9:                                      dump_path: ./dumped/short_qlm_mix/2186799
 9:                                      epoch_size: 1000
 9:                                      exp_id: 2186799
 9:                                      exp_name: short_qlm_mix
 9:                                      fp16: True
 9:                                      global_rank: 9
 9:                                      is_master: False
 9:                                      is_slurm_job: True
 9:                                      lambda_qlm: 1.0
 9:                                      lambda_rr: 1.0
 9:                                      local_rank: 1
 9:                                      master_addr: dgx-1
 9:                                      master_port: 12223
 9:                                      max_epoch: 20
 9:                                      max_pairs: 100000
 9:                                      mlm_probability: 0.3
 9:                                      model_path: bert-base-multilingual-uncased
 9:                                      model_type: mbert
 9:                                      multi_gpu: True
 9:                                      multi_node: True
 9:                                      n_gpu_per_node: 8
 9:                                      n_nodes: 2
 9:                                      neg_val: 0
 9:                                      node_id: 1
 9:                                      num_neg: 1
 9:                                      optimizer: adam,lr=0.00001
 9:                                      qlm_mask_mode: mix
 9:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 9:                                      rr_steps: None
 9:                                      save_periodic: 0
 9:                                      slurm_debug: False
 9:                                      world_size: 16
11: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 9: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 9:                                      
 9: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 9: 
 9: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
15: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
15: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
15:                                      amp: 2
15:                                      batch_size: 4
15:                                      clip_grad_norm: 5.0
15:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
15:                                      dump_path: ./dumped/short_qlm_mix/2186799
15:                                      epoch_size: 1000
15:                                      exp_id: 2186799
15:                                      exp_name: short_qlm_mix
15:                                      fp16: True
15:                                      global_rank: 15
15:                                      is_master: False
15:                                      is_slurm_job: True
15:                                      lambda_qlm: 1.0
15:                                      lambda_rr: 1.0
15:                                      local_rank: 7
15:                                      master_addr: dgx-1
15:                                      master_port: 12223
15:                                      max_epoch: 20
15:                                      max_pairs: 100000
15:                                      mlm_probability: 0.3
15:                                      model_path: bert-base-multilingual-uncased
15:                                      model_type: mbert
15:                                      multi_gpu: True
15:                                      multi_node: True
15:                                      n_gpu_per_node: 8
15:                                      n_nodes: 2
15:                                      neg_val: 0
15:                                      node_id: 1
15:                                      num_neg: 1
15:                                      optimizer: adam,lr=0.00001
15:                                      qlm_mask_mode: mix
15:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
15:                                      rr_steps: None
15:                                      save_periodic: 0
15:                                      slurm_debug: False
15:                                      world_size: 16
15: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
15:                                      
15: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
15: 
15: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
10: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
10: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
10:                                        "architectures": [
10:                                          "BertForMaskedLM"
10:                                        ],
10:                                        "attention_probs_dropout_prob": 0.1,
10:                                        "directionality": "bidi",
10:                                        "gradient_checkpointing": false,
10:                                        "hidden_act": "gelu",
10:                                        "hidden_dropout_prob": 0.1,
10:                                        "hidden_size": 768,
10:                                        "initializer_range": 0.02,
10:                                        "intermediate_size": 3072,
10:                                        "layer_norm_eps": 1e-12,
10:                                        "max_position_embeddings": 512,
10:                                        "model_type": "bert",
10:                                        "num_attention_heads": 12,
10:                                        "num_hidden_layers": 12,
10:                                        "pad_token_id": 0,
10:                                        "pooler_fc_size": 768,
10:                                        "pooler_num_attention_heads": 12,
10:                                        "pooler_num_fc_layers": 3,
10:                                        "pooler_size_per_head": 128,
10:                                        "pooler_type": "first_token_transform",
10:                                        "type_vocab_size": 2,
10:                                        "vocab_size": 105879
10:                                      }
10:                                      
15: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
15: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
15:                                        "architectures": [
15:                                          "BertForMaskedLM"
15:                                        ],
15:                                        "attention_probs_dropout_prob": 0.1,
15:                                        "directionality": "bidi",
15:                                        "gradient_checkpointing": false,
15:                                        "hidden_act": "gelu",
15:                                        "hidden_dropout_prob": 0.1,
15:                                        "hidden_size": 768,
15:                                        "initializer_range": 0.02,
15:                                        "intermediate_size": 3072,
15:                                        "layer_norm_eps": 1e-12,
15:                                        "max_position_embeddings": 512,
15:                                        "model_type": "bert",
15:                                        "num_attention_heads": 12,
15:                                        "num_hidden_layers": 12,
15:                                        "pad_token_id": 0,
15:                                        "pooler_fc_size": 768,
15:                                        "pooler_num_attention_heads": 12,
15:                                        "pooler_num_fc_layers": 3,
15:                                        "pooler_size_per_head": 128,
15:                                        "pooler_type": "first_token_transform",
15:                                        "type_vocab_size": 2,
15:                                        "vocab_size": 105879
15:                                      }
15:                                      
14: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
14: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
14:                                        "architectures": [
14:                                          "BertForMaskedLM"
14:                                        ],
14:                                        "attention_probs_dropout_prob": 0.1,
14:                                        "directionality": "bidi",
14:                                        "gradient_checkpointing": false,
14:                                        "hidden_act": "gelu",
14:                                        "hidden_dropout_prob": 0.1,
14:                                        "hidden_size": 768,
14:                                        "initializer_range": 0.02,
14:                                        "intermediate_size": 3072,
14:                                        "layer_norm_eps": 1e-12,
14:                                        "max_position_embeddings": 512,
14:                                        "model_type": "bert",
14:                                        "num_attention_heads": 12,
14:                                        "num_hidden_layers": 12,
14:                                        "pad_token_id": 0,
14:                                        "pooler_fc_size": 768,
14:                                        "pooler_num_attention_heads": 12,
14:                                        "pooler_num_fc_layers": 3,
14:                                        "pooler_size_per_head": 128,
14:                                        "pooler_type": "first_token_transform",
14:                                        "type_vocab_size": 2,
14:                                        "vocab_size": 105879
14:                                      }
14:                                      
 4: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 4: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
 4:                                        "architectures": [
 4:                                          "BertForMaskedLM"
 4:                                        ],
 4:                                        "attention_probs_dropout_prob": 0.1,
 4:                                        "directionality": "bidi",
 4:                                        "gradient_checkpointing": false,
 4:                                        "hidden_act": "gelu",
 4:                                        "hidden_dropout_prob": 0.1,
 4:                                        "hidden_size": 768,
 4:                                        "initializer_range": 0.02,
 4:                                        "intermediate_size": 3072,
 4:                                        "layer_norm_eps": 1e-12,
 4:                                        "max_position_embeddings": 512,
 4:                                        "model_type": "bert",
 4:                                        "num_attention_heads": 12,
 4:                                        "num_hidden_layers": 12,
 4:                                        "pad_token_id": 0,
 4:                                        "pooler_fc_size": 768,
 4:                                        "pooler_num_attention_heads": 12,
 4:                                        "pooler_num_fc_layers": 3,
 4:                                        "pooler_size_per_head": 128,
 4:                                        "pooler_type": "first_token_transform",
 4:                                        "type_vocab_size": 2,
 4:                                        "vocab_size": 105879
 4:                                      }
 4:                                      
13: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
13: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
13:                                        "architectures": [
13:                                          "BertForMaskedLM"
13:                                        ],
13:                                        "attention_probs_dropout_prob": 0.1,
13:                                        "directionality": "bidi",
13:                                        "gradient_checkpointing": false,
13:                                        "hidden_act": "gelu",
13:                                        "hidden_dropout_prob": 0.1,
13:                                        "hidden_size": 768,
13:                                        "initializer_range": 0.02,
13:                                        "intermediate_size": 3072,
13:                                        "layer_norm_eps": 1e-12,
13:                                        "max_position_embeddings": 512,
13:                                        "model_type": "bert",
13:                                        "num_attention_heads": 12,
13:                                        "num_hidden_layers": 12,
13:                                        "pad_token_id": 0,
13:                                        "pooler_fc_size": 768,
13:                                        "pooler_num_attention_heads": 12,
13:                                        "pooler_num_fc_layers": 3,
13:                                        "pooler_size_per_head": 128,
13:                                        "pooler_type": "first_token_transform",
13:                                        "type_vocab_size": 2,
13:                                        "vocab_size": 105879
13:                                      }
13:                                      
 9: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 9: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
 9:                                        "architectures": [
 9:                                          "BertForMaskedLM"
 9:                                        ],
 9:                                        "attention_probs_dropout_prob": 0.1,
 9:                                        "directionality": "bidi",
 9:                                        "gradient_checkpointing": false,
 9:                                        "hidden_act": "gelu",
 9:                                        "hidden_dropout_prob": 0.1,
 9:                                        "hidden_size": 768,
 9:                                        "initializer_range": 0.02,
 9:                                        "intermediate_size": 3072,
 9:                                        "layer_norm_eps": 1e-12,
 9:                                        "max_position_embeddings": 512,
 9:                                        "model_type": "bert",
 9:                                        "num_attention_heads": 12,
 9:                                        "num_hidden_layers": 12,
 9:                                        "pad_token_id": 0,
 9:                                        "pooler_fc_size": 768,
 9:                                        "pooler_num_attention_heads": 12,
 9:                                        "pooler_num_fc_layers": 3,
 9:                                        "pooler_size_per_head": 128,
 9:                                        "pooler_type": "first_token_transform",
 9:                                        "type_vocab_size": 2,
 9:                                        "vocab_size": 105879
 9:                                      }
 9:                                      
 8: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
12: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 8: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
 8:                                        "architectures": [
 8:                                          "BertForMaskedLM"
 8:                                        ],
 8:                                        "attention_probs_dropout_prob": 0.1,
 8:                                        "directionality": "bidi",
 8:                                        "gradient_checkpointing": false,
 8:                                        "hidden_act": "gelu",
 8:                                        "hidden_dropout_prob": 0.1,
 8:                                        "hidden_size": 768,
 8:                                        "initializer_range": 0.02,
 8:                                        "intermediate_size": 3072,
 8:                                        "layer_norm_eps": 1e-12,
 8:                                        "max_position_embeddings": 512,
 8:                                        "model_type": "bert",
 8:                                        "num_attention_heads": 12,
 8:                                        "num_hidden_layers": 12,
 8:                                        "pad_token_id": 0,
 8:                                        "pooler_fc_size": 768,
 8:                                        "pooler_num_attention_heads": 12,
 8:                                        "pooler_num_fc_layers": 3,
 8:                                        "pooler_size_per_head": 128,
 8:                                        "pooler_type": "first_token_transform",
 8:                                        "type_vocab_size": 2,
 8:                                        "vocab_size": 105879
 8:                                      }
 8:                                      
12: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
12:                                        "architectures": [
12:                                          "BertForMaskedLM"
12:                                        ],
12:                                        "attention_probs_dropout_prob": 0.1,
12:                                        "directionality": "bidi",
12:                                        "gradient_checkpointing": false,
12:                                        "hidden_act": "gelu",
12:                                        "hidden_dropout_prob": 0.1,
12:                                        "hidden_size": 768,
12:                                        "initializer_range": 0.02,
12:                                        "intermediate_size": 3072,
12:                                        "layer_norm_eps": 1e-12,
12:                                        "max_position_embeddings": 512,
12:                                        "model_type": "bert",
12:                                        "num_attention_heads": 12,
12:                                        "num_hidden_layers": 12,
12:                                        "pad_token_id": 0,
12:                                        "pooler_fc_size": 768,
12:                                        "pooler_num_attention_heads": 12,
12:                                        "pooler_num_fc_layers": 3,
12:                                        "pooler_size_per_head": 128,
12:                                        "pooler_type": "first_token_transform",
12:                                        "type_vocab_size": 2,
12:                                        "vocab_size": 105879
12:                                      }
12:                                      
11: INFO - 08/08/20 08:28:49 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
11: INFO - 08/08/20 08:28:49 - 0:00:00 - Model config BertConfig {
11:                                        "architectures": [
11:                                          "BertForMaskedLM"
11:                                        ],
11:                                        "attention_probs_dropout_prob": 0.1,
11:                                        "directionality": "bidi",
11:                                        "gradient_checkpointing": false,
11:                                        "hidden_act": "gelu",
11:                                        "hidden_dropout_prob": 0.1,
11:                                        "hidden_size": 768,
11:                                        "initializer_range": 0.02,
11:                                        "intermediate_size": 3072,
11:                                        "layer_norm_eps": 1e-12,
11:                                        "max_position_embeddings": 512,
11:                                        "model_type": "bert",
11:                                        "num_attention_heads": 12,
11:                                        "num_hidden_layers": 12,
11:                                        "pad_token_id": 0,
11:                                        "pooler_fc_size": 768,
11:                                        "pooler_num_attention_heads": 12,
11:                                        "pooler_num_fc_layers": 3,
11:                                        "pooler_size_per_head": 128,
11:                                        "pooler_type": "first_token_transform",
11:                                        "type_vocab_size": 2,
11:                                        "vocab_size": 105879
11:                                      }
11:                                      
 9: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
11: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
13: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 8: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
15: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
12: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
14: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
10: INFO - 08/08/20 08:28:49 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 4: INFO - 08/08/20 08:28:49 - 0:00:01 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 3: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
 7: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
 3: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 3:                                      amp: 2
 3:                                      batch_size: 4
 3:                                      clip_grad_norm: 5.0
 3:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 3:                                      dump_path: ./dumped/short_qlm_mix/2186799
 3:                                      epoch_size: 1000
 3:                                      exp_id: 2186799
 3:                                      exp_name: short_qlm_mix
 3:                                      fp16: True
 3:                                      global_rank: 3
 3:                                      is_master: False
 3:                                      is_slurm_job: True
 3:                                      lambda_qlm: 1.0
 3:                                      lambda_rr: 1.0
 3:                                      local_rank: 3
 3:                                      master_addr: dgx-1
 3:                                      master_port: 12223
 3:                                      max_epoch: 20
 3:                                      max_pairs: 100000
 3:                                      mlm_probability: 0.3
 3:                                      model_path: bert-base-multilingual-uncased
 3:                                      model_type: mbert
 3:                                      multi_gpu: True
 3:                                      multi_node: True
 3:                                      n_gpu_per_node: 8
 3:                                      n_nodes: 2
 3:                                      neg_val: 0
 3:                                      node_id: 0
 3:                                      num_neg: 1
 3:                                      optimizer: adam,lr=0.00001
 3:                                      qlm_mask_mode: mix
 3:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 3:                                      rr_steps: None
 3:                                      save_periodic: 0
 3:                                      slurm_debug: False
 3:                                      world_size: 16
 3: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 3:                                      
 7: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 7:                                      amp: 2
 7:                                      batch_size: 4
 7:                                      clip_grad_norm: 5.0
 7:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 7:                                      dump_path: ./dumped/short_qlm_mix/2186799
 7:                                      epoch_size: 1000
 7:                                      exp_id: 2186799
 7:                                      exp_name: short_qlm_mix
 7:                                      fp16: True
 7:                                      global_rank: 7
 7:                                      is_master: False
 7:                                      is_slurm_job: True
 7:                                      lambda_qlm: 1.0
 7:                                      lambda_rr: 1.0
 7:                                      local_rank: 7
 7:                                      master_addr: dgx-1
 7:                                      master_port: 12223
 7:                                      max_epoch: 20
 7:                                      max_pairs: 100000
 7:                                      mlm_probability: 0.3
 7:                                      model_path: bert-base-multilingual-uncased
 7:                                      model_type: mbert
 7:                                      multi_gpu: True
 7:                                      multi_node: True
 7:                                      n_gpu_per_node: 8
 7:                                      n_nodes: 2
 7:                                      neg_val: 0
 7:                                      node_id: 0
 7:                                      num_neg: 1
 7:                                      optimizer: adam,lr=0.00001
 7:                                      qlm_mask_mode: mix
 7:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 7:                                      rr_steps: None
 7:                                      save_periodic: 0
 7:                                      slurm_debug: False
 7:                                      world_size: 16
 3: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 7: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 7:                                      
 3: 
 7: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 7: 
 3: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 7: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 5: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
 5: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 5:                                      amp: 2
 5:                                      batch_size: 4
 5:                                      clip_grad_norm: 5.0
 5:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 5:                                      dump_path: ./dumped/short_qlm_mix/2186799
 5:                                      epoch_size: 1000
 5:                                      exp_id: 2186799
 5:                                      exp_name: short_qlm_mix
 5:                                      fp16: True
 5:                                      global_rank: 5
 5:                                      is_master: False
 5:                                      is_slurm_job: True
 5:                                      lambda_qlm: 1.0
 5:                                      lambda_rr: 1.0
 5:                                      local_rank: 5
 5:                                      master_addr: dgx-1
 5:                                      master_port: 12223
 5:                                      max_epoch: 20
 5:                                      max_pairs: 100000
 5:                                      mlm_probability: 0.3
 5:                                      model_path: bert-base-multilingual-uncased
 5:                                      model_type: mbert
 5:                                      multi_gpu: True
 5:                                      multi_node: True
 5:                                      n_gpu_per_node: 8
 5:                                      n_nodes: 2
 5:                                      neg_val: 0
 5:                                      node_id: 0
 5:                                      num_neg: 1
 5:                                      optimizer: adam,lr=0.00001
 5:                                      qlm_mask_mode: mix
 5:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 5:                                      rr_steps: None
 5:                                      save_periodic: 0
 5:                                      slurm_debug: False
 5:                                      world_size: 16
 5: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 5:                                      
 5: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 5: 
 5: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 6: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
 6: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 6:                                      amp: 2
 6:                                      batch_size: 4
 6:                                      clip_grad_norm: 5.0
 6:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 6:                                      dump_path: ./dumped/short_qlm_mix/2186799
 6:                                      epoch_size: 1000
 6:                                      exp_id: 2186799
 6:                                      exp_name: short_qlm_mix
 6:                                      fp16: True
 6:                                      global_rank: 6
 6:                                      is_master: False
 6:                                      is_slurm_job: True
 6:                                      lambda_qlm: 1.0
 6:                                      lambda_rr: 1.0
 6:                                      local_rank: 6
 6:                                      master_addr: dgx-1
 6:                                      master_port: 12223
 6:                                      max_epoch: 20
 6:                                      max_pairs: 100000
 6:                                      mlm_probability: 0.3
 6:                                      model_path: bert-base-multilingual-uncased
 6:                                      model_type: mbert
 6:                                      multi_gpu: True
 6:                                      multi_node: True
 6:                                      n_gpu_per_node: 8
 6:                                      n_nodes: 2
 6:                                      neg_val: 0
 6:                                      node_id: 0
 6:                                      num_neg: 1
 6:                                      optimizer: adam,lr=0.00001
 6:                                      qlm_mask_mode: mix
 6:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 6:                                      rr_steps: None
 6:                                      save_periodic: 0
 6:                                      slurm_debug: False
 6:                                      world_size: 16
 6: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 6:                                      
 6: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 6: 
 6: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 1: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
 2: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
 1: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 1:                                      amp: 2
 1:                                      batch_size: 4
 1:                                      clip_grad_norm: 5.0
 1:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 1:                                      dump_path: ./dumped/short_qlm_mix/2186799
 1:                                      epoch_size: 1000
 1:                                      exp_id: 2186799
 1:                                      exp_name: short_qlm_mix
 1:                                      fp16: True
 1:                                      global_rank: 1
 1:                                      is_master: False
 1:                                      is_slurm_job: True
 1:                                      lambda_qlm: 1.0
 1:                                      lambda_rr: 1.0
 1:                                      local_rank: 1
 1:                                      master_addr: dgx-1
 1:                                      master_port: 12223
 1:                                      max_epoch: 20
 1:                                      max_pairs: 100000
 1:                                      mlm_probability: 0.3
 1:                                      model_path: bert-base-multilingual-uncased
 1:                                      model_type: mbert
 1:                                      multi_gpu: True
 1:                                      multi_node: True
 1:                                      n_gpu_per_node: 8
 1:                                      n_nodes: 2
 1:                                      neg_val: 0
 1:                                      node_id: 0
 1:                                      num_neg: 1
 1:                                      optimizer: adam,lr=0.00001
 1:                                      qlm_mask_mode: mix
 1:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 1:                                      rr_steps: None
 1:                                      save_periodic: 0
 1:                                      slurm_debug: False
 1:                                      world_size: 16
 1: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 1:                                      
 1: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 2: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 2:                                      amp: 2
 2:                                      batch_size: 4
 2:                                      clip_grad_norm: 5.0
 2:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 2:                                      dump_path: ./dumped/short_qlm_mix/2186799
 2:                                      epoch_size: 1000
 2:                                      exp_id: 2186799
 2:                                      exp_name: short_qlm_mix
 2:                                      fp16: True
 2:                                      global_rank: 2
 2:                                      is_master: False
 2:                                      is_slurm_job: True
 2:                                      lambda_qlm: 1.0
 2:                                      lambda_rr: 1.0
 2:                                      local_rank: 2
 2:                                      master_addr: dgx-1
 2:                                      master_port: 12223
 2:                                      max_epoch: 20
 2:                                      max_pairs: 100000
 2:                                      mlm_probability: 0.3
 2:                                      model_path: bert-base-multilingual-uncased
 2:                                      model_type: mbert
 2:                                      multi_gpu: True
 2:                                      multi_node: True
 2:                                      n_gpu_per_node: 8
 2:                                      n_nodes: 2
 2:                                      neg_val: 0
 2:                                      node_id: 0
 2:                                      num_neg: 1
 2:                                      optimizer: adam,lr=0.00001
 2:                                      qlm_mask_mode: mix
 2:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 2:                                      rr_steps: None
 2:                                      save_periodic: 0
 2:                                      slurm_debug: False
 2:                                      world_size: 16
 0: INFO - 08/08/20 08:28:49 - 0:00:00 - ============ Initialized logger ============
 1: 
 2: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 2:                                      
 2: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 1: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 2: 
 0: INFO - 08/08/20 08:28:49 - 0:00:00 - accumulate_gradients: 1
 0:                                      amp: 2
 0:                                      batch_size: 4
 0:                                      clip_grad_norm: 5.0
 0:                                      command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223 --exp_id "2186799"
 0:                                      dump_path: ./dumped/short_qlm_mix/2186799
 0:                                      epoch_size: 1000
 0:                                      exp_id: 2186799
 0:                                      exp_name: short_qlm_mix
 0:                                      fp16: True
 0:                                      global_rank: 0
 0:                                      is_master: True
 0:                                      is_slurm_job: True
 0:                                      lambda_qlm: 1.0
 0:                                      lambda_rr: 1.0
 0:                                      local_rank: 0
 0:                                      master_addr: dgx-1
 0:                                      master_port: 12223
 0:                                      max_epoch: 20
 0:                                      max_pairs: 100000
 0:                                      mlm_probability: 0.3
 0:                                      model_path: bert-base-multilingual-uncased
 0:                                      model_type: mbert
 0:                                      multi_gpu: True
 0:                                      multi_node: True
 0:                                      n_gpu_per_node: 8
 0:                                      n_nodes: 2
 0:                                      neg_val: 0
 0:                                      node_id: 0
 0:                                      num_neg: 1
 0:                                      optimizer: adam,lr=0.00001
 0:                                      qlm_mask_mode: mix
 0:                                      qlm_steps: ['enfr', 'enes', 'ende', 'fres', 'frde', 'esde']
 0:                                      rr_steps: None
 0:                                      save_periodic: 0
 0:                                      slurm_debug: False
 0:                                      world_size: 16
 0: INFO - 08/08/20 08:28:49 - 0:00:00 - The experiment will be stored in ./dumped/short_qlm_mix/2186799
 0:                                      
 2: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 0: INFO - 08/08/20 08:28:49 - 0:00:00 - Running command: python train.py --exp_name short_qlm_mix --fp16 true --amp 2 --model_type mbert --model_path 'bert-base-multilingual-uncased' --batch_size 4 --qlm_mask_mode mix --mlm_probability '0.30' --optimizer 'adam,lr=0.00001' --clip_grad_norm 5 --epoch_size 1000 --max_epoch 20 --accumulate_gradients 1 --lambda_qlm 1 --lambda_rr 1 --num_neg 1 --qlm_steps 'enfr,enes,ende,fres,frde,esde' --max_pairs 100000 --master_port 12223
 0: 
 0: WARNING - 08/08/20 08:28:49 - 0:00:00 - Signal handler installed.
 7: INFO - 08/08/20 08:28:50 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 7: INFO - 08/08/20 08:28:50 - 0:00:00 - Model config BertConfig {
 7:                                        "architectures": [
 7:                                          "BertForMaskedLM"
 7:                                        ],
 7:                                        "attention_probs_dropout_prob": 0.1,
 7:                                        "directionality": "bidi",
 7:                                        "gradient_checkpointing": false,
 7:                                        "hidden_act": "gelu",
 7:                                        "hidden_dropout_prob": 0.1,
 7:                                        "hidden_size": 768,
 7:                                        "initializer_range": 0.02,
 7:                                        "intermediate_size": 3072,
 7:                                        "layer_norm_eps": 1e-12,
 7:                                        "max_position_embeddings": 512,
 7:                                        "model_type": "bert",
 7:                                        "num_attention_heads": 12,
 7:                                        "num_hidden_layers": 12,
 7:                                        "pad_token_id": 0,
 7:                                        "pooler_fc_size": 768,
 7:                                        "pooler_num_attention_heads": 12,
 7:                                        "pooler_num_fc_layers": 3,
 7:                                        "pooler_size_per_head": 128,
 7:                                        "pooler_type": "first_token_transform",
 7:                                        "type_vocab_size": 2,
 7:                                        "vocab_size": 105879
 7:                                      }
 7:                                      
 0: INFO - 08/08/20 08:28:50 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 0: INFO - 08/08/20 08:28:50 - 0:00:00 - Model config BertConfig {
 0:                                        "architectures": [
 0:                                          "BertForMaskedLM"
 0:                                        ],
 0:                                        "attention_probs_dropout_prob": 0.1,
 0:                                        "directionality": "bidi",
 0:                                        "gradient_checkpointing": false,
 0:                                        "hidden_act": "gelu",
 0:                                        "hidden_dropout_prob": 0.1,
 0:                                        "hidden_size": 768,
 0:                                        "initializer_range": 0.02,
 0:                                        "intermediate_size": 3072,
 0:                                        "layer_norm_eps": 1e-12,
 0:                                        "max_position_embeddings": 512,
 0:                                        "model_type": "bert",
 0:                                        "num_attention_heads": 12,
 0:                                        "num_hidden_layers": 12,
 0:                                        "pad_token_id": 0,
 0:                                        "pooler_fc_size": 768,
 0:                                        "pooler_num_attention_heads": 12,
 0:                                        "pooler_num_fc_layers": 3,
 0:                                        "pooler_size_per_head": 128,
 0:                                        "pooler_type": "first_token_transform",
 0:                                        "type_vocab_size": 2,
 0:                                        "vocab_size": 105879
 0:                                      }
 0:                                      
 6: INFO - 08/08/20 08:28:50 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 6: INFO - 08/08/20 08:28:50 - 0:00:00 - Model config BertConfig {
 6:                                        "architectures": [
 6:                                          "BertForMaskedLM"
 6:                                        ],
 6:                                        "attention_probs_dropout_prob": 0.1,
 6:                                        "directionality": "bidi",
 6:                                        "gradient_checkpointing": false,
 6:                                        "hidden_act": "gelu",
 6:                                        "hidden_dropout_prob": 0.1,
 6:                                        "hidden_size": 768,
 6:                                        "initializer_range": 0.02,
 6:                                        "intermediate_size": 3072,
 6:                                        "layer_norm_eps": 1e-12,
 6:                                        "max_position_embeddings": 512,
 6:                                        "model_type": "bert",
 6:                                        "num_attention_heads": 12,
 6:                                        "num_hidden_layers": 12,
 6:                                        "pad_token_id": 0,
 6:                                        "pooler_fc_size": 768,
 6:                                        "pooler_num_attention_heads": 12,
 6:                                        "pooler_num_fc_layers": 3,
 6:                                        "pooler_size_per_head": 128,
 6:                                        "pooler_type": "first_token_transform",
 6:                                        "type_vocab_size": 2,
 6:                                        "vocab_size": 105879
 6:                                      }
 6:                                      
 3: INFO - 08/08/20 08:28:50 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 3: INFO - 08/08/20 08:28:50 - 0:00:00 - Model config BertConfig {
 3:                                        "architectures": [
 3:                                          "BertForMaskedLM"
 3:                                        ],
 3:                                        "attention_probs_dropout_prob": 0.1,
 3:                                        "directionality": "bidi",
 3:                                        "gradient_checkpointing": false,
 3:                                        "hidden_act": "gelu",
 3:                                        "hidden_dropout_prob": 0.1,
 3:                                        "hidden_size": 768,
 3:                                        "initializer_range": 0.02,
 3:                                        "intermediate_size": 3072,
 3:                                        "layer_norm_eps": 1e-12,
 3:                                        "max_position_embeddings": 512,
 3:                                        "model_type": "bert",
 3:                                        "num_attention_heads": 12,
 3:                                        "num_hidden_layers": 12,
 3:                                        "pad_token_id": 0,
 3:                                        "pooler_fc_size": 768,
 3:                                        "pooler_num_attention_heads": 12,
 3:                                        "pooler_num_fc_layers": 3,
 3:                                        "pooler_size_per_head": 128,
 3:                                        "pooler_type": "first_token_transform",
 3:                                        "type_vocab_size": 2,
 3:                                        "vocab_size": 105879
 3:                                      }
 3:                                      
 2: INFO - 08/08/20 08:28:50 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 5: INFO - 08/08/20 08:28:50 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 2: INFO - 08/08/20 08:28:50 - 0:00:00 - Model config BertConfig {
 2:                                        "architectures": [
 2:                                          "BertForMaskedLM"
 2:                                        ],
 2:                                        "attention_probs_dropout_prob": 0.1,
 2:                                        "directionality": "bidi",
 2:                                        "gradient_checkpointing": false,
 2:                                        "hidden_act": "gelu",
 2:                                        "hidden_dropout_prob": 0.1,
 2:                                        "hidden_size": 768,
 2:                                        "initializer_range": 0.02,
 2:                                        "intermediate_size": 3072,
 2:                                        "layer_norm_eps": 1e-12,
 2:                                        "max_position_embeddings": 512,
 2:                                        "model_type": "bert",
 2:                                        "num_attention_heads": 12,
 2:                                        "num_hidden_layers": 12,
 2:                                        "pad_token_id": 0,
 2:                                        "pooler_fc_size": 768,
 2:                                        "pooler_num_attention_heads": 12,
 2:                                        "pooler_num_fc_layers": 3,
 2:                                        "pooler_size_per_head": 128,
 2:                                        "pooler_type": "first_token_transform",
 2:                                        "type_vocab_size": 2,
 2:                                        "vocab_size": 105879
 2:                                      }
 2:                                      
 5: INFO - 08/08/20 08:28:50 - 0:00:00 - Model config BertConfig {
 5:                                        "architectures": [
 5:                                          "BertForMaskedLM"
 5:                                        ],
 5:                                        "attention_probs_dropout_prob": 0.1,
 5:                                        "directionality": "bidi",
 5:                                        "gradient_checkpointing": false,
 5:                                        "hidden_act": "gelu",
 5:                                        "hidden_dropout_prob": 0.1,
 5:                                        "hidden_size": 768,
 5:                                        "initializer_range": 0.02,
 5:                                        "intermediate_size": 3072,
 5:                                        "layer_norm_eps": 1e-12,
 5:                                        "max_position_embeddings": 512,
 5:                                        "model_type": "bert",
 5:                                        "num_attention_heads": 12,
 5:                                        "num_hidden_layers": 12,
 5:                                        "pad_token_id": 0,
 5:                                        "pooler_fc_size": 768,
 5:                                        "pooler_num_attention_heads": 12,
 5:                                        "pooler_num_fc_layers": 3,
 5:                                        "pooler_size_per_head": 128,
 5:                                        "pooler_type": "first_token_transform",
 5:                                        "type_vocab_size": 2,
 5:                                        "vocab_size": 105879
 5:                                      }
 5:                                      
 1: INFO - 08/08/20 08:28:50 - 0:00:00 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
 1: INFO - 08/08/20 08:28:50 - 0:00:00 - Model config BertConfig {
 1:                                        "architectures": [
 1:                                          "BertForMaskedLM"
 1:                                        ],
 1:                                        "attention_probs_dropout_prob": 0.1,
 1:                                        "directionality": "bidi",
 1:                                        "gradient_checkpointing": false,
 1:                                        "hidden_act": "gelu",
 1:                                        "hidden_dropout_prob": 0.1,
 1:                                        "hidden_size": 768,
 1:                                        "initializer_range": 0.02,
 1:                                        "intermediate_size": 3072,
 1:                                        "layer_norm_eps": 1e-12,
 1:                                        "max_position_embeddings": 512,
 1:                                        "model_type": "bert",
 1:                                        "num_attention_heads": 12,
 1:                                        "num_hidden_layers": 12,
 1:                                        "pad_token_id": 0,
 1:                                        "pooler_fc_size": 768,
 1:                                        "pooler_num_attention_heads": 12,
 1:                                        "pooler_num_fc_layers": 3,
 1:                                        "pooler_size_per_head": 128,
 1:                                        "pooler_type": "first_token_transform",
 1:                                        "type_vocab_size": 2,
 1:                                        "vocab_size": 105879
 1:                                      }
 1:                                      
 6: INFO - 08/08/20 08:28:50 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 0: INFO - 08/08/20 08:28:50 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 1: INFO - 08/08/20 08:28:50 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 2: INFO - 08/08/20 08:28:50 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 7: INFO - 08/08/20 08:28:50 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 3: INFO - 08/08/20 08:28:50 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 5: INFO - 08/08/20 08:28:50 - 0:00:00 - loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
 4: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 4:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 4:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 4: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 4:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 8: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 8:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 8:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 8: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 8:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 9: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 9:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 9:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 9: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 9:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
15: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
15:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
15:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
15: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
15:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
11:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
11:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
11:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
13: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
13:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
13:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
13: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
13:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
10:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
10:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
10:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
12:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
12:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
12: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
12:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14: WARNING - 08/08/20 08:28:56 - 0:00:08 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
14:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
14:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
14: WARNING - 08/08/20 08:28:56 - 0:00:08 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
14:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 5: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 5:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 5:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 5: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 5:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 7: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 7:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 7:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 7: WARNING - 08/08/20 08:28:56 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 7:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 1: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 1:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 1:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 1: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 1:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 0: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 0:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 0:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 0: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 0:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 2: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 2:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 2:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 2: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 2:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 6: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 6:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 6:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 6: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 6:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 3: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
 3:                                         - This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
 3:                                         - This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 3: WARNING - 08/08/20 08:28:57 - 0:00:07 - Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
 3:                                         You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 4: INFO - 08/08/20 08:29:00 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 4: INFO - 08/08/20 08:29:00 - 0:00:11 - Creating data collator for relevance ranking:
 4: INFO - 08/08/20 08:29:00 - 0:00:11 -     max_len = 512
 4: INFO - 08/08/20 08:29:00 - 0:00:11 -     glb_att = False
 4: INFO - 08/08/20 08:29:00 - 0:00:11 - Creating data collator for query language model:
 4: INFO - 08/08/20 08:29:00 - 0:00:11 -     mlm_prob = 0.3
 4: INFO - 08/08/20 08:29:00 - 0:00:11 -     max_len = 512
 4: INFO - 08/08/20 08:29:00 - 0:00:11 -     glb_att = False
 4: INFO - 08/08/20 08:29:00 - 0:00:11 - Found 207 parameters in model.
 4: INFO - 08/08/20 08:29:00 - 0:00:11 - Optimizers: model
 4: INFO - 08/08/20 08:29:00 - 0:00:11 - Using apex.parallel.DistributedDataParallel ...
 9: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 8: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
15: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
11: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
10: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
14: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
13: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
12: INFO - 08/08/20 08:29:00 - 0:00:12 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 9: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
 9: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
 9: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
 9: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
 9: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
 9: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
 9: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
 9: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
 8: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
 8: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
 8: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
 8: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
 8: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
 8: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
 8: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
 8: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
 9: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
11: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
11: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
11: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
11: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
11: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
11: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
11: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
11: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
15: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
15: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
15: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
15: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
15: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
15: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
15: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
15: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
10: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
10: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
10: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
10: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
10: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
10: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
10: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
10: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
 8: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
14: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
14: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
14: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
14: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
14: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
14: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
14: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
11: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
14: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
15: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
10: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
13: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
13: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
13: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
13: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
13: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
13: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
13: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
13: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
12: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for relevance ranking:
12: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
12: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
12: INFO - 08/08/20 08:29:00 - 0:00:12 - Creating data collator for query language model:
12: INFO - 08/08/20 08:29:00 - 0:00:12 -     mlm_prob = 0.3
12: INFO - 08/08/20 08:29:00 - 0:00:12 -     max_len = 512
12: INFO - 08/08/20 08:29:00 - 0:00:12 -     glb_att = False
12: INFO - 08/08/20 08:29:00 - 0:00:12 - Found 207 parameters in model.
 9: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
14: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
 8: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
13: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
12: INFO - 08/08/20 08:29:00 - 0:00:12 - Optimizers: model
11: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
15: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
10: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
14: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
13: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
12: INFO - 08/08/20 08:29:00 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
 1: INFO - 08/08/20 08:29:01 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 0: INFO - 08/08/20 08:29:01 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 5: INFO - 08/08/20 08:29:01 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 7: INFO - 08/08/20 08:29:01 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 1: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for relevance ranking:
 1: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 1: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 1: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for query language model:
 1: INFO - 08/08/20 08:29:01 - 0:00:11 -     mlm_prob = 0.3
 1: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 1: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 1: INFO - 08/08/20 08:29:01 - 0:00:11 - Found 207 parameters in model.
 1: INFO - 08/08/20 08:29:01 - 0:00:11 - Optimizers: model
 0: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for relevance ranking:
 0: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 0: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 0: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for query language model:
 0: INFO - 08/08/20 08:29:01 - 0:00:11 -     mlm_prob = 0.3
 0: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 0: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 0: INFO - 08/08/20 08:29:01 - 0:00:11 - Found 207 parameters in model.
 6: INFO - 08/08/20 08:29:01 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 2: INFO - 08/08/20 08:29:01 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 0: INFO - 08/08/20 08:29:01 - 0:00:11 - Optimizers: model
 5: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for relevance ranking:
 5: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 5: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 5: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for query language model:
 5: INFO - 08/08/20 08:29:01 - 0:00:11 -     mlm_prob = 0.3
 5: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 5: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 5: INFO - 08/08/20 08:29:01 - 0:00:11 - Found 207 parameters in model.
 7: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for relevance ranking:
 7: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 7: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 7: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for query language model:
 7: INFO - 08/08/20 08:29:01 - 0:00:11 -     mlm_prob = 0.3
 7: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 7: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 7: INFO - 08/08/20 08:29:01 - 0:00:11 - Found 207 parameters in model.
 3: INFO - 08/08/20 08:29:01 - 0:00:11 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
 5: INFO - 08/08/20 08:29:01 - 0:00:11 - Optimizers: model
 1: INFO - 08/08/20 08:29:01 - 0:00:11 - Using apex.parallel.DistributedDataParallel ...
 7: INFO - 08/08/20 08:29:01 - 0:00:11 - Optimizers: model
 0: INFO - 08/08/20 08:29:01 - 0:00:11 - Using apex.parallel.DistributedDataParallel ...
 6: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for relevance ranking:
 6: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 6: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 6: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for query language model:
 6: INFO - 08/08/20 08:29:01 - 0:00:11 -     mlm_prob = 0.3
 6: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 6: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 6: INFO - 08/08/20 08:29:01 - 0:00:11 - Found 207 parameters in model.
 5: INFO - 08/08/20 08:29:01 - 0:00:11 - Using apex.parallel.DistributedDataParallel ...
 2: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for relevance ranking:
 2: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 2: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 2: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for query language model:
 2: INFO - 08/08/20 08:29:01 - 0:00:11 -     mlm_prob = 0.3
 2: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 2: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 2: INFO - 08/08/20 08:29:01 - 0:00:11 - Found 207 parameters in model.
 7: INFO - 08/08/20 08:29:01 - 0:00:11 - Using apex.parallel.DistributedDataParallel ...
 6: INFO - 08/08/20 08:29:01 - 0:00:11 - Optimizers: model
 2: INFO - 08/08/20 08:29:01 - 0:00:11 - Optimizers: model
 3: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for relevance ranking:
 3: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 3: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 3: INFO - 08/08/20 08:29:01 - 0:00:11 - Creating data collator for query language model:
 3: INFO - 08/08/20 08:29:01 - 0:00:11 -     mlm_prob = 0.3
 3: INFO - 08/08/20 08:29:01 - 0:00:11 -     max_len = 512
 3: INFO - 08/08/20 08:29:01 - 0:00:11 -     glb_att = False
 3: INFO - 08/08/20 08:29:01 - 0:00:11 - Found 207 parameters in model.
 3: INFO - 08/08/20 08:29:01 - 0:00:12 - Optimizers: model
 6: INFO - 08/08/20 08:29:01 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
 2: INFO - 08/08/20 08:29:01 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
 3: INFO - 08/08/20 08:29:01 - 0:00:12 - Using apex.parallel.DistributedDataParallel ...
 4: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
 7: INFO - 08/08/20 08:29:02 - 0:00:12 - ============ Starting epoch 0 ... ============
 6: INFO - 08/08/20 08:29:02 - 0:00:12 - ============ Starting epoch 0 ... ============
15: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
12: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
14: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
 9: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
13: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
 3: INFO - 08/08/20 08:29:02 - 0:00:12 - ============ Starting epoch 0 ... ============
 5: INFO - 08/08/20 08:29:02 - 0:00:12 - ============ Starting epoch 0 ... ============
 1: INFO - 08/08/20 08:29:02 - 0:00:12 - ============ Starting epoch 0 ... ============
11: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
 2: INFO - 08/08/20 08:29:02 - 0:00:12 - ============ Starting epoch 0 ... ============
10: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
 8: INFO - 08/08/20 08:29:02 - 0:00:14 - ============ Starting epoch 0 ... ============
 0: INFO - 08/08/20 08:29:02 - 0:00:13 - ============ Starting epoch 0 ... ============
 3: INFO - 08/08/20 08:29:09 - 0:00:19 - # parallel sections pairs: 100000
 3: INFO - 08/08/20 08:29:09 - 0:00:19 - Created new training data iterator (qlm,esde) ...
 7: INFO - 08/08/20 08:29:09 - 0:00:19 - # parallel sections pairs: 100000
 7: INFO - 08/08/20 08:29:09 - 0:00:19 - Created new training data iterator (qlm,fres) ...
13: INFO - 08/08/20 08:29:09 - 0:00:21 - # parallel sections pairs: 100000
 7: Traceback (most recent call last):
 7:   File "train.py", line 156, in <module>
 7:     main(params)
 7:   File "train.py", line 135, in main
 7:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 7:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 7:     inputs = self.get_batch("qlm", lang_pair)
 7:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 7:     x = next(iterator)
 7:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 7:     data = self._next_data()
 7:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 7:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 7:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 7:     return self.collate_fn(data)
 7:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 7:     assert False
 7: AssertionError
13: INFO - 08/08/20 08:29:09 - 0:00:21 - Created new training data iterator (qlm,fres) ...
 3: Traceback (most recent call last):
 3:   File "train.py", line 156, in <module>
 3:     main(params)
 3:   File "train.py", line 135, in main
 3:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 3:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 3:     inputs = self.get_batch("qlm", lang_pair)
 3:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 3:     x = next(iterator)
 3:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 3:     data = self._next_data()
 3:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 3:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 3:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 3:     return self.collate_fn(data)
 3:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 3:     assert False
 3: AssertionError
13: Traceback (most recent call last):
13:   File "train.py", line 156, in <module>
13:     main(params)
13:   File "train.py", line 135, in main
13:     trainer.qlm_step(lang_pair, params.lambda_qlm)
13:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
13:     inputs = self.get_batch("qlm", lang_pair)
13:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
13:     x = next(iterator)
13:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
13:     data = self._next_data()
13:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
13:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
13:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
13:     return self.collate_fn(data)
13:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
13:     assert False
13: AssertionError
10: INFO - 08/08/20 08:29:10 - 0:00:21 - # parallel sections pairs: 100000
10: INFO - 08/08/20 08:29:10 - 0:00:21 - Created new training data iterator (qlm,esde) ...
10: Traceback (most recent call last):
10:   File "train.py", line 156, in <module>
10:     main(params)
10:   File "train.py", line 135, in main
10:     trainer.qlm_step(lang_pair, params.lambda_qlm)
10:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
10:     inputs = self.get_batch("qlm", lang_pair)
10:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
10:     x = next(iterator)
10:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
10:     data = self._next_data()
10:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
10:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
10:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
10:     return self.collate_fn(data)
10:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
10:     assert False
10: AssertionError
srun: error: dgx-1: tasks 3,7: Exited with exit code 1
srun: error: dgx-2: task 13: Exited with exit code 1
srun: error: dgx-2: task 10: Exited with exit code 1
 0: INFO - 08/08/20 08:29:11 - 0:00:21 - # parallel sections pairs: 100000
 0: INFO - 08/08/20 08:29:11 - 0:00:21 - Created new training data iterator (qlm,frde) ...
 0: Traceback (most recent call last):
 0:   File "train.py", line 156, in <module>
 0:     main(params)
 0:   File "train.py", line 135, in main
 0:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 0:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 0:     inputs = self.get_batch("qlm", lang_pair)
 0:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 0:     x = next(iterator)
 0:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 0:     data = self._next_data()
 0:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 0:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 0:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 0:     return self.collate_fn(data)
 0:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 0:     assert False
 0: AssertionError
 1: INFO - 08/08/20 08:29:11 - 0:00:22 - # parallel sections pairs: 100000
 1: INFO - 08/08/20 08:29:11 - 0:00:22 - Created new training data iterator (qlm,frde) ...
14: INFO - 08/08/20 08:29:11 - 0:00:23 - # parallel sections pairs: 100000
14: INFO - 08/08/20 08:29:11 - 0:00:23 - Created new training data iterator (qlm,frde) ...
 1: Traceback (most recent call last):
 1:   File "train.py", line 156, in <module>
 1:     main(params)
 1:   File "train.py", line 135, in main
 1:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 1:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 1:     inputs = self.get_batch("qlm", lang_pair)
 1:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 1:     x = next(iterator)
 1:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 1:     data = self._next_data()
 1:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 1:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 1:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 1:     return self.collate_fn(data)
 1:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 1:     assert False
 1: AssertionError
14: Traceback (most recent call last):
14:   File "train.py", line 156, in <module>
14:     main(params)
14:   File "train.py", line 135, in main
14:     trainer.qlm_step(lang_pair, params.lambda_qlm)
14:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
14:     inputs = self.get_batch("qlm", lang_pair)
14:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
14:     x = next(iterator)
14:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
14:     data = self._next_data()
14:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
14:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
14:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
14:     return self.collate_fn(data)
14:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
14:     assert False
14: AssertionError
 6: INFO - 08/08/20 08:29:12 - 0:00:23 - # parallel sections pairs: 100000
15: INFO - 08/08/20 08:29:12 - 0:00:24 - # parallel sections pairs: 100000
 6: INFO - 08/08/20 08:29:12 - 0:00:23 - Created new training data iterator (qlm,enes) ...
15: INFO - 08/08/20 08:29:12 - 0:00:24 - Created new training data iterator (qlm,enes) ...
srun: error: dgx-1: task 0: Exited with exit code 1
 9: INFO - 08/08/20 08:29:12 - 0:00:24 - # parallel sections pairs: 100000
 9: INFO - 08/08/20 08:29:12 - 0:00:24 - Created new training data iterator (qlm,enes) ...
 6: Traceback (most recent call last):
 6:   File "train.py", line 156, in <module>
 6:     main(params)
 6:   File "train.py", line 135, in main
 6:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 6:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 6:     inputs = self.get_batch("qlm", lang_pair)
 6:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 6:     x = next(iterator)
 6:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 6:     data = self._next_data()
 6:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 6:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 6:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 6:     return self.collate_fn(data)
 6:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 6:     assert False
 6: AssertionError
15: Traceback (most recent call last):
15:   File "train.py", line 156, in <module>
15:     main(params)
15:   File "train.py", line 135, in main
15:     trainer.qlm_step(lang_pair, params.lambda_qlm)
15:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
15:     inputs = self.get_batch("qlm", lang_pair)
15:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
15:     x = next(iterator)
15:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
15:     data = self._next_data()
15:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
15:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
15:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
15:     return self.collate_fn(data)
15:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
15:     assert False
15: AssertionError
 9: Traceback (most recent call last):
 9:   File "train.py", line 156, in <module>
 9:     main(params)
 9:   File "train.py", line 135, in main
 9:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 9:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 9:     inputs = self.get_batch("qlm", lang_pair)
 9:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 9:     x = next(iterator)
 9:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 9:     data = self._next_data()
 9:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 9:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 9:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 9:     return self.collate_fn(data)
 9:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 9:     assert False
 9: AssertionError
srun: error: dgx-2: task 14: Exited with exit code 1
srun: error: dgx-1: task 1: Exited with exit code 1
11: INFO - 08/08/20 08:29:13 - 0:00:24 - # parallel sections pairs: 100000
11: INFO - 08/08/20 08:29:13 - 0:00:25 - Created new training data iterator (qlm,enes) ...
11: Traceback (most recent call last):
11:   File "train.py", line 156, in <module>
11:     main(params)
11:   File "train.py", line 135, in main
11:     trainer.qlm_step(lang_pair, params.lambda_qlm)
11:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
11:     inputs = self.get_batch("qlm", lang_pair)
11:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
11:     x = next(iterator)
11:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
11:     data = self._next_data()
11:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
11:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
11:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
11:     return self.collate_fn(data)
11:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
11:     assert False
11: AssertionError
12: INFO - 08/08/20 08:29:13 - 0:00:25 - # parallel sections pairs: 100000
 8: INFO - 08/08/20 08:29:13 - 0:00:25 - # parallel sections pairs: 100000
12: INFO - 08/08/20 08:29:13 - 0:00:25 - Created new training data iterator (qlm,enes) ...
 4: INFO - 08/08/20 08:29:13 - 0:00:25 - # parallel sections pairs: 100000
 8: INFO - 08/08/20 08:29:13 - 0:00:25 - Created new training data iterator (qlm,enes) ...
srun: error: dgx-1: task 6: Exited with exit code 1
 4: INFO - 08/08/20 08:29:13 - 0:00:25 - Created new training data iterator (qlm,enes) ...
srun: error: dgx-2: task 15: Exited with exit code 1
12: Traceback (most recent call last):
12:   File "train.py", line 156, in <module>
12:     main(params)
12:   File "train.py", line 135, in main
12:     trainer.qlm_step(lang_pair, params.lambda_qlm)
12:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
12:     inputs = self.get_batch("qlm", lang_pair)
12:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
12:     x = next(iterator)
12:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
12:     data = self._next_data()
12:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
12:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
12:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
12:     return self.collate_fn(data)
12:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
12:     assert False
12: AssertionError
 2: INFO - 08/08/20 08:29:14 - 0:00:24 - # parallel sections pairs: 100000
 2: INFO - 08/08/20 08:29:14 - 0:00:24 - Created new training data iterator (qlm,enfr) ...
 8: Traceback (most recent call last):
 8:   File "train.py", line 156, in <module>
 8:     main(params)
 8:   File "train.py", line 135, in main
 8:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 8:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 8:     inputs = self.get_batch("qlm", lang_pair)
 8:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 8:     x = next(iterator)
 8:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 8:     data = self._next_data()
 8:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 8:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 8:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 8:     return self.collate_fn(data)
 8:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 8:     assert False
 8: AssertionError
 4: Traceback (most recent call last):
 4:   File "train.py", line 156, in <module>
 4:     main(params)
 4:   File "train.py", line 135, in main
 4:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 4:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 4:     inputs = self.get_batch("qlm", lang_pair)
 4:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 4:     x = next(iterator)
 4:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 4:     data = self._next_data()
 4:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 4:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 4:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 4:     return self.collate_fn(data)
 4:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 4:     assert False
 4: AssertionError
 2: Traceback (most recent call last):
 2:   File "train.py", line 156, in <module>
 2:     main(params)
 2:   File "train.py", line 135, in main
 2:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 2:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 2:     inputs = self.get_batch("qlm", lang_pair)
 2:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 2:     x = next(iterator)
 2:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 2:     data = self._next_data()
 2:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 2:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 2:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 2:     return self.collate_fn(data)
 2:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 2:     assert False
 2: AssertionError
 5: INFO - 08/08/20 08:29:14 - 0:00:25 - # parallel sections pairs: 100000
srun: error: dgx-2: task 11: Exited with exit code 1
 5: INFO - 08/08/20 08:29:14 - 0:00:25 - Created new training data iterator (qlm,enfr) ...
 5: Traceback (most recent call last):
 5:   File "train.py", line 156, in <module>
 5:     main(params)
 5:   File "train.py", line 135, in main
 5:     trainer.qlm_step(lang_pair, params.lambda_qlm)
 5:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 317, in qlm_step
 5:     inputs = self.get_batch("qlm", lang_pair)
 5:   File "/mnt/home/puxuan/XLM-Trainer/trainer.py", line 301, in get_batch
 5:     x = next(iterator)
 5:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
 5:     data = self._next_data()
 5:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
 5:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 5:   File "/mnt/home/puxuan/miniconda3/envs/dgx/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
 5:     return self.collate_fn(data)
 5:   File "/mnt/home/puxuan/XLM-Trainer/src/dataset/wiki_dataset.py", line 382, in __call__
 5:     assert False
 5: AssertionError
srun: error: dgx-2: tasks 8,12: Exited with exit code 1
srun: error: dgx-1: task 4: Exited with exit code 1
srun: error: dgx-1: task 2: Exited with exit code 1
srun: error: dgx-2: task 9: Exited with exit code 1
srun: error: dgx-1: task 5: Exited with exit code 1

INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 801755.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 180.60it/s]100%|██████████| 176/176 [00:00<00:00, 1411.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17609.25it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 845319.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 756466.47it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 765048.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 172.70it/s]100%|██████████| 176/176 [00:00<00:00, 1354.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17569.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 690898.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 730206.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 839868.64it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 12%|█▏        | 21/176 [00:00<00:00, 192.22it/s]100%|██████████| 176/176 [00:00<00:00, 1497.45it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17689.00it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 778597.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 12%|█▏        | 21/176 [00:00<00:01, 114.46it/s]100%|██████████| 176/176 [00:00<00:00, 917.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17563.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 114.35it/s] 12%|█▏        | 21/176 [00:00<00:01, 114.32it/s]100%|██████████| 176/176 [00:00<00:00, 916.96it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
100%|██████████| 176/176 [00:00<00:00, 915.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17089.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 16526.32it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 198.44it/s]100%|██████████| 176/176 [00:00<00:00, 1545.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 18357.64it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 39%|███▉      | 69/176 [00:00<00:00, 689.34it/s]100%|██████████| 176/176 [00:00<00:00, 1669.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 18601.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4172.1 words/s - loss: 0.3449
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3932.3 words/s - loss: 0.3540
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 3886.9 words/s - loss: 0.3393
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3773.1 words/s - loss: 0.3573
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3808.1 words/s - loss: 0.3786
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3877.8 words/s - loss: 0.3378
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3715.8 words/s - loss: 0.3290
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 3586.3 words/s - loss: 0.3494
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4200.6 words/s - loss: 0.2096
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4183.7 words/s - loss: 0.2243
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3740.4 words/s - loss: 0.2364
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4064.8 words/s - loss: 0.2601
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3892.1 words/s - loss: 0.2608
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3895.4 words/s - loss: 0.2579
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 3658.9 words/s - loss: 0.2490
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3288.0 words/s - loss: 0.2485
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4400.9 words/s - loss: 0.2183
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4028.4 words/s - loss: 0.2225
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3812.2 words/s - loss: 0.2417
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 3984.9 words/s - loss: 0.2272
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4177.3 words/s - loss: 0.2281
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 3794.6 words/s - loss: 0.2152
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3402.8 words/s - loss: 0.2136
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4068.9 words/s - loss: 0.2065
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4350.0 words/s - loss: 0.1880
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4473.2 words/s - loss: 0.1968
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4039.9 words/s - loss: 0.1949
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4000.7 words/s - loss: 0.2040
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4072.8 words/s - loss: 0.2020
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4075.3 words/s - loss: 0.2144
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3485.0 words/s - loss: 0.1898
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3638.0 words/s - loss: 0.1879
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 4405.1 words/s - loss: 0.1815
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3865.1 words/s - loss: 0.1896
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4134.2 words/s - loss: 0.1878
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 3710.6 words/s - loss: 0.1578
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3791.1 words/s - loss: 0.1901
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4064.1 words/s - loss: 0.2000
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4286.9 words/s - loss: 0.1901
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 3437.2 words/s - loss: 0.1801
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 3782.1 words/s - loss: 0.1684
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4361.6 words/s - loss: 0.1867
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3727.3 words/s - loss: 0.1640
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3975.7 words/s - loss: 0.1907
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3892.9 words/s - loss: 0.1527
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3471.2 words/s - loss: 0.1598
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3878.0 words/s - loss: 0.1590
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4076.3 words/s - loss: 0.1761
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.313 w/ 88 queries
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.270 w/ 88 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4199.2 words/s - loss: 0.1986
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 4093.4 words/s - loss: 0.1666
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4093.1 words/s - loss: 0.1778
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3941.0 words/s - loss: 0.1719
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3758.7 words/s - loss: 0.1836
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3699.7 words/s - loss: 0.1763
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3628.3 words/s - loss: 0.1756
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3628.0 words/s - loss: 0.1461
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.319 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.274 w/ 88 queries
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_deen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4376.7 words/s - loss: 0.1464
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4047.2 words/s - loss: 0.1663
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3931.6 words/s - loss: 0.1569
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3915.0 words/s - loss: 0.1640
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 3912.1 words/s - loss: 0.1681
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3797.0 words/s - loss: 0.1657
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3716.3 words/s - loss: 0.1652
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 3704.7 words/s - loss: 0.1627
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.277 w/ 88 queries
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_deen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4430.8 words/s - loss: 0.1596
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4178.7 words/s - loss: 0.1739
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4132.0 words/s - loss: 0.1515
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3827.2 words/s - loss: 0.1404
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3725.2 words/s - loss: 0.1702
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 3710.0 words/s - loss: 0.1698
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3692.6 words/s - loss: 0.1866
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3632.9 words/s - loss: 0.1558
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.344 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4300.6 words/s - loss: 0.1848
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4262.6 words/s - loss: 0.1470
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4056.8 words/s - loss: 0.1476
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4069.3 words/s - loss: 0.1626
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4037.0 words/s - loss: 0.1509
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3991.8 words/s - loss: 0.1530
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3831.2 words/s - loss: 0.1388
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 3661.3 words/s - loss: 0.1471
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.335 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.278 w/ 88 queries
INFO:__main__:removing file tmp/mbert_deen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_9.txt
INFO:__main__:[0.31251425970593255, 0.31896311952137407, 0.3415567427166293, 0.34362670040763876, 0.33543396482953464]
INFO:__main__:[0.270406084983869, 0.27373897716391193, 0.277132019071376, 0.2777770069994486, 0.2776897189110132]
INFO:__main__:0.34362670040763876
INFO:__main__:0.2777770069994486
INFO:__main__:best MAP: 0.311
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 786805.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 813417.11it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 795732.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 797153.72it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 774514.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 722732.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 778799.76it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 785303.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 179.46it/s]100%|██████████| 176/176 [00:00<00:00, 1290.82it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 124.74it/s]100%|██████████| 176/176 [00:00<00:00, 17767.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 914.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 178.76it/s] 13%|█▎        | 23/176 [00:00<00:00, 178.11it/s] 13%|█▎        | 23/176 [00:00<00:00, 176.24it/s]100%|██████████| 176/176 [00:00<00:00, 17501.96it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1286.25it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
100%|██████████| 176/176 [00:00<00:00, 1284.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1270.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17689.43it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 17224.26it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 17426.76it/s]
INFO:__main__:f1 has 88 queries ...
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 125.83it/s]100%|██████████| 176/176 [00:00<00:00, 922.08it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17349.76it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 116.41it/s] 13%|█▎        | 23/176 [00:00<00:00, 180.46it/s]100%|██████████| 176/176 [00:00<00:00, 855.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1292.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17324.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16890.85it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4034.2 words/s - loss: 0.2972
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3977.0 words/s - loss: 0.3094
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3870.7 words/s - loss: 0.3040
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 3743.2 words/s - loss: 0.2978
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3734.4 words/s - loss: 0.2956
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3651.2 words/s - loss: 0.3210
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3709.4 words/s - loss: 0.3231
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 3747.8 words/s - loss: 0.2955
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4216.9 words/s - loss: 0.2095
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4325.6 words/s - loss: 0.2193
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4300.8 words/s - loss: 0.2212
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4357.4 words/s - loss: 0.1827
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4125.1 words/s - loss: 0.1921
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3589.5 words/s - loss: 0.2213
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3625.2 words/s - loss: 0.2140
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3612.3 words/s - loss: 0.2138
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 3997.6 words/s - loss: 0.1825
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4003.9 words/s - loss: 0.1654
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3956.0 words/s - loss: 0.1841
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4058.3 words/s - loss: 0.1563
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3823.6 words/s - loss: 0.2132
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3723.7 words/s - loss: 0.1592
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 3588.2 words/s - loss: 0.1809
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3630.4 words/s - loss: 0.1556
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4131.7 words/s - loss: 0.1752
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 3825.0 words/s - loss: 0.1617
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 3794.4 words/s - loss: 0.1767
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3600.1 words/s - loss: 0.1577
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3816.4 words/s - loss: 0.1520
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4179.6 words/s - loss: 0.1692
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3838.6 words/s - loss: 0.1686
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3995.0 words/s - loss: 0.1785
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 3596.5 words/s - loss: 0.1517
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3866.3 words/s - loss: 0.1458
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3875.2 words/s - loss: 0.1543
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 3673.0 words/s - loss: 0.1632
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4152.6 words/s - loss: 0.1717
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4138.9 words/s - loss: 0.1506
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3566.3 words/s - loss: 0.1372
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3496.1 words/s - loss: 0.1753
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3883.0 words/s - loss: 0.1577
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 3716.5 words/s - loss: 0.1704
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3871.2 words/s - loss: 0.1521
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3846.4 words/s - loss: 0.1410
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4462.0 words/s - loss: 0.1457
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3769.0 words/s - loss: 0.1749
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3988.2 words/s - loss: 0.1462
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3823.8 words/s - loss: 0.1702
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.363 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.336 w/ 88 queries
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4167.5 words/s - loss: 0.1273
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4138.4 words/s - loss: 0.1487
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3916.8 words/s - loss: 0.1599
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3794.2 words/s - loss: 0.1357
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3702.9 words/s - loss: 0.1852
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3762.0 words/s - loss: 0.1312
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3677.1 words/s - loss: 0.1593
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3602.3 words/s - loss: 0.1411
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.384 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_deen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 4373.9 words/s - loss: 0.1583
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4266.9 words/s - loss: 0.1438
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 3940.0 words/s - loss: 0.1460
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 3928.3 words/s - loss: 0.1210
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3803.6 words/s - loss: 0.1419
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3813.9 words/s - loss: 0.1325
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3811.2 words/s - loss: 0.1330
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3634.2 words/s - loss: 0.1173
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.387 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_deen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4424.3 words/s - loss: 0.1469
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4088.4 words/s - loss: 0.1332
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 4071.1 words/s - loss: 0.1467
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3969.1 words/s - loss: 0.1580
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3919.1 words/s - loss: 0.1339
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3800.5 words/s - loss: 0.1351
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3752.7 words/s - loss: 0.1648
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3498.5 words/s - loss: 0.1195
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.381 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4381.4 words/s - loss: 0.1650
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4250.8 words/s - loss: 0.1288
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4113.2 words/s - loss: 0.1305
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4123.9 words/s - loss: 0.1517
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 4092.3 words/s - loss: 0.1240
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 3771.4 words/s - loss: 0.1199
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3614.4 words/s - loss: 0.1256
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3438.2 words/s - loss: 0.1336
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.342 w/ 88 queries
INFO:__main__:removing file tmp/mbert_deen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_9.txt
INFO:__main__:[0.36282805993427086, 0.3844992056926087, 0.3865157123816427, 0.3808394476099132, 0.3586448037924461]
INFO:__main__:[0.3355340206136852, 0.35531199498855276, 0.34187021570989834, 0.3504883580947652, 0.3418792500002479]
INFO:__main__:0.3844992056926087
INFO:__main__:0.34187021570989834
INFO:__main__:best MAP: 0.363
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 731301.04it/s]
100%|██████████| 5000/5000 [00:00<00:00, 727925.03it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 777154.72it/s]
100%|██████████| 5000/5000 [00:00<00:00, 769484.11it/s]
100%|██████████| 5000/5000 [00:00<00:00, 764491.11it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 682466.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 872867.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 764937.26it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 175.91it/s] 13%|█▎        | 23/176 [00:00<00:00, 171.52it/s] 13%|█▎        | 23/176 [00:00<00:00, 165.94it/s]100%|██████████| 176/176 [00:00<00:00, 1266.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1238.22it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1199.30it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 14820.86it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 17522.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 129.16it/s]100%|██████████| 176/176 [00:00<00:00, 17407.03it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 207.03it/s]100%|██████████| 176/176 [00:00<00:00, 945.34it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 122.41it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1477.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 895.93it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16886.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 17990.78it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 17486.62it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 113.74it/s]100%|██████████| 176/176 [00:00<00:00, 836.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 17343.24it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 171.19it/s]100%|██████████| 176/176 [00:00<00:00, 1229.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16315.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4147.9 words/s - loss: 0.3014
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4191.7 words/s - loss: 0.3149
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4052.9 words/s - loss: 0.3156
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4061.3 words/s - loss: 0.3171
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3947.9 words/s - loss: 0.3111
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 3939.4 words/s - loss: 0.2912
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3764.8 words/s - loss: 0.2974
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3689.5 words/s - loss: 0.3099
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4085.7 words/s - loss: 0.2208
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4344.2 words/s - loss: 0.1892
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3835.1 words/s - loss: 0.2324
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3908.0 words/s - loss: 0.2152
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3880.4 words/s - loss: 0.1973
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 3902.9 words/s - loss: 0.1894
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3844.0 words/s - loss: 0.1956
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3677.1 words/s - loss: 0.2029
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4261.9 words/s - loss: 0.2002
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3756.0 words/s - loss: 0.1854
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4049.9 words/s - loss: 0.1735
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3920.9 words/s - loss: 0.1988
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3911.5 words/s - loss: 0.1866
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 3573.3 words/s - loss: 0.1962
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4031.1 words/s - loss: 0.1777
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3523.2 words/s - loss: 0.1595
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4234.5 words/s - loss: 0.1763
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4319.9 words/s - loss: 0.1526
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4135.1 words/s - loss: 0.1585
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4072.1 words/s - loss: 0.1738
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3914.0 words/s - loss: 0.1695
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 3756.7 words/s - loss: 0.1589
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3529.3 words/s - loss: 0.1609
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 3854.8 words/s - loss: 0.1697
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4084.9 words/s - loss: 0.1518
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4205.5 words/s - loss: 0.1491
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 3912.9 words/s - loss: 0.1533
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 4014.6 words/s - loss: 0.1340
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 3877.0 words/s - loss: 0.1681
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4012.2 words/s - loss: 0.1711
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4005.2 words/s - loss: 0.1683
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 3534.5 words/s - loss: 0.1615
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4256.6 words/s - loss: 0.1775
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3861.6 words/s - loss: 0.1304
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3863.7 words/s - loss: 0.1612
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3853.0 words/s - loss: 0.1391
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4093.9 words/s - loss: 0.1694
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 3910.0 words/s - loss: 0.1408
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3946.1 words/s - loss: 0.1357
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 3854.0 words/s - loss: 0.1521
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.393 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.350 w/ 88 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 4327.7 words/s - loss: 0.1462
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4245.9 words/s - loss: 0.1278
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4166.3 words/s - loss: 0.1387
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3981.0 words/s - loss: 0.1572
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3897.0 words/s - loss: 0.1470
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3892.9 words/s - loss: 0.1541
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3641.2 words/s - loss: 0.1154
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3582.6 words/s - loss: 0.1420
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.391 w/ 88 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_deen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4362.3 words/s - loss: 0.1539
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 4133.8 words/s - loss: 0.1497
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3976.7 words/s - loss: 0.1462
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4027.9 words/s - loss: 0.1314
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4007.3 words/s - loss: 0.1407
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3910.8 words/s - loss: 0.1378
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3686.1 words/s - loss: 0.1748
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3618.5 words/s - loss: 0.1324
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.345 w/ 88 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_deen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4213.7 words/s - loss: 0.1463
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4127.1 words/s - loss: 0.1512
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4095.3 words/s - loss: 0.1451
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 4048.1 words/s - loss: 0.1510
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3978.9 words/s - loss: 0.1290
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3729.4 words/s - loss: 0.1287
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3661.0 words/s - loss: 0.1242
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3402.4 words/s - loss: 0.1295
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.349 w/ 88 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4143.9 words/s - loss: 0.1217
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3883.0 words/s - loss: 0.1179
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 3759.5 words/s - loss: 0.1311
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3755.9 words/s - loss: 0.1112
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3704.9 words/s - loss: 0.1430
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 3649.2 words/s - loss: 0.1338
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3736.4 words/s - loss: 0.1214
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 3659.3 words/s - loss: 0.1334
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:f1 set during evaluation: 35912/35910
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.343 w/ 88 queries
INFO:__main__:removing file tmp/mbert_deen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_9.txt
INFO:__main__:[0.3927423031873569, 0.39116277718755205, 0.3858829174380649, 0.37577002813011, 0.36416524871059974]
INFO:__main__:[0.3498207433772407, 0.36107200010706236, 0.3452174309440186, 0.3489531489375499, 0.3433805784834355]
INFO:__main__:0.39116277718755205
INFO:__main__:0.3498207433772407
INFO:__main__:best MAP: 0.370

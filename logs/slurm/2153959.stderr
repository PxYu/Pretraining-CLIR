INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 702634.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 724680.19it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 594397.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 746317.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 694927.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 721092.05it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 684627.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 691194.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 657538.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 646909.74it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 176.44it/s] 15%|█▍        | 23/156 [00:00<00:00, 190.72it/s]100%|██████████| 156/156 [00:00<00:00, 1131.35it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1219.52it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18044.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18151.62it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 189.06it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1210.04it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17900.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 185.07it/s]100%|██████████| 156/156 [00:00<00:00, 1182.73it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18567.29it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 180.00it/s] 15%|█▍        | 23/156 [00:00<00:00, 149.93it/s] 15%|█▍        | 23/156 [00:00<00:00, 135.39it/s]100%|██████████| 156/156 [00:00<00:00, 1154.45it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 969.14it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 881.45it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18390.38it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18055.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18721.36it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 179.33it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1147.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18272.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 194.06it/s]100%|██████████| 156/156 [00:00<00:00, 1237.87it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17671.67it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 174.75it/s]100%|██████████| 156/156 [00:00<00:00, 1119.05it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16902.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 4804.6 words/s - loss: 0.4587
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4426.6 words/s - loss: 0.4833
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4189.0 words/s - loss: 0.4735
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 4004.8 words/s - loss: 0.4903
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4088.9 words/s - loss: 0.4801
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3917.3 words/s - loss: 0.4676
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3916.9 words/s - loss: 0.4494
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3508.4 words/s - loss: 0.4838
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3480.3 words/s - loss: 0.4810
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2879.3 words/s - loss: 0.4793
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4256.9 words/s - loss: 0.3186
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4626.1 words/s - loss: 0.3336
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3901.8 words/s - loss: 0.3282
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3902.0 words/s - loss: 0.2848
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4103.6 words/s - loss: 0.3396
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3720.3 words/s - loss: 0.2658
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3461.1 words/s - loss: 0.3211
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4024.5 words/s - loss: 0.3406
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3743.1 words/s - loss: 0.3167
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 4261.1 words/s - loss: 0.3072
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 4107.6 words/s - loss: 0.2610
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4462.3 words/s - loss: 0.2797
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 4280.0 words/s - loss: 0.2538
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3549.6 words/s - loss: 0.2886
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3595.0 words/s - loss: 0.2836
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3618.4 words/s - loss: 0.2897
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4367.1 words/s - loss: 0.2667
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3693.1 words/s - loss: 0.2484
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 4579.5 words/s - loss: 0.2622
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3682.4 words/s - loss: 0.2736
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3755.5 words/s - loss: 0.2480
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3959.1 words/s - loss: 0.2720
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4459.3 words/s - loss: 0.2430
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3768.6 words/s - loss: 0.2412
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3448.0 words/s - loss: 0.2230
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4090.6 words/s - loss: 0.2702
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4297.8 words/s - loss: 0.2227
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3718.0 words/s - loss: 0.2546
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3300.5 words/s - loss: 0.2645
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3507.8 words/s - loss: 0.2450
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4018.2 words/s - loss: 0.2385
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 4354.0 words/s - loss: 0.2421
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4132.0 words/s - loss: 0.2306
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3597.7 words/s - loss: 0.2394
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3509.8 words/s - loss: 0.2462
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 4210.5 words/s - loss: 0.2129
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3476.4 words/s - loss: 0.2154
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 4074.1 words/s - loss: 0.2427
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3730.1 words/s - loss: 0.2008
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3584.7 words/s - loss: 0.1951
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 4029.0 words/s - loss: 0.1992
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 4339.5 words/s - loss: 0.2092
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4547.3 words/s - loss: 0.2229
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3412.3 words/s - loss: 0.2270
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3674.3 words/s - loss: 0.2049
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 4012.6 words/s - loss: 0.2219
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3892.5 words/s - loss: 0.1711
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3661.8 words/s - loss: 0.2193
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3945.7 words/s - loss: 0.2195
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3550.5 words/s - loss: 0.2038
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.378 w/ 78 queries
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.453 w/ 78 queries
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 4674.4 words/s - loss: 0.2145
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4636.0 words/s - loss: 0.2059
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 4292.8 words/s - loss: 0.1975
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4060.1 words/s - loss: 0.2067
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4051.3 words/s - loss: 0.1736
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3906.3 words/s - loss: 0.1961
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3685.9 words/s - loss: 0.1760
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3788.2 words/s - loss: 0.1948
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3743.9 words/s - loss: 0.2142
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3379.7 words/s - loss: 0.2314
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 5033.4 words/s - loss: 0.1956
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4414.2 words/s - loss: 0.1770
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 4395.0 words/s - loss: 0.2078
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 4221.4 words/s - loss: 0.1822
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 4104.0 words/s - loss: 0.2182
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4111.9 words/s - loss: 0.2059
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3881.8 words/s - loss: 0.1834
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3375.7 words/s - loss: 0.1879
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3595.8 words/s - loss: 0.1988
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3575.6 words/s - loss: 0.1671
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4237.7 words/s - loss: 0.1698
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 4207.6 words/s - loss: 0.1951
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4186.1 words/s - loss: 0.1786
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3885.4 words/s - loss: 0.1762
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3792.0 words/s - loss: 0.1894
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3830.6 words/s - loss: 0.2054
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3744.3 words/s - loss: 0.1920
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3647.3 words/s - loss: 0.1818
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3639.8 words/s - loss: 0.1663
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3549.4 words/s - loss: 0.1850
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.410 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4642.8 words/s - loss: 0.2045
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4499.2 words/s - loss: 0.1625
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4352.5 words/s - loss: 0.2084
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3960.1 words/s - loss: 0.1747
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4005.7 words/s - loss: 0.1783
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3849.4 words/s - loss: 0.1621
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3854.2 words/s - loss: 0.1585
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3853.4 words/s - loss: 0.1937
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3653.7 words/s - loss: 0.1815
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3473.1 words/s - loss: 0.1550
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.416 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.485 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:0.40960127123923146
INFO:__main__:0.4853851952907985
INFO:__main__:best MAP: 0.447
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 688674.64it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 695849.76it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 583953.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 681446.63it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 693571.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 669909.60it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 720745.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 719311.27it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 709336.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 693938.65it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 198.48it/s]100%|██████████| 156/156 [00:00<00:00, 1092.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18103.41it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 188.15it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1037.35it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17635.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 210.76it/s]100%|██████████| 156/156 [00:00<00:00, 1157.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18406.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 175.75it/s]100%|██████████| 156/156 [00:00<00:00, 969.15it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16409.89it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 208.05it/s]100%|██████████| 156/156 [00:00<00:00, 1120.76it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17866.62it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 153.93it/s]100%|██████████| 156/156 [00:00<00:00, 855.00it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17684.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 195.71it/s]100%|██████████| 156/156 [00:00<00:00, 1074.55it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17156.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 181.22it/s]100%|██████████| 156/156 [00:00<00:00, 999.90it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17771.99it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 165.61it/s]100%|██████████| 156/156 [00:00<00:00, 918.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17742.60it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 183.01it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1010.15it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17509.47it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4726.6 words/s - loss: 0.3161
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4496.7 words/s - loss: 0.3245
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 4283.2 words/s - loss: 0.3364
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4270.1 words/s - loss: 0.3132
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3965.8 words/s - loss: 0.3208
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3780.9 words/s - loss: 0.2964
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3852.4 words/s - loss: 0.3172
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3763.9 words/s - loss: 0.2942
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3699.8 words/s - loss: 0.3581
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3220.3 words/s - loss: 0.3677
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4159.9 words/s - loss: 0.2106
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4688.0 words/s - loss: 0.2294
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4284.1 words/s - loss: 0.2155
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3733.4 words/s - loss: 0.2049
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3686.1 words/s - loss: 0.2315
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3437.0 words/s - loss: 0.2351
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4048.2 words/s - loss: 0.2174
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3777.6 words/s - loss: 0.2619
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3957.2 words/s - loss: 0.1950
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3021.2 words/s - loss: 0.2305
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4350.0 words/s - loss: 0.1928
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 4513.9 words/s - loss: 0.1804
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 4610.6 words/s - loss: 0.2015
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 4349.4 words/s - loss: 0.2188
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3872.5 words/s - loss: 0.2289
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3825.2 words/s - loss: 0.2029
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3488.9 words/s - loss: 0.2098
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3542.8 words/s - loss: 0.2277
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3614.3 words/s - loss: 0.2218
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3812.0 words/s - loss: 0.1757
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4017.1 words/s - loss: 0.1660
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 4313.3 words/s - loss: 0.1733
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 4337.4 words/s - loss: 0.1889
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4134.5 words/s - loss: 0.2235
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3727.5 words/s - loss: 0.1847
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3874.1 words/s - loss: 0.1908
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3499.2 words/s - loss: 0.2195
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3745.5 words/s - loss: 0.2027
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3747.8 words/s - loss: 0.2061
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4224.8 words/s - loss: 0.1866
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3706.5 words/s - loss: 0.1889
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3938.1 words/s - loss: 0.1804
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3914.5 words/s - loss: 0.1845
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4325.9 words/s - loss: 0.1939
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 4498.2 words/s - loss: 0.1894
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4335.0 words/s - loss: 0.1769
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3712.1 words/s - loss: 0.1967
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3525.2 words/s - loss: 0.1665
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3756.9 words/s - loss: 0.1912
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3448.3 words/s - loss: 0.1621
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 4503.9 words/s - loss: 0.2026
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3970.4 words/s - loss: 0.1808
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 4188.4 words/s - loss: 0.1944
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3928.5 words/s - loss: 0.1937
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 4216.5 words/s - loss: 0.1958
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3646.8 words/s - loss: 0.1776
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3931.3 words/s - loss: 0.1478
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4311.2 words/s - loss: 0.2107
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3873.6 words/s - loss: 0.1711
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3277.6 words/s - loss: 0.1803
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4386.2 words/s - loss: 0.1604
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 4113.1 words/s - loss: 0.1583
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4086.4 words/s - loss: 0.1804
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3826.4 words/s - loss: 0.1646
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3886.2 words/s - loss: 0.1805
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3852.6 words/s - loss: 0.1691
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3670.5 words/s - loss: 0.1659
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3700.0 words/s - loss: 0.1814
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3576.6 words/s - loss: 0.1437
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3470.1 words/s - loss: 0.1568
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4567.6 words/s - loss: 0.1749
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4399.8 words/s - loss: 0.1338
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4130.0 words/s - loss: 0.1573
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4004.4 words/s - loss: 0.1792
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3833.9 words/s - loss: 0.1771
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3765.1 words/s - loss: 0.1787
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3793.9 words/s - loss: 0.1787
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3616.7 words/s - loss: 0.1524
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3454.9 words/s - loss: 0.1700
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3316.9 words/s - loss: 0.1551
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.487 w/ 78 queries
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 4771.1 words/s - loss: 0.1505
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4596.9 words/s - loss: 0.1843
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4549.3 words/s - loss: 0.1614
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 4381.5 words/s - loss: 0.1634
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3982.9 words/s - loss: 0.1752
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3929.8 words/s - loss: 0.1482
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3846.0 words/s - loss: 0.1430
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3646.1 words/s - loss: 0.1732
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3611.6 words/s - loss: 0.1751
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3562.0 words/s - loss: 0.1590
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4889.8 words/s - loss: 0.1499
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 4251.5 words/s - loss: 0.1629
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4157.7 words/s - loss: 0.1250
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 4041.3 words/s - loss: 0.1665
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3975.7 words/s - loss: 0.1567
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3943.3 words/s - loss: 0.1650
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3943.8 words/s - loss: 0.1749
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3599.4 words/s - loss: 0.1574
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3595.6 words/s - loss: 0.1658
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3502.0 words/s - loss: 0.1954
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.478 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.545 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:0.48393866438873356
INFO:__main__:0.5515800464040088
INFO:__main__:best MAP: 0.518
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 759150.05it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 221.48it/s]100%|██████████| 156/156 [00:00<00:00, 1213.78it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18286.56it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 672681.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 625194.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 659917.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 712686.74it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 714167.21it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 743038.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 711019.49it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 683222.67it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 696334.96it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 151.89it/s]100%|██████████| 156/156 [00:00<00:00, 844.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17684.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 200.34it/s]100%|██████████| 156/156 [00:00<00:00, 1095.98it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17483.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 162.34it/s]100%|██████████| 156/156 [00:00<00:00, 899.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18118.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 175.23it/s]100%|██████████| 156/156 [00:00<00:00, 967.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16263.86it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 192.36it/s]100%|██████████| 156/156 [00:00<00:00, 1056.78it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16966.46it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 175.35it/s]100%|██████████| 156/156 [00:00<00:00, 969.37it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 180.15it/s] 17%|█▋        | 27/156 [00:00<00:00, 158.95it/s]100%|██████████| 156/156 [00:00<00:00, 995.33it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 18030.08it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 882.73it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 151.08it/s]100%|██████████| 156/156 [00:00<00:00, 18085.89it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 841.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18213.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18047.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4650.6 words/s - loss: 0.3437
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3948.0 words/s - loss: 0.3133
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3986.6 words/s - loss: 0.3174
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3849.6 words/s - loss: 0.3265
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3957.1 words/s - loss: 0.3349
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3987.0 words/s - loss: 0.3121
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3714.2 words/s - loss: 0.3238
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3717.1 words/s - loss: 0.2996
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3502.2 words/s - loss: 0.3520
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3457.7 words/s - loss: 0.3377
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 4216.7 words/s - loss: 0.2502
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4969.0 words/s - loss: 0.2512
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4857.4 words/s - loss: 0.2261
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4504.9 words/s - loss: 0.2008
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3992.3 words/s - loss: 0.2129
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3814.8 words/s - loss: 0.2128
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3471.0 words/s - loss: 0.2241
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3755.4 words/s - loss: 0.2134
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4019.5 words/s - loss: 0.2449
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3541.5 words/s - loss: 0.2181
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3900.7 words/s - loss: 0.2235
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 4085.8 words/s - loss: 0.1809
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3435.4 words/s - loss: 0.2226
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 4076.4 words/s - loss: 0.2239
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3754.7 words/s - loss: 0.1816
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4444.9 words/s - loss: 0.1870
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4209.4 words/s - loss: 0.2069
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 4461.7 words/s - loss: 0.2086
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3638.2 words/s - loss: 0.2310
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3517.6 words/s - loss: 0.2040
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 4139.8 words/s - loss: 0.2296
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4443.4 words/s - loss: 0.2196
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3683.0 words/s - loss: 0.2024
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4108.6 words/s - loss: 0.1657
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4348.6 words/s - loss: 0.1764
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4309.9 words/s - loss: 0.2214
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3999.6 words/s - loss: 0.1793
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 4123.8 words/s - loss: 0.2044
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3890.4 words/s - loss: 0.1922
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4081.6 words/s - loss: 0.1942
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4230.3 words/s - loss: 0.1878
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3717.5 words/s - loss: 0.1734
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4067.4 words/s - loss: 0.1664
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3638.9 words/s - loss: 0.1756
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3784.3 words/s - loss: 0.1833
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3851.7 words/s - loss: 0.1972
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3448.2 words/s - loss: 0.2103
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4372.3 words/s - loss: 0.1699
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3973.3 words/s - loss: 0.1648
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3655.3 words/s - loss: 0.1831
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 5148.3 words/s - loss: 0.1677
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3635.0 words/s - loss: 0.1625
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3846.6 words/s - loss: 0.1483
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4230.4 words/s - loss: 0.1969
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 4159.5 words/s - loss: 0.1875
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3314.2 words/s - loss: 0.1725
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 4440.8 words/s - loss: 0.1717
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3445.3 words/s - loss: 0.1781
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3042.9 words/s - loss: 0.1549
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3690.0 words/s - loss: 0.1965
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4353.5 words/s - loss: 0.1565
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4059.2 words/s - loss: 0.1834
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 4025.0 words/s - loss: 0.1522
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 4015.1 words/s - loss: 0.1696
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3893.8 words/s - loss: 0.1524
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3866.2 words/s - loss: 0.1670
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4037.4 words/s - loss: 0.1539
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3537.4 words/s - loss: 0.1766
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3559.0 words/s - loss: 0.1556
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3108.3 words/s - loss: 0.1535
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.492 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 5042.4 words/s - loss: 0.1587
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 4657.3 words/s - loss: 0.1678
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4454.9 words/s - loss: 0.1802
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4190.9 words/s - loss: 0.1614
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4116.7 words/s - loss: 0.1738
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4056.7 words/s - loss: 0.1550
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3949.3 words/s - loss: 0.1471
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3900.3 words/s - loss: 0.1644
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3608.4 words/s - loss: 0.1648
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3131.5 words/s - loss: 0.1447
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.486 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.561 w/ 78 queries
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4862.5 words/s - loss: 0.1667
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4441.1 words/s - loss: 0.1623
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 4206.4 words/s - loss: 0.1611
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 4256.1 words/s - loss: 0.1572
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 4044.9 words/s - loss: 0.1509
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3958.6 words/s - loss: 0.1386
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3536.3 words/s - loss: 0.1648
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3656.6 words/s - loss: 0.1395
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3476.3 words/s - loss: 0.1719
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3434.8 words/s - loss: 0.1410
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.570 w/ 78 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4146.4 words/s - loss: 0.1668
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4102.4 words/s - loss: 0.1481
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3888.0 words/s - loss: 0.1781
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3905.7 words/s - loss: 0.1567
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3923.4 words/s - loss: 0.1520
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3786.6 words/s - loss: 0.1415
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3816.6 words/s - loss: 0.1024
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3786.2 words/s - loss: 0.1507
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3607.1 words/s - loss: 0.1438
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3383.6 words/s - loss: 0.1564
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
Traceback (most recent call last):
  File "finetune-search.py", line 580, in <module>
    clir.run()
  File "finetune-search.py", line 377, in run
    logger.info(f1_maps)
NameError: name 'f1_maps' is not defined
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', '--dataset', 'mix', '--source_lang', 'en', '--target_lang', 'es', '--batch_size', '16', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '4', '--seed', '611']' returned non-zero exit status 1.

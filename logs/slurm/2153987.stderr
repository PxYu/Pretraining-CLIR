INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 809242.52it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 169.62it/s]100%|██████████| 185/185 [00:00<00:00, 1201.03it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22687.16it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 724154.70it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 727596.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 694398.20it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 708281.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 689807.25it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 706754.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 637897.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 714313.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 693961.61it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 14%|█▎        | 25/185 [00:00<00:00, 173.67it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1228.91it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22902.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 202.74it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 204.24it/s]100%|██████████| 185/185 [00:00<00:00, 1424.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1435.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22771.72it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
100%|██████████| 185/185 [00:00<00:00, 22551.33it/s]
INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 178.22it/s]100%|██████████| 185/185 [00:00<00:00, 1260.03it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23116.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 169.55it/s]100%|██████████| 185/185 [00:00<00:00, 1199.32it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22816.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 193.58it/s] 14%|█▎        | 25/185 [00:00<00:00, 209.31it/s]100%|██████████| 185/185 [00:00<00:00, 1361.86it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1468.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22333.24it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22646.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 183.59it/s]100%|██████████| 185/185 [00:00<00:00, 1292.97it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21910.72it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 182.09it/s]100%|██████████| 185/185 [00:00<00:00, 1282.34it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21562.45it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2930.2 words/s - loss: 0.5738
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2855.2 words/s - loss: 0.5687
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2820.2 words/s - loss: 0.5671
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2602.1 words/s - loss: 0.5572
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2514.9 words/s - loss: 0.5750
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2500.9 words/s - loss: 0.5611
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2471.7 words/s - loss: 0.5647
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2388.7 words/s - loss: 0.5770
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2421.6 words/s - loss: 0.5655
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2430.2 words/s - loss: 0.5713
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3146.7 words/s - loss: 0.3989
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2519.0 words/s - loss: 0.3844
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2899.2 words/s - loss: 0.3777
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2840.9 words/s - loss: 0.3462
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2497.5 words/s - loss: 0.3447
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2734.2 words/s - loss: 0.3649
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2550.2 words/s - loss: 0.3679
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2619.9 words/s - loss: 0.3962
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2410.0 words/s - loss: 0.3812
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2122.3 words/s - loss: 0.3983
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2474.8 words/s - loss: 0.3351
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2882.2 words/s - loss: 0.3208
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2699.5 words/s - loss: 0.3118
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2730.5 words/s - loss: 0.3348
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2779.4 words/s - loss: 0.3225
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2461.9 words/s - loss: 0.2862
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2722.8 words/s - loss: 0.3271
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2746.8 words/s - loss: 0.3249
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2346.8 words/s - loss: 0.3455
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2588.4 words/s - loss: 0.3327
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2527.4 words/s - loss: 0.2913
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2671.1 words/s - loss: 0.2753
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2864.2 words/s - loss: 0.3144
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2202.1 words/s - loss: 0.3123
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2658.6 words/s - loss: 0.2792
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2922.9 words/s - loss: 0.2850
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2375.0 words/s - loss: 0.3011
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2305.8 words/s - loss: 0.2935
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2560.3 words/s - loss: 0.3196
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2592.8 words/s - loss: 0.2814
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3087.7 words/s - loss: 0.2931
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2614.1 words/s - loss: 0.2531
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2496.1 words/s - loss: 0.2559
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2463.0 words/s - loss: 0.2891
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2427.7 words/s - loss: 0.2607
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2585.0 words/s - loss: 0.2576
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2828.4 words/s - loss: 0.2609
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2416.5 words/s - loss: 0.2700
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3075.4 words/s - loss: 0.2915
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2545.5 words/s - loss: 0.2625
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3261.3 words/s - loss: 0.2456
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2672.4 words/s - loss: 0.2498
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2739.4 words/s - loss: 0.2870
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2341.3 words/s - loss: 0.2551
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2632.2 words/s - loss: 0.2438
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2544.9 words/s - loss: 0.2494
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2324.8 words/s - loss: 0.2476
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2659.8 words/s - loss: 0.2917
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2358.2 words/s - loss: 0.2564
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2479.1 words/s - loss: 0.2428
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.276 w/ 93 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.271 w/ 92 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_defr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2897.4 words/s - loss: 0.2477
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2730.1 words/s - loss: 0.2672
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2789.9 words/s - loss: 0.2374
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 2830.3 words/s - loss: 0.2438
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2577.0 words/s - loss: 0.2911
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2535.9 words/s - loss: 0.2570
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2480.2 words/s - loss: 0.2387
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2364.8 words/s - loss: 0.2587
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2250.9 words/s - loss: 0.2720
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2219.8 words/s - loss: 0.2103
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.268 w/ 92 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3178.3 words/s - loss: 0.2332
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3038.5 words/s - loss: 0.2302
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2961.9 words/s - loss: 0.2725
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2771.4 words/s - loss: 0.2620
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2695.1 words/s - loss: 0.2618
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2599.1 words/s - loss: 0.2407
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2509.0 words/s - loss: 0.2590
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2523.2 words/s - loss: 0.2507
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2135.7 words/s - loss: 0.2037
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2096.9 words/s - loss: 0.2230
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.313 w/ 93 queries
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.294 w/ 92 queries
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2903.1 words/s - loss: 0.2528
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2917.2 words/s - loss: 0.2582
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2837.4 words/s - loss: 0.2238
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2921.2 words/s - loss: 0.2253
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2851.9 words/s - loss: 0.2292
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2657.8 words/s - loss: 0.2200
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2565.9 words/s - loss: 0.2512
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2558.3 words/s - loss: 0.2369
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2589.6 words/s - loss: 0.2482
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2241.1 words/s - loss: 0.2397
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.316 w/ 93 queries
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_defr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2903.2 words/s - loss: 0.2435
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2908.9 words/s - loss: 0.2559
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2947.5 words/s - loss: 0.2239
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2713.3 words/s - loss: 0.2168
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2616.8 words/s - loss: 0.2382
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2602.4 words/s - loss: 0.2702
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2516.6 words/s - loss: 0.2448
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2519.8 words/s - loss: 0.2242
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2413.8 words/s - loss: 0.2242
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2260.1 words/s - loss: 0.2176
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.327 w/ 93 queries
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.292 w/ 92 queries
INFO:__main__:removing file tmp/mbert_defr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_9.txt
INFO:__main__:[0.27568275282631105, 0.29003918618990693, 0.3127290472772196, 0.3161657458936386, 0.32724711233684745]
INFO:__main__:[0.27082178237374455, 0.26845762576118354, 0.2944832513838404, 0.2776560353717592, 0.2919123968800949]
INFO:__main__:0.3127290472772196
INFO:__main__:0.2919123968800949
INFO:__main__:best MAP: 0.302

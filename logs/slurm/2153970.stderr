INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 769484.11it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 762378.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 706801.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 675715.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 741069.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 785361.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 415994.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 705067.24it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 635596.91it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 487687.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 217.61it/s] 14%|█▎        | 25/185 [00:00<00:00, 210.77it/s]100%|██████████| 185/185 [00:00<00:00, 1522.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1477.38it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 197.89it/s] 14%|█▎        | 25/185 [00:00<00:00, 196.45it/s]100%|██████████| 185/185 [00:00<00:00, 22916.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
 14%|█▎        | 25/185 [00:00<00:00, 193.12it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 187.95it/s]100%|██████████| 185/185 [00:00<00:00, 1390.65it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22976.70it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1375.63it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1356.33it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1325.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21785.84it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 20468.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 21287.38it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22899.40it/s]
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:01, 159.87it/s] 14%|█▎        | 25/185 [00:00<00:01, 155.02it/s]100%|██████████| 185/185 [00:00<00:00, 1132.83it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 205.81it/s]100%|██████████| 185/185 [00:00<00:00, 1099.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1445.09it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21448.01it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 21327.75it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 22807.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 163.38it/s]100%|██████████| 185/185 [00:00<00:00, 1159.19it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23031.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2850.7 words/s - loss: 0.5441
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2504.8 words/s - loss: 0.5241
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2512.2 words/s - loss: 0.5333
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2513.6 words/s - loss: 0.5340
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2546.6 words/s - loss: 0.5418
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2518.9 words/s - loss: 0.5379
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2413.3 words/s - loss: 0.5185
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2416.9 words/s - loss: 0.5297
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2402.7 words/s - loss: 0.5184
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2078.4 words/s - loss: 0.5401
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2863.8 words/s - loss: 0.3869
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2765.0 words/s - loss: 0.3901
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2817.5 words/s - loss: 0.3405
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2670.7 words/s - loss: 0.3632
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2625.8 words/s - loss: 0.4010
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2508.3 words/s - loss: 0.3542
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2465.1 words/s - loss: 0.3519
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2459.5 words/s - loss: 0.3461
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2245.5 words/s - loss: 0.3772
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2713.9 words/s - loss: 0.3727
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3087.1 words/s - loss: 0.3597
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2751.4 words/s - loss: 0.2950
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2538.8 words/s - loss: 0.3028
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3098.2 words/s - loss: 0.3149
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2583.1 words/s - loss: 0.3031
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2606.2 words/s - loss: 0.3319
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2837.5 words/s - loss: 0.3178
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2453.9 words/s - loss: 0.3390
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2754.8 words/s - loss: 0.2962
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2699.8 words/s - loss: 0.3539
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2748.2 words/s - loss: 0.2712
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3007.2 words/s - loss: 0.2755
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2523.4 words/s - loss: 0.3021
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3042.3 words/s - loss: 0.2882
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2581.6 words/s - loss: 0.2877
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2366.2 words/s - loss: 0.2918
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3018.8 words/s - loss: 0.2912
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2590.2 words/s - loss: 0.3094
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2583.0 words/s - loss: 0.2815
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2555.8 words/s - loss: 0.2773
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2637.9 words/s - loss: 0.2561
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2795.6 words/s - loss: 0.2972
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2740.4 words/s - loss: 0.2616
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2548.1 words/s - loss: 0.2762
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2472.9 words/s - loss: 0.2400
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2408.6 words/s - loss: 0.2979
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2297.4 words/s - loss: 0.2870
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2670.2 words/s - loss: 0.2832
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2675.6 words/s - loss: 0.2865
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2299.1 words/s - loss: 0.2845
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3014.4 words/s - loss: 0.2661
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2459.0 words/s - loss: 0.2599
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2607.2 words/s - loss: 0.2564
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2719.7 words/s - loss: 0.2382
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2729.8 words/s - loss: 0.2606
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2835.4 words/s - loss: 0.2373
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2760.2 words/s - loss: 0.2440
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2481.6 words/s - loss: 0.2525
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2386.2 words/s - loss: 0.2790
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2212.5 words/s - loss: 0.2586
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.287 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.259 w/ 92 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_defr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3139.4 words/s - loss: 0.2806
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3080.3 words/s - loss: 0.2307
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2802.6 words/s - loss: 0.2738
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2737.6 words/s - loss: 0.2703
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2808.4 words/s - loss: 0.2605
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2546.9 words/s - loss: 0.2564
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2401.0 words/s - loss: 0.2277
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2354.0 words/s - loss: 0.2558
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2345.6 words/s - loss: 0.2217
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2252.7 words/s - loss: 0.2636
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.309 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2972.6 words/s - loss: 0.2424
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2963.5 words/s - loss: 0.2587
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2899.1 words/s - loss: 0.2522
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2664.3 words/s - loss: 0.2385
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2658.1 words/s - loss: 0.2325
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2624.0 words/s - loss: 0.2330
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2381.1 words/s - loss: 0.2693
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2254.4 words/s - loss: 0.2579
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2346.8 words/s - loss: 0.2360
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2274.4 words/s - loss: 0.1977
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.290 w/ 93 queries
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.269 w/ 92 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3595.0 words/s - loss: 0.2176
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3318.2 words/s - loss: 0.1968
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3033.8 words/s - loss: 0.2396
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2633.6 words/s - loss: 0.2705
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2790.4 words/s - loss: 0.2116
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2675.0 words/s - loss: 0.2389
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2395.4 words/s - loss: 0.2450
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2309.0 words/s - loss: 0.2515
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2223.6 words/s - loss: 0.2711
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2218.3 words/s - loss: 0.2543
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.314 w/ 93 queries
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.278 w/ 92 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_defr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3174.2 words/s - loss: 0.2366
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3031.6 words/s - loss: 0.2306
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2893.5 words/s - loss: 0.2436
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2710.7 words/s - loss: 0.2232
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2699.9 words/s - loss: 0.2375
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2652.3 words/s - loss: 0.2308
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2491.6 words/s - loss: 0.2204
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2574.2 words/s - loss: 0.2106
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2385.2 words/s - loss: 0.2265
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2360.3 words/s - loss: 0.2315
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.271 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.261 w/ 92 queries
INFO:__main__:removing file tmp/mbert_defr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_9.txt
Traceback (most recent call last):
  File "finetune-search.py", line 580, in <module>
    clir.run()
  File "finetune-search.py", line 377, in run
    logger.info(self.f1_maps)
NameError: name 'f1_maps' is not defined
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', 'bert-base-multilingual-uncased', '--dataset', 'mix', '--source_lang', 'de', '--target_lang', 'fr', '--batch_size', '16', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '4', '--seed', '611']' returned non-zero exit status 1.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 741645.86it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 703175.97it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 720943.31it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 707708.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 690306.78it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 708952.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 739527.47it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 674173.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 686712.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 578796.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 230.44it/s]100%|██████████| 185/185 [00:00<00:00, 1107.83it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23732.15it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 211.92it/s]100%|██████████| 185/185 [00:00<00:00, 1022.25it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 220.88it/s]100%|██████████| 185/185 [00:00<00:00, 23527.07it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1063.86it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 226.42it/s]100%|██████████| 185/185 [00:00<00:00, 1091.44it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
100%|██████████| 185/185 [00:00<00:00, 24169.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 216.62it/s] 20%|██        | 37/185 [00:00<00:00, 229.63it/s] 20%|██        | 37/185 [00:00<00:00, 228.58it/s] 20%|██        | 37/185 [00:00<00:00, 277.84it/s] 20%|██        | 37/185 [00:00<00:00, 197.51it/s]100%|██████████| 185/185 [00:00<00:00, 24504.86it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 1044.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1105.13it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1100.46it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
100%|██████████| 185/185 [00:00<00:00, 956.31it/s]
100%|██████████| 185/185 [00:00<00:00, 1325.36it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 23412.78it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 23728.52it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 23729.24it/s]
100%|██████████| 185/185 [00:00<00:00, 24099.95it/s]INFO:root:Number of labelled query-document pairs in [f2] set: 27153

INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 23436.82it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 273.63it/s]100%|██████████| 185/185 [00:00<00:00, 1306.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23318.50it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3206.9 words/s - loss: 0.4246
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3103.0 words/s - loss: 0.4107
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3085.1 words/s - loss: 0.4202
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2738.9 words/s - loss: 0.4319
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2688.5 words/s - loss: 0.4218
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2763.1 words/s - loss: 0.3838
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2598.9 words/s - loss: 0.3721
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2628.5 words/s - loss: 0.4066
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2587.6 words/s - loss: 0.4021
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2342.1 words/s - loss: 0.4371
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2656.2 words/s - loss: 0.2882
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3079.4 words/s - loss: 0.2924
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2661.8 words/s - loss: 0.3035
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2989.3 words/s - loss: 0.2774
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2637.9 words/s - loss: 0.3279
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2274.0 words/s - loss: 0.3319
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2743.9 words/s - loss: 0.2853
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2911.0 words/s - loss: 0.2881
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2648.7 words/s - loss: 0.2672
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2529.5 words/s - loss: 0.2887
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2791.4 words/s - loss: 0.2624
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2608.8 words/s - loss: 0.2540
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2395.2 words/s - loss: 0.2805
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2569.6 words/s - loss: 0.2351
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2637.2 words/s - loss: 0.2485
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2642.3 words/s - loss: 0.3037
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2593.6 words/s - loss: 0.2465
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2507.0 words/s - loss: 0.2568
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2375.1 words/s - loss: 0.2411
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2373.3 words/s - loss: 0.2572
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2749.2 words/s - loss: 0.2521
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2634.6 words/s - loss: 0.2479
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3003.1 words/s - loss: 0.2468
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2442.6 words/s - loss: 0.2459
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2320.5 words/s - loss: 0.2509
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2976.0 words/s - loss: 0.2602
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2630.4 words/s - loss: 0.2161
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3012.4 words/s - loss: 0.2453
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2198.2 words/s - loss: 0.2618
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2500.0 words/s - loss: 0.2506
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2526.0 words/s - loss: 0.2151
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2505.2 words/s - loss: 0.2468
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2779.4 words/s - loss: 0.2017
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2924.8 words/s - loss: 0.2628
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2296.9 words/s - loss: 0.2476
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2660.2 words/s - loss: 0.2282
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2602.0 words/s - loss: 0.2375
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2479.2 words/s - loss: 0.2144
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2505.8 words/s - loss: 0.2348
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2112.2 words/s - loss: 0.2558
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2666.8 words/s - loss: 0.2343
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2724.2 words/s - loss: 0.2302
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3135.6 words/s - loss: 0.2224
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2636.3 words/s - loss: 0.2623
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2580.4 words/s - loss: 0.1945
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2811.7 words/s - loss: 0.2432
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2301.0 words/s - loss: 0.2332
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2555.7 words/s - loss: 0.2155
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2439.7 words/s - loss: 0.2382
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2818.2 words/s - loss: 0.2200
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.421 w/ 93 queries
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_defr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3209.6 words/s - loss: 0.2534
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3010.9 words/s - loss: 0.2048
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3025.5 words/s - loss: 0.2334
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2987.4 words/s - loss: 0.2240
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 2915.6 words/s - loss: 0.2003
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2753.3 words/s - loss: 0.2123
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2690.3 words/s - loss: 0.2430
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2459.4 words/s - loss: 0.2201
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2100.9 words/s - loss: 0.2346
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2090.0 words/s - loss: 0.1979
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.383 w/ 92 queries
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_6.txt
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3023.8 words/s - loss: 0.2209
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3004.8 words/s - loss: 0.2232
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2865.5 words/s - loss: 0.2234
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2812.3 words/s - loss: 0.2445
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2689.1 words/s - loss: 0.2319
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2681.6 words/s - loss: 0.2093
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2626.7 words/s - loss: 0.1983
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2637.8 words/s - loss: 0.2227
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2354.3 words/s - loss: 0.2072
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2286.8 words/s - loss: 0.1826
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.412 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3054.1 words/s - loss: 0.1891
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2858.8 words/s - loss: 0.2155
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2623.0 words/s - loss: 0.2195
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2480.8 words/s - loss: 0.2167
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2429.6 words/s - loss: 0.2434
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2391.3 words/s - loss: 0.2242
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2321.9 words/s - loss: 0.2213
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2335.4 words/s - loss: 0.2148
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2253.5 words/s - loss: 0.2226
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2266.9 words/s - loss: 0.1952
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.394 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.364 w/ 92 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_defr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3300.8 words/s - loss: 0.2077
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3044.7 words/s - loss: 0.2099
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2783.4 words/s - loss: 0.2247
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2597.8 words/s - loss: 0.1904
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2606.3 words/s - loss: 0.1893
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2549.6 words/s - loss: 0.1992
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2534.9 words/s - loss: 0.2182
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2547.8 words/s - loss: 0.1993
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2216.9 words/s - loss: 0.2156
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 1973.7 words/s - loss: 0.2022
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.395 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.377 w/ 92 queries
INFO:__main__:removing file tmp/mbert_defr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_9.txt
INFO:__main__:[0.4211728123152919, 0.4120276690739034, 0.41159200976521276, 0.3941989475643074, 0.3947490415539745]
INFO:__main__:[0.3798221943352266, 0.3827072965618172, 0.3843211491955601, 0.36436977609711446, 0.3773227729642724]
INFO:__main__:0.41159200976521276
INFO:__main__:0.3798221943352266
INFO:__main__:best MAP: 0.396
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 695965.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 708712.78it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 238.11it/s]100%|██████████| 185/185 [00:00<00:00, 1144.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23592.16it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 261.05it/s]100%|██████████| 185/185 [00:00<00:00, 1245.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 21828.13it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 811842.68it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 747248.17it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 705992.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 734322.63it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 752665.54it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 657496.86it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 693663.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 689149.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 299.92it/s]100%|██████████| 185/185 [00:00<00:00, 1424.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23424.79it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 301.33it/s]100%|██████████| 185/185 [00:00<00:00, 1431.22it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23603.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 272.78it/s]100%|██████████| 185/185 [00:00<00:00, 1303.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23903.22it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 257.66it/s]100%|██████████| 185/185 [00:00<00:00, 1234.25it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 262.84it/s] 20%|██        | 37/185 [00:00<00:00, 250.19it/s]100%|██████████| 185/185 [00:00<00:00, 23429.04it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
 20%|██        | 37/185 [00:00<00:00, 268.47it/s]100%|██████████| 185/185 [00:00<00:00, 1199.94it/s]
100%|██████████| 185/185 [00:00<00:00, 1258.13it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1284.33it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24219.56it/s]
100%|██████████| 185/185 [00:00<00:00, 24041.71it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 23432.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 255.71it/s]100%|██████████| 185/185 [00:00<00:00, 1218.06it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 20581.04it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3062.9 words/s - loss: 0.4301
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3035.7 words/s - loss: 0.4289
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2812.0 words/s - loss: 0.4187
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2785.5 words/s - loss: 0.3950
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2768.9 words/s - loss: 0.4308
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2669.5 words/s - loss: 0.3702
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2489.5 words/s - loss: 0.4043
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2507.6 words/s - loss: 0.3991
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2352.0 words/s - loss: 0.4214
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2288.0 words/s - loss: 0.3895
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2817.8 words/s - loss: 0.3034
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2904.9 words/s - loss: 0.2860
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2686.7 words/s - loss: 0.3320
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 2513.4 words/s - loss: 0.2852
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2575.0 words/s - loss: 0.3116
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2755.6 words/s - loss: 0.2804
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2575.5 words/s - loss: 0.2906
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2667.2 words/s - loss: 0.3153
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2585.5 words/s - loss: 0.3263
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2500.8 words/s - loss: 0.2736
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2999.3 words/s - loss: 0.2428
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2593.7 words/s - loss: 0.2631
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2825.0 words/s - loss: 0.2640
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2705.0 words/s - loss: 0.2502
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2340.4 words/s - loss: 0.2769
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3050.7 words/s - loss: 0.3030
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2802.1 words/s - loss: 0.2481
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2664.1 words/s - loss: 0.2569
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2416.6 words/s - loss: 0.2696
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2266.5 words/s - loss: 0.2394
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2748.2 words/s - loss: 0.2364
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3313.4 words/s - loss: 0.2514
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2755.2 words/s - loss: 0.2357
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2386.0 words/s - loss: 0.2524
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2564.6 words/s - loss: 0.2507
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2448.6 words/s - loss: 0.2485
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2671.9 words/s - loss: 0.2141
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2632.5 words/s - loss: 0.2655
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2542.8 words/s - loss: 0.2172
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2282.2 words/s - loss: 0.2189
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2815.4 words/s - loss: 0.2296
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2616.0 words/s - loss: 0.2362
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3088.3 words/s - loss: 0.2371
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2703.7 words/s - loss: 0.2408
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2420.3 words/s - loss: 0.2093
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2695.2 words/s - loss: 0.2443
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2381.2 words/s - loss: 0.2526
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2724.8 words/s - loss: 0.2657
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2713.7 words/s - loss: 0.2501
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2426.8 words/s - loss: 0.2108
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2597.6 words/s - loss: 0.2154
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2885.2 words/s - loss: 0.2366
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2897.1 words/s - loss: 0.2266
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2698.6 words/s - loss: 0.2040
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2821.4 words/s - loss: 0.2162
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2557.0 words/s - loss: 0.2117
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2466.3 words/s - loss: 0.2311
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2476.7 words/s - loss: 0.2212
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2955.2 words/s - loss: 0.2536
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2617.3 words/s - loss: 0.2356
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.440 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.385 w/ 92 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_defr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 2994.5 words/s - loss: 0.2140
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2799.6 words/s - loss: 0.2465
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2676.3 words/s - loss: 0.2293
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2655.3 words/s - loss: 0.2412
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2637.9 words/s - loss: 0.2180
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2443.9 words/s - loss: 0.2306
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2475.9 words/s - loss: 0.2458
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2450.2 words/s - loss: 0.2218
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2286.5 words/s - loss: 0.2207
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2246.7 words/s - loss: 0.1875
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.434 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3394.9 words/s - loss: 0.2093
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2993.9 words/s - loss: 0.2310
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2996.5 words/s - loss: 0.2084
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2920.2 words/s - loss: 0.2527
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2741.0 words/s - loss: 0.2121
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2679.0 words/s - loss: 0.2006
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2503.0 words/s - loss: 0.2261
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2519.9 words/s - loss: 0.2085
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2502.0 words/s - loss: 0.2160
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2163.7 words/s - loss: 0.2144
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.427 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_defr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3373.8 words/s - loss: 0.1836
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2855.7 words/s - loss: 0.2404
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 2931.7 words/s - loss: 0.2015
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2818.3 words/s - loss: 0.2142
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2901.5 words/s - loss: 0.2023
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2761.6 words/s - loss: 0.2326
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2647.9 words/s - loss: 0.1938
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2488.4 words/s - loss: 0.2107
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2316.5 words/s - loss: 0.2200
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2088.6 words/s - loss: 0.2134
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.407 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.366 w/ 92 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_defr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2926.3 words/s - loss: 0.2115
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2903.9 words/s - loss: 0.2043
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2790.9 words/s - loss: 0.2142
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2834.4 words/s - loss: 0.1967
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2693.8 words/s - loss: 0.1995
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2724.9 words/s - loss: 0.2044
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 2653.1 words/s - loss: 0.2111
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2490.7 words/s - loss: 0.1884
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2491.1 words/s - loss: 0.2205
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2405.8 words/s - loss: 0.1941
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.419 w/ 93 queries
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_defr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.372 w/ 92 queries
INFO:__main__:removing file tmp/mbert_defr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_defr_f2_9_9.txt
INFO:__main__:[0.43962008441467837, 0.4342446624353066, 0.4266381272451513, 0.406896319911325, 0.41897218872580955]
INFO:__main__:[0.38459016931241125, 0.37896839282556266, 0.3790209442137191, 0.36586230586859314, 0.37225376451235975]
INFO:__main__:0.43962008441467837
INFO:__main__:0.38459016931241125
INFO:__main__:best MAP: 0.412

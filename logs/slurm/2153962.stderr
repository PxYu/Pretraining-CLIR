INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 791646.97it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 168.11it/s]100%|██████████| 176/176 [00:00<00:00, 1315.81it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16482.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 656262.36it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 663508.72it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 652302.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 655052.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 657311.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 645814.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 637199.81it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 637587.26it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 646610.55it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 174.31it/s] 12%|█▏        | 21/176 [00:00<00:01, 148.41it/s] 12%|█▏        | 21/176 [00:00<00:00, 168.03it/s]100%|██████████| 176/176 [00:00<00:00, 1356.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1167.36it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1314.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15633.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16143.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16477.26it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:01, 146.84it/s]100%|██████████| 176/176 [00:00<00:00, 1158.45it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 145.54it/s]100%|██████████| 176/176 [00:00<00:00, 16404.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1148.76it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 167.79it/s]100%|██████████| 176/176 [00:00<00:00, 16304.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1308.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15617.28it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:00, 166.70it/s]100%|██████████| 176/176 [00:00<00:00, 1301.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
 12%|█▏        | 21/176 [00:00<00:00, 159.15it/s]  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 129.50it/s]100%|██████████| 176/176 [00:00<00:00, 1248.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15978.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1027.14it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16141.46it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 15879.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3846.3 words/s - loss: 0.3635
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3811.6 words/s - loss: 0.3547
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3741.6 words/s - loss: 0.3496
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3738.7 words/s - loss: 0.3740
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3629.3 words/s - loss: 0.3681
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3465.0 words/s - loss: 0.3561
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3342.1 words/s - loss: 0.3708
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3321.2 words/s - loss: 0.3918
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3370.6 words/s - loss: 0.3697
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3397.7 words/s - loss: 0.3597
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4102.1 words/s - loss: 0.2545
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3555.3 words/s - loss: 0.2703
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3523.3 words/s - loss: 0.2278
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3417.0 words/s - loss: 0.2559
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3229.0 words/s - loss: 0.2498
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3290.1 words/s - loss: 0.2436
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3346.8 words/s - loss: 0.2433
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3389.4 words/s - loss: 0.2491
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3204.2 words/s - loss: 0.2899
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3062.7 words/s - loss: 0.2355
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3530.6 words/s - loss: 0.2246
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3553.6 words/s - loss: 0.2096
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3721.9 words/s - loss: 0.2077
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3498.7 words/s - loss: 0.2166
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3917.0 words/s - loss: 0.2290
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3452.9 words/s - loss: 0.2136
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3255.1 words/s - loss: 0.1942
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3190.4 words/s - loss: 0.2207
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3510.8 words/s - loss: 0.1863
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3258.5 words/s - loss: 0.2456
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3544.4 words/s - loss: 0.1856
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 4185.3 words/s - loss: 0.1826
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3568.3 words/s - loss: 0.2009
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3883.3 words/s - loss: 0.1766
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3694.0 words/s - loss: 0.1636
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3338.6 words/s - loss: 0.1705
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3928.9 words/s - loss: 0.1905
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3430.1 words/s - loss: 0.1912
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3262.0 words/s - loss: 0.2011
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3221.1 words/s - loss: 0.1902
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3649.3 words/s - loss: 0.1666
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3584.8 words/s - loss: 0.1685
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3274.1 words/s - loss: 0.1951
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3388.6 words/s - loss: 0.1847
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3554.6 words/s - loss: 0.1671
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3211.5 words/s - loss: 0.1407
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3540.2 words/s - loss: 0.1791
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3537.7 words/s - loss: 0.1640
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3790.0 words/s - loss: 0.1942
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2781.2 words/s - loss: 0.1609
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3818.1 words/s - loss: 0.1944
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4171.8 words/s - loss: 0.1612
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3799.7 words/s - loss: 0.1495
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3817.3 words/s - loss: 0.1479
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3898.5 words/s - loss: 0.1883
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3937.0 words/s - loss: 0.1281
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3650.5 words/s - loss: 0.1498
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3279.9 words/s - loss: 0.1635
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3851.6 words/s - loss: 0.1464
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3898.4 words/s - loss: 0.1648
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.308 w/ 88 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esen_f2_5_5.txt
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4033.0 words/s - loss: 0.1530
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3733.2 words/s - loss: 0.1720
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3735.4 words/s - loss: 0.1698
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3764.6 words/s - loss: 0.1973
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3658.9 words/s - loss: 0.1670
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3640.5 words/s - loss: 0.1688
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3583.6 words/s - loss: 0.1400
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3588.0 words/s - loss: 0.1282
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3447.7 words/s - loss: 0.1557
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3255.8 words/s - loss: 0.1446
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.314 w/ 88 queries
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3913.5 words/s - loss: 0.1377
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3789.6 words/s - loss: 0.1332
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3723.0 words/s - loss: 0.1503
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3785.9 words/s - loss: 0.1494
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3717.7 words/s - loss: 0.1654
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3604.0 words/s - loss: 0.1453
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3313.9 words/s - loss: 0.1518
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3283.5 words/s - loss: 0.1226
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3187.8 words/s - loss: 0.1264
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3070.6 words/s - loss: 0.1519
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esen_f2_7_7.txt
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3846.6 words/s - loss: 0.1404
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3634.1 words/s - loss: 0.1136
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3587.4 words/s - loss: 0.1486
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3560.7 words/s - loss: 0.1607
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3565.5 words/s - loss: 0.1408
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3440.4 words/s - loss: 0.1094
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3445.8 words/s - loss: 0.1340
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3414.5 words/s - loss: 0.1510
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3379.2 words/s - loss: 0.1478
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3062.2 words/s - loss: 0.1320
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.307 w/ 88 queries
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3904.1 words/s - loss: 0.1353
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3735.0 words/s - loss: 0.1368
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3565.1 words/s - loss: 0.1711
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3530.4 words/s - loss: 0.1359
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3460.9 words/s - loss: 0.1171
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3495.2 words/s - loss: 0.1189
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3398.0 words/s - loss: 0.1436
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3480.1 words/s - loss: 0.1397
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3220.4 words/s - loss: 0.1422
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3138.7 words/s - loss: 0.1263
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.378 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.312 w/ 88 queries
INFO:__main__:removing file tmp/mbert_esen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_9.txt
INFO:__main__:0.3745857798986417
INFO:__main__:0.31219868740195006
INFO:__main__:best MAP: 0.343
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 634002.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 566430.42it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 626576.64it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 644385.31it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 674498.91it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 633083.38it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 664328.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 665107.99it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 665656.88it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 696173.15it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 13%|█▎        | 23/176 [00:00<00:00, 169.99it/s]100%|██████████| 176/176 [00:00<00:00, 1221.00it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16169.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 153.14it/s]100%|██████████| 176/176 [00:00<00:00, 1106.41it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 12223.84it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 137.25it/s]100%|██████████| 176/176 [00:00<00:00, 997.99it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16516.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
 13%|█▎        | 23/176 [00:00<00:01, 133.57it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 970.76it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15639.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 151.03it/s]100%|██████████| 176/176 [00:00<00:00, 1091.57it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16051.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 169.61it/s]100%|██████████| 176/176 [00:00<00:00, 1219.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16350.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 164.24it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 145.08it/s]100%|██████████| 176/176 [00:00<00:00, 1179.99it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1048.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15664.67it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 15015.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 162.20it/s]100%|██████████| 176/176 [00:00<00:00, 1168.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15902.23it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 164.15it/s]100%|██████████| 176/176 [00:00<00:00, 1178.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15847.27it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3917.3 words/s - loss: 0.2749
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3661.6 words/s - loss: 0.2637
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3543.9 words/s - loss: 0.2527
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3518.1 words/s - loss: 0.2521
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3496.6 words/s - loss: 0.2353
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3470.7 words/s - loss: 0.2579
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3314.2 words/s - loss: 0.2406
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3387.7 words/s - loss: 0.2465
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3108.0 words/s - loss: 0.2605
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2929.7 words/s - loss: 0.2603
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4198.7 words/s - loss: 0.1633
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4141.0 words/s - loss: 0.1636
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3530.8 words/s - loss: 0.1556
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3824.7 words/s - loss: 0.1699
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3551.5 words/s - loss: 0.1535
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3178.2 words/s - loss: 0.1719
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3603.9 words/s - loss: 0.1869
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3089.6 words/s - loss: 0.1587
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3543.6 words/s - loss: 0.1772
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3637.4 words/s - loss: 0.1857
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3645.7 words/s - loss: 0.1516
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3969.9 words/s - loss: 0.1752
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3546.4 words/s - loss: 0.1368
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3963.1 words/s - loss: 0.1636
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3639.5 words/s - loss: 0.1150
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3491.3 words/s - loss: 0.1340
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3558.8 words/s - loss: 0.1558
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2984.1 words/s - loss: 0.1485
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3538.2 words/s - loss: 0.1517
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3370.9 words/s - loss: 0.1520
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3764.9 words/s - loss: 0.1264
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3643.7 words/s - loss: 0.1391
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3760.5 words/s - loss: 0.1785
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3350.7 words/s - loss: 0.1393
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3251.9 words/s - loss: 0.1296
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3799.5 words/s - loss: 0.1312
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3787.5 words/s - loss: 0.1663
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3690.4 words/s - loss: 0.1733
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3694.8 words/s - loss: 0.1642
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3127.9 words/s - loss: 0.1403
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3500.5 words/s - loss: 0.1339
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3409.9 words/s - loss: 0.1295
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3737.9 words/s - loss: 0.1461
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3687.6 words/s - loss: 0.1303
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3556.6 words/s - loss: 0.1047
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3677.3 words/s - loss: 0.1158
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3951.9 words/s - loss: 0.1471
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3888.6 words/s - loss: 0.1518
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3126.3 words/s - loss: 0.1217
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3619.6 words/s - loss: 0.1174
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3304.8 words/s - loss: 0.1212
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3855.2 words/s - loss: 0.1503
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3630.1 words/s - loss: 0.1451
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3491.9 words/s - loss: 0.1106
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 4073.8 words/s - loss: 0.1339
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3533.6 words/s - loss: 0.1543
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3558.2 words/s - loss: 0.0869
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3448.7 words/s - loss: 0.1521
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3595.7 words/s - loss: 0.1167
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2974.3 words/s - loss: 0.1237
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.414 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3901.6 words/s - loss: 0.1426
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3881.1 words/s - loss: 0.1149
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3735.8 words/s - loss: 0.1091
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3674.6 words/s - loss: 0.1041
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3659.0 words/s - loss: 0.1349
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3197.4 words/s - loss: 0.1333
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3286.0 words/s - loss: 0.0818
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3261.6 words/s - loss: 0.1017
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3218.2 words/s - loss: 0.1375
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3166.8 words/s - loss: 0.1133
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.361 w/ 88 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3851.7 words/s - loss: 0.0911
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3747.9 words/s - loss: 0.1173
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3701.5 words/s - loss: 0.1200
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3631.5 words/s - loss: 0.1071
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3553.6 words/s - loss: 0.1121
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3476.0 words/s - loss: 0.1070
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3392.9 words/s - loss: 0.1125
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3314.5 words/s - loss: 0.1283
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3348.5 words/s - loss: 0.1209
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3213.8 words/s - loss: 0.1425
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.411 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_1_7.txt
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3713.1 words/s - loss: 0.1347
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3720.2 words/s - loss: 0.1654
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3676.4 words/s - loss: 0.1161
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3628.6 words/s - loss: 0.1423
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3490.0 words/s - loss: 0.1110
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3458.3 words/s - loss: 0.1076
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3332.7 words/s - loss: 0.1067
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3370.1 words/s - loss: 0.1144
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3182.2 words/s - loss: 0.1208
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3139.7 words/s - loss: 0.1276
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.424 w/ 88 queries
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3788.8 words/s - loss: 0.0886
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3619.5 words/s - loss: 0.1229
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3655.6 words/s - loss: 0.1091
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3643.3 words/s - loss: 0.1113
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3592.5 words/s - loss: 0.0995
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3451.3 words/s - loss: 0.1275
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3511.2 words/s - loss: 0.1044
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3474.4 words/s - loss: 0.1161
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3445.5 words/s - loss: 0.1139
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3173.7 words/s - loss: 0.1118
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.374 w/ 88 queries
INFO:__main__:removing file tmp/mbert_esen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_9.txt
INFO:__main__:0.4338717665044273
INFO:__main__:0.37447954306047354
INFO:__main__:best MAP: 0.404
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 640703.90it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 650663.03it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 529276.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
  0%|          | 0/5000 [00:00<?, ?it/s]INFO:__main__:Evaluating every 1 epochs ...
100%|██████████| 5000/5000 [00:00<00:00, 409408.09it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 175.24it/s]100%|██████████| 176/176 [00:00<00:00, 1256.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16230.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 170.32it/s] 13%|█▎        | 23/176 [00:00<00:00, 166.91it/s]100%|██████████| 176/176 [00:00<00:00, 1224.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1199.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 201.41it/s]100%|██████████| 176/176 [00:00<00:00, 16208.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 15976.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1428.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15938.97it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 651613.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 643120.61it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 672164.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 670766.67it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 658012.61it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 674889.62it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 142.42it/s]100%|██████████| 176/176 [00:00<00:00, 1032.23it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 168.26it/s] 13%|█▎        | 23/176 [00:00<00:00, 167.66it/s]100%|██████████| 176/176 [00:00<00:00, 16012.96it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1208.19it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
100%|██████████| 176/176 [00:00<00:00, 1204.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16314.84it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 15972.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 176.56it/s]100%|██████████| 176/176 [00:00<00:00, 1261.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16088.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 178.44it/s]100%|██████████| 176/176 [00:00<00:00, 1275.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15877.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 171.84it/s]100%|██████████| 176/176 [00:00<00:00, 1230.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15480.38it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3931.8 words/s - loss: 0.2699
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3657.0 words/s - loss: 0.2568
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3725.1 words/s - loss: 0.2337
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3483.6 words/s - loss: 0.2529
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3442.7 words/s - loss: 0.2667
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3498.7 words/s - loss: 0.2410
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3531.9 words/s - loss: 0.2537
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3351.4 words/s - loss: 0.2551
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3426.9 words/s - loss: 0.2421
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3328.6 words/s - loss: 0.2679
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4077.9 words/s - loss: 0.1598
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3746.9 words/s - loss: 0.1561
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3768.5 words/s - loss: 0.1668
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4026.2 words/s - loss: 0.1609
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3400.7 words/s - loss: 0.1674
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3480.4 words/s - loss: 0.1563
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3340.0 words/s - loss: 0.1522
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3485.1 words/s - loss: 0.1573
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3242.6 words/s - loss: 0.1472
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3012.3 words/s - loss: 0.1644
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3915.4 words/s - loss: 0.1455
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3648.4 words/s - loss: 0.1557
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3416.3 words/s - loss: 0.1534
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3445.3 words/s - loss: 0.1538
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3880.3 words/s - loss: 0.1616
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3672.7 words/s - loss: 0.1407
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3400.9 words/s - loss: 0.1334
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3619.8 words/s - loss: 0.1249
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3388.2 words/s - loss: 0.1682
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3540.7 words/s - loss: 0.1316
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3269.5 words/s - loss: 0.1363
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3493.9 words/s - loss: 0.1309
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3380.0 words/s - loss: 0.1651
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3840.4 words/s - loss: 0.1346
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3933.5 words/s - loss: 0.1405
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3215.0 words/s - loss: 0.1257
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3469.2 words/s - loss: 0.1492
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3108.1 words/s - loss: 0.1371
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3222.4 words/s - loss: 0.1342
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3321.9 words/s - loss: 0.1473
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3466.8 words/s - loss: 0.1421
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3874.1 words/s - loss: 0.1327
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3595.6 words/s - loss: 0.1461
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3784.2 words/s - loss: 0.1242
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3688.1 words/s - loss: 0.1045
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3189.6 words/s - loss: 0.1154
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3498.9 words/s - loss: 0.1498
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3926.7 words/s - loss: 0.1448
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3448.3 words/s - loss: 0.1450
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3154.2 words/s - loss: 0.1322
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 4019.8 words/s - loss: 0.1213
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3817.5 words/s - loss: 0.1350
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3388.4 words/s - loss: 0.1135
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3158.5 words/s - loss: 0.0979
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3522.2 words/s - loss: 0.1188
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3611.9 words/s - loss: 0.1268
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3721.7 words/s - loss: 0.1153
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3457.5 words/s - loss: 0.1481
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3664.3 words/s - loss: 0.1072
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3575.5 words/s - loss: 0.1302
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.430 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.343 w/ 88 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3760.3 words/s - loss: 0.1171
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3622.0 words/s - loss: 0.1179
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3583.7 words/s - loss: 0.0956
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3570.5 words/s - loss: 0.0994
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3632.2 words/s - loss: 0.1489
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3666.1 words/s - loss: 0.1165
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3471.7 words/s - loss: 0.1307
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3374.0 words/s - loss: 0.1231
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3332.8 words/s - loss: 0.0966
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3263.9 words/s - loss: 0.0970
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.434 w/ 88 queries
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3911.5 words/s - loss: 0.1392
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3976.4 words/s - loss: 0.0994
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3898.1 words/s - loss: 0.1296
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3875.4 words/s - loss: 0.1146
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3803.3 words/s - loss: 0.1364
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3731.3 words/s - loss: 0.1340
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3661.9 words/s - loss: 0.0980
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3502.2 words/s - loss: 0.1039
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3295.5 words/s - loss: 0.1062
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3190.0 words/s - loss: 0.1284
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.417 w/ 88 queries
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4106.8 words/s - loss: 0.1211
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3697.7 words/s - loss: 0.1352
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3600.7 words/s - loss: 0.1120
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3532.8 words/s - loss: 0.1205
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3529.1 words/s - loss: 0.1201
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3529.3 words/s - loss: 0.1002
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3442.9 words/s - loss: 0.1313
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3421.9 words/s - loss: 0.1041
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3357.1 words/s - loss: 0.1252
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3213.5 words/s - loss: 0.1035
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.415 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esen_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esen_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.341 w/ 88 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esen_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
slurmstepd-asimov-214: error: *** JOB 2153962 ON asimov-214 CANCELLED AT 2020-08-01T19:43:01 ***

INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 714800.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 677549.75it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 714288.83it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 609601.77it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 703128.81it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 651572.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 645774.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 713389.80it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 696381.21it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 666587.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 189.13it/s]100%|██████████| 156/156 [00:00<00:00, 1210.61it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18386.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 196.17it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1251.82it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18100.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 193.72it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1238.03it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18739.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 150.93it/s]100%|██████████| 156/156 [00:00<00:00, 975.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17944.53it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 163.26it/s] 15%|█▍        | 23/156 [00:00<00:01, 130.16it/s]100%|██████████| 156/156 [00:00<00:00, 848.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 1052.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19131.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18156.66it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 15%|█▍        | 23/156 [00:00<00:00, 156.95it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1015.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18263.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 149.22it/s] 15%|█▍        | 23/156 [00:00<00:00, 155.33it/s]100%|██████████| 156/156 [00:00<00:00, 965.93it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1003.30it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18442.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 17633.10it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 151.06it/s]100%|██████████| 156/156 [00:00<00:00, 977.66it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18188.96it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 5483.3 words/s - loss: 0.5194
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 4517.3 words/s - loss: 0.5221
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 4348.5 words/s - loss: 0.5383
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4322.1 words/s - loss: 0.4999
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4393.3 words/s - loss: 0.5342
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 4190.3 words/s - loss: 0.5169
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3858.4 words/s - loss: 0.5333
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3893.9 words/s - loss: 0.5135
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3629.7 words/s - loss: 0.5254
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3539.7 words/s - loss: 0.5276
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3878.2 words/s - loss: 0.3275
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4227.8 words/s - loss: 0.2962
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4396.2 words/s - loss: 0.2894
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3976.1 words/s - loss: 0.2862
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4632.2 words/s - loss: 0.3518
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4230.8 words/s - loss: 0.3005
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3844.3 words/s - loss: 0.2948
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4358.7 words/s - loss: 0.2850
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3624.5 words/s - loss: 0.2751
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3683.0 words/s - loss: 0.2878
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 4360.4 words/s - loss: 0.2620
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 4433.5 words/s - loss: 0.2564
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4520.4 words/s - loss: 0.2416
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 4638.5 words/s - loss: 0.2458
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 4254.7 words/s - loss: 0.2780
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 4182.6 words/s - loss: 0.2803
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4069.4 words/s - loss: 0.2456
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 4860.0 words/s - loss: 0.2463
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3390.1 words/s - loss: 0.2225
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3578.0 words/s - loss: 0.2249
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4382.3 words/s - loss: 0.2047
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4078.7 words/s - loss: 0.2105
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4808.0 words/s - loss: 0.2703
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3971.0 words/s - loss: 0.2137
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3603.1 words/s - loss: 0.2377
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3245.0 words/s - loss: 0.2191
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3378.3 words/s - loss: 0.2359
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3657.1 words/s - loss: 0.2148
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4066.7 words/s - loss: 0.2230
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3887.5 words/s - loss: 0.2368
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4450.1 words/s - loss: 0.2166
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4384.2 words/s - loss: 0.1762
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3801.6 words/s - loss: 0.2285
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3660.2 words/s - loss: 0.2207
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3535.4 words/s - loss: 0.2022
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 4302.5 words/s - loss: 0.2100
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 4537.7 words/s - loss: 0.2203
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3651.3 words/s - loss: 0.2148
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3720.0 words/s - loss: 0.1756
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3915.9 words/s - loss: 0.2369
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4824.5 words/s - loss: 0.1925
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3802.4 words/s - loss: 0.1764
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3284.5 words/s - loss: 0.2314
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 4224.5 words/s - loss: 0.1931
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3834.5 words/s - loss: 0.1938
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3560.4 words/s - loss: 0.1838
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 4260.4 words/s - loss: 0.1995
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4020.5 words/s - loss: 0.2066
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3254.3 words/s - loss: 0.2057
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3688.8 words/s - loss: 0.1942
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.363 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.429 w/ 78 queries
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4729.6 words/s - loss: 0.1997
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 4609.2 words/s - loss: 0.2167
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4374.4 words/s - loss: 0.1883
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4137.6 words/s - loss: 0.2075
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 4110.1 words/s - loss: 0.1773
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3881.1 words/s - loss: 0.2140
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3990.3 words/s - loss: 0.1966
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3883.9 words/s - loss: 0.1727
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3899.6 words/s - loss: 0.2044
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3912.2 words/s - loss: 0.1760
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.374 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.438 w/ 78 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 4678.3 words/s - loss: 0.1678
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4506.7 words/s - loss: 0.1931
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4510.5 words/s - loss: 0.1827
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4264.7 words/s - loss: 0.2004
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4204.1 words/s - loss: 0.2280
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3945.6 words/s - loss: 0.1961
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3898.5 words/s - loss: 0.1748
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3862.0 words/s - loss: 0.1671
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3896.6 words/s - loss: 0.1753
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3766.2 words/s - loss: 0.1629
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.356 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.418 w/ 78 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4646.6 words/s - loss: 0.1785
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4257.5 words/s - loss: 0.1773
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 4109.1 words/s - loss: 0.2024
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 4074.8 words/s - loss: 0.1899
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4064.1 words/s - loss: 0.1913
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3918.1 words/s - loss: 0.1771
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3977.0 words/s - loss: 0.1585
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3858.9 words/s - loss: 0.1811
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3695.5 words/s - loss: 0.1824
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3496.6 words/s - loss: 0.1884
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.345 w/ 78 queries
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.408 w/ 78 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 4848.5 words/s - loss: 0.1858
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4700.1 words/s - loss: 0.1769
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 4636.3 words/s - loss: 0.2122
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4727.6 words/s - loss: 0.1700
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4553.5 words/s - loss: 0.1808
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4129.5 words/s - loss: 0.1865
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 4047.0 words/s - loss: 0.1694
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3646.4 words/s - loss: 0.1716
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3500.3 words/s - loss: 0.2031
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3416.4 words/s - loss: 0.1762
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.431 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:0.3741128221538401
INFO:__main__:0.43822116232023095
INFO:__main__:best MAP: 0.406
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 696358.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 643693.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 598690.23it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 717932.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684270.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 617172.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 682244.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 698119.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 677243.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 670680.87it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 246.32it/s]100%|██████████| 156/156 [00:00<00:00, 1339.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17433.89it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 240.37it/s]100%|██████████| 156/156 [00:00<00:00, 1310.06it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17897.41it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 221.98it/s]100%|██████████| 156/156 [00:00<00:00, 1210.84it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16833.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 177.07it/s]100%|██████████| 156/156 [00:00<00:00, 978.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17540.45it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 183.39it/s]100%|██████████| 156/156 [00:00<00:00, 1011.74it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18273.28it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 202.01it/s]100%|██████████| 156/156 [00:00<00:00, 1109.55it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17756.56it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 194.04it/s]100%|██████████| 156/156 [00:00<00:00, 1068.33it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17719.53it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 201.37it/s]100%|██████████| 156/156 [00:00<00:00, 1107.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 14605.49it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 154.79it/s]100%|██████████| 156/156 [00:00<00:00, 862.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19258.05it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 164.82it/s]100%|██████████| 156/156 [00:00<00:00, 909.97it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 16310.89it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4671.3 words/s - loss: 0.3392
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 4272.6 words/s - loss: 0.3448
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 4414.5 words/s - loss: 0.3038
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 4184.5 words/s - loss: 0.3206
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4177.6 words/s - loss: 0.3069
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4081.0 words/s - loss: 0.3312
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3793.5 words/s - loss: 0.3379
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3785.8 words/s - loss: 0.3183
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3608.5 words/s - loss: 0.3373
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3363.9 words/s - loss: 0.3108
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4705.2 words/s - loss: 0.2490
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4361.8 words/s - loss: 0.2342
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 4197.3 words/s - loss: 0.2389
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4009.4 words/s - loss: 0.2417
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 4048.1 words/s - loss: 0.1926
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 4002.6 words/s - loss: 0.2485
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4753.4 words/s - loss: 0.2508
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4048.8 words/s - loss: 0.2085
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3750.8 words/s - loss: 0.2016
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3701.5 words/s - loss: 0.2350
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3819.5 words/s - loss: 0.1910
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3755.3 words/s - loss: 0.1930
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3845.1 words/s - loss: 0.1840
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 4093.1 words/s - loss: 0.1997
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3752.8 words/s - loss: 0.2176
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 4328.6 words/s - loss: 0.1804
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3866.8 words/s - loss: 0.1837
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 4173.1 words/s - loss: 0.2098
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 4068.5 words/s - loss: 0.2092
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3493.1 words/s - loss: 0.2040
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4086.8 words/s - loss: 0.1830
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4260.9 words/s - loss: 0.1872
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4547.2 words/s - loss: 0.2142
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 4285.6 words/s - loss: 0.2078
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4098.3 words/s - loss: 0.1757
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4141.6 words/s - loss: 0.1993
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3857.5 words/s - loss: 0.1741
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3990.0 words/s - loss: 0.2018
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 4024.4 words/s - loss: 0.2019
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 4066.9 words/s - loss: 0.2001
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4710.1 words/s - loss: 0.1699
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4666.7 words/s - loss: 0.1669
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3594.2 words/s - loss: 0.2017
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 4189.6 words/s - loss: 0.2030
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3534.6 words/s - loss: 0.1642
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4559.6 words/s - loss: 0.1783
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3720.1 words/s - loss: 0.1621
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3720.9 words/s - loss: 0.1617
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 4178.9 words/s - loss: 0.1656
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3592.9 words/s - loss: 0.1563
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 4496.9 words/s - loss: 0.1581
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3907.9 words/s - loss: 0.1800
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4151.1 words/s - loss: 0.1623
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 4391.3 words/s - loss: 0.1792
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3772.8 words/s - loss: 0.1725
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4312.3 words/s - loss: 0.1805
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3945.4 words/s - loss: 0.1699
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3573.8 words/s - loss: 0.1820
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3967.8 words/s - loss: 0.1866
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3663.3 words/s - loss: 0.1594
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.508 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 4269.9 words/s - loss: 0.1580
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 4073.3 words/s - loss: 0.1875
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 4096.0 words/s - loss: 0.1704
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3884.1 words/s - loss: 0.1629
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3751.4 words/s - loss: 0.1727
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3907.2 words/s - loss: 0.1569
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3784.1 words/s - loss: 0.1791
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3777.4 words/s - loss: 0.1439
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3656.7 words/s - loss: 0.1633
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3631.0 words/s - loss: 0.1534
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.489 w/ 78 queries
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 4656.1 words/s - loss: 0.1678
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4227.5 words/s - loss: 0.1821
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4203.5 words/s - loss: 0.1945
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4120.8 words/s - loss: 0.1610
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3920.8 words/s - loss: 0.1532
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3957.5 words/s - loss: 0.1665
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3828.9 words/s - loss: 0.1653
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3780.7 words/s - loss: 0.1520
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3513.3 words/s - loss: 0.1576
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3490.8 words/s - loss: 0.1464
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4703.6 words/s - loss: 0.1662
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4407.2 words/s - loss: 0.1535
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3943.6 words/s - loss: 0.1649
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3987.4 words/s - loss: 0.1221
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3920.5 words/s - loss: 0.1578
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3802.1 words/s - loss: 0.1478
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3691.7 words/s - loss: 0.1640
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3670.1 words/s - loss: 0.1635
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3520.8 words/s - loss: 0.1455
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3405.5 words/s - loss: 0.1915
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4721.2 words/s - loss: 0.1435
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4236.2 words/s - loss: 0.1426
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4159.7 words/s - loss: 0.1411
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3796.5 words/s - loss: 0.1597
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3620.8 words/s - loss: 0.1412
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3662.1 words/s - loss: 0.1746
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3618.4 words/s - loss: 0.1748
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3596.2 words/s - loss: 0.1342
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3526.8 words/s - loss: 0.1457
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3183.4 words/s - loss: 0.1715
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.512 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.559 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
INFO:__main__:0.511903129451424
INFO:__main__:0.5592687280459586
INFO:__main__:best MAP: 0.536
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='fr', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 748074.48it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 658943.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 720868.97it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 713535.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 682333.50it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 709480.02it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 727849.23it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 716656.53it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 17%|█▋        | 27/156 [00:00<00:00, 265.31it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1437.00it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 671582.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 156/156 [00:00<00:00, 17884.69it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 686825.18it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 17%|█▋        | 27/156 [00:00<00:00, 221.75it/s]100%|██████████| 156/156 [00:00<00:00, 1215.00it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18236.10it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 171.80it/s]100%|██████████| 156/156 [00:00<00:00, 951.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17975.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 204.72it/s] 17%|█▋        | 27/156 [00:00<00:00, 188.14it/s]100%|██████████| 156/156 [00:00<00:00, 1123.84it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 209.10it/s]100%|██████████| 156/156 [00:00<00:00, 1037.84it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 204.71it/s]100%|██████████| 156/156 [00:00<00:00, 1147.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 18181.88it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1123.52it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18160.69it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
100%|██████████| 156/156 [00:00<00:00, 17675.49it/s]INFO:__main__:f2 has 78 queries ...

INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 17790.35it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 171.21it/s]100%|██████████| 156/156 [00:00<00:00, 947.85it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17792.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 178.01it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 983.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
 17%|█▋        | 27/156 [00:00<00:00, 178.97it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 990.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17913.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18150.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4621.8 words/s - loss: 0.3142
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 4613.3 words/s - loss: 0.3268
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4510.1 words/s - loss: 0.3467
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4530.5 words/s - loss: 0.3204
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 4206.9 words/s - loss: 0.3457
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 4029.4 words/s - loss: 0.3361
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 4235.1 words/s - loss: 0.3312
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 4058.0 words/s - loss: 0.3090
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3958.2 words/s - loss: 0.3116
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3615.3 words/s - loss: 0.3363
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 4483.8 words/s - loss: 0.2038
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 4389.8 words/s - loss: 0.2107
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 4110.8 words/s - loss: 0.2455
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3986.5 words/s - loss: 0.2323
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3728.2 words/s - loss: 0.2667
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3878.1 words/s - loss: 0.2176
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4323.9 words/s - loss: 0.2217
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3770.3 words/s - loss: 0.1878
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4025.1 words/s - loss: 0.2291
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3733.9 words/s - loss: 0.1858
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 4019.6 words/s - loss: 0.2303
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4212.8 words/s - loss: 0.1841
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 4694.4 words/s - loss: 0.1718
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 4132.0 words/s - loss: 0.1994
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4304.1 words/s - loss: 0.1911
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3857.6 words/s - loss: 0.1937
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3755.2 words/s - loss: 0.2044
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3521.5 words/s - loss: 0.1811
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 4520.5 words/s - loss: 0.1956
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3366.6 words/s - loss: 0.1977
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 4137.4 words/s - loss: 0.1913
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4149.5 words/s - loss: 0.2345
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4169.1 words/s - loss: 0.1851
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3769.8 words/s - loss: 0.1777
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4075.9 words/s - loss: 0.1874
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3619.4 words/s - loss: 0.1901
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 4162.0 words/s - loss: 0.1990
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3562.7 words/s - loss: 0.1823
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4130.6 words/s - loss: 0.1833
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3594.7 words/s - loss: 0.1992
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 5061.0 words/s - loss: 0.1537
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3717.4 words/s - loss: 0.2013
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 4556.1 words/s - loss: 0.1515
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 4042.1 words/s - loss: 0.1725
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 4117.4 words/s - loss: 0.1583
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4227.3 words/s - loss: 0.1598
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4810.6 words/s - loss: 0.1695
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3707.1 words/s - loss: 0.1653
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3254.7 words/s - loss: 0.1521
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3883.6 words/s - loss: 0.1906
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 4229.9 words/s - loss: 0.1771
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4695.9 words/s - loss: 0.1626
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4296.8 words/s - loss: 0.1523
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4477.1 words/s - loss: 0.1578
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 4367.0 words/s - loss: 0.1433
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3555.1 words/s - loss: 0.1467
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3777.3 words/s - loss: 0.1696
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3374.2 words/s - loss: 0.1676
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 4104.8 words/s - loss: 0.1948
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3661.6 words/s - loss: 0.1576
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.496 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4381.1 words/s - loss: 0.1604
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4292.5 words/s - loss: 0.1623
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4381.1 words/s - loss: 0.1828
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 4032.8 words/s - loss: 0.1685
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3932.6 words/s - loss: 0.1562
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3776.2 words/s - loss: 0.1667
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3627.4 words/s - loss: 0.1560
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3544.8 words/s - loss: 0.1887
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3528.6 words/s - loss: 0.1773
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3463.7 words/s - loss: 0.1714
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.499 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.555 w/ 78 queries
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_fres_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4324.8 words/s - loss: 0.1572
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 4333.4 words/s - loss: 0.1602
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4200.9 words/s - loss: 0.1297
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 4046.9 words/s - loss: 0.1757
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 4001.7 words/s - loss: 0.1580
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3946.1 words/s - loss: 0.1549
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3798.8 words/s - loss: 0.1610
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3721.5 words/s - loss: 0.1586
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3607.6 words/s - loss: 0.1378
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3404.9 words/s - loss: 0.1694
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.519 w/ 78 queries
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.562 w/ 78 queries
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_fres_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 5069.0 words/s - loss: 0.1674
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 4637.0 words/s - loss: 0.1585
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4295.7 words/s - loss: 0.1648
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 4168.4 words/s - loss: 0.1364
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 4213.8 words/s - loss: 0.1524
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4052.7 words/s - loss: 0.1424
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3843.8 words/s - loss: 0.1404
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3734.0 words/s - loss: 0.1574
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3567.7 words/s - loss: 0.1349
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3463.8 words/s - loss: 0.1576
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.505 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.556 w/ 78 queries
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_fres_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 5198.5 words/s - loss: 0.1403
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4391.6 words/s - loss: 0.1445
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4309.8 words/s - loss: 0.1430
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 4188.6 words/s - loss: 0.1486
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4297.1 words/s - loss: 0.1575
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 4092.4 words/s - loss: 0.1592
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3663.6 words/s - loss: 0.1403
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3769.5 words/s - loss: 0.1549
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3553.5 words/s - loss: 0.1505
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3359.6 words/s - loss: 0.1240
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.509 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_fres_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f1_7_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.557 w/ 78 queries
INFO:__main__:removing file tmp/mbert_fres_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_fres_f2_6_9.txt
Traceback (most recent call last):
  File "finetune-search.py", line 580, in <module>
    clir.run()
  File "finetune-search.py", line 377, in run
    logger.info(f1_maps)
NameError: name 'f1_maps' is not defined
Traceback (most recent call last):
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/mnt/home/puxuan/miniconda3/envs/rtx/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/mnt/home/puxuan/miniconda3/envs/rtx/bin/python', '-u', 'finetune-search.py', '--local_rank=9', '--model_type', 'mbert', '--model_path', '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', '--dataset', 'mix', '--source_lang', 'fr', '--target_lang', 'es', '--batch_size', '16', '--full_doc_length', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '10', '--apex_level', 'O2', '--encoder_lr', '2e-5', '--projector_lr', '2e-5', '--num_ft_encoders', '4', '--seed', '611']' returned non-zero exit status 1.

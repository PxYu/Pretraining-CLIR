INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 817858.20it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 770020.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 706230.68it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 207.68it/s]100%|██████████| 156/156 [00:00<00:00, 1321.09it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18364.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
 15%|█▍        | 23/156 [00:00<00:00, 152.02it/s]100%|██████████| 156/156 [00:00<00:00, 984.74it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18669.54it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 713341.27it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 729342.70it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 699120.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 690465.87it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 15%|█▍        | 23/156 [00:00<00:00, 222.69it/s]100%|██████████| 156/156 [00:00<00:00, 1412.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18671.67it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 688991.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 653298.03it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 672185.65it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 195.00it/s]100%|██████████| 156/156 [00:00<00:00, 1248.14it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18948.52it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 187.44it/s] 15%|█▍        | 23/156 [00:00<00:00, 188.95it/s]100%|██████████| 156/156 [00:00<00:00, 1200.78it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1209.10it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18451.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
100%|██████████| 156/156 [00:00<00:00, 18595.26it/s]
  0%|          | 0/156 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:01, 124.46it/s]100%|██████████| 156/156 [00:00<00:00, 811.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18441.70it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:01, 121.26it/s] 15%|█▍        | 23/156 [00:00<00:00, 199.19it/s]100%|██████████| 156/156 [00:00<00:00, 791.37it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1270.25it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18218.84it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 18199.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 170.36it/s]100%|██████████| 156/156 [00:00<00:00, 1093.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17552.68it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4742.5 words/s - loss: 0.3928
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4486.0 words/s - loss: 0.3942
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 4304.2 words/s - loss: 0.4028
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4063.0 words/s - loss: 0.3977
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3761.6 words/s - loss: 0.4119
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3688.6 words/s - loss: 0.4318
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3674.1 words/s - loss: 0.3907
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3573.2 words/s - loss: 0.4180
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3155.3 words/s - loss: 0.4186
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3061.3 words/s - loss: 0.4075
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4270.0 words/s - loss: 0.2410
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3900.3 words/s - loss: 0.2288
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3935.2 words/s - loss: 0.2428
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3491.3 words/s - loss: 0.2808
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4247.9 words/s - loss: 0.2825
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4085.9 words/s - loss: 0.2693
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3673.2 words/s - loss: 0.2745
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3860.9 words/s - loss: 0.2489
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4107.2 words/s - loss: 0.2843
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3658.0 words/s - loss: 0.2587
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3587.8 words/s - loss: 0.2315
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 4061.4 words/s - loss: 0.2436
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4048.1 words/s - loss: 0.2325
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3622.0 words/s - loss: 0.2336
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4259.6 words/s - loss: 0.2195
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3840.4 words/s - loss: 0.2364
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3474.6 words/s - loss: 0.2470
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 4039.7 words/s - loss: 0.2297
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3085.0 words/s - loss: 0.2572
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3716.5 words/s - loss: 0.2408
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4081.5 words/s - loss: 0.2094
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4290.8 words/s - loss: 0.2124
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3862.0 words/s - loss: 0.2830
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3821.3 words/s - loss: 0.2023
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3459.6 words/s - loss: 0.2233
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3634.9 words/s - loss: 0.2204
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 4158.8 words/s - loss: 0.2328
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 4054.8 words/s - loss: 0.2222
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3682.7 words/s - loss: 0.1929
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3692.1 words/s - loss: 0.2391
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 4284.8 words/s - loss: 0.2213
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4435.1 words/s - loss: 0.2134
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3964.7 words/s - loss: 0.2113
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4006.6 words/s - loss: 0.2114
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3742.7 words/s - loss: 0.2084
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3097.6 words/s - loss: 0.2396
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3475.8 words/s - loss: 0.1979
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3305.1 words/s - loss: 0.2155
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4394.8 words/s - loss: 0.1938
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3513.8 words/s - loss: 0.2193
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 4500.6 words/s - loss: 0.1801
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 4606.6 words/s - loss: 0.1882
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3257.7 words/s - loss: 0.1989
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3499.2 words/s - loss: 0.1956
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 4064.3 words/s - loss: 0.2029
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4179.8 words/s - loss: 0.1840
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4307.8 words/s - loss: 0.1884
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 4123.2 words/s - loss: 0.2037
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4185.2 words/s - loss: 0.1885
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3677.2 words/s - loss: 0.2136
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.409 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.465 w/ 78 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4309.6 words/s - loss: 0.1691
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 4086.4 words/s - loss: 0.1729
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3794.2 words/s - loss: 0.2180
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3788.1 words/s - loss: 0.1922
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3775.5 words/s - loss: 0.2051
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3790.9 words/s - loss: 0.1731
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3630.5 words/s - loss: 0.1995
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3560.6 words/s - loss: 0.1858
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3691.3 words/s - loss: 0.1982
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3381.0 words/s - loss: 0.2005
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.369 w/ 78 queries
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.442 w/ 78 queries
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4414.0 words/s - loss: 0.1810
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4159.8 words/s - loss: 0.1824
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4190.3 words/s - loss: 0.1839
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 4132.7 words/s - loss: 0.1840
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4077.9 words/s - loss: 0.1853
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3995.3 words/s - loss: 0.1823
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3752.8 words/s - loss: 0.1714
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3415.0 words/s - loss: 0.1833
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3421.5 words/s - loss: 0.1838
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3370.9 words/s - loss: 0.1933
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.405 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.485 w/ 78 queries
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4532.1 words/s - loss: 0.1694
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4520.8 words/s - loss: 0.1901
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3982.6 words/s - loss: 0.1762
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 4022.4 words/s - loss: 0.1922
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3918.4 words/s - loss: 0.1883
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3911.1 words/s - loss: 0.1873
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3740.9 words/s - loss: 0.2000
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3698.1 words/s - loss: 0.1706
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3580.9 words/s - loss: 0.1726
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3437.0 words/s - loss: 0.1943
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.398 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.461 w/ 78 queries
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 5112.0 words/s - loss: 0.1616
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 4938.5 words/s - loss: 0.1784
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4246.7 words/s - loss: 0.1613
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 4068.7 words/s - loss: 0.1743
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3823.0 words/s - loss: 0.1804
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3532.8 words/s - loss: 0.1864
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3623.7 words/s - loss: 0.1797
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3540.7 words/s - loss: 0.1802
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3483.9 words/s - loss: 0.1802
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3372.8 words/s - loss: 0.1608
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.411 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.471 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:[0.4088194981721138, 0.36930896216991477, 0.4049726609309847, 0.3983863201561815, 0.41119062273913815]
INFO:__main__:[0.46536935362379667, 0.44219146771315265, 0.48501464584722515, 0.4607644304089416, 0.4713980958759268]
INFO:__main__:0.4049726609309847
INFO:__main__:0.4713980958759268
INFO:__main__:best MAP: 0.438
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 685321.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 664454.72it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 734451.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 724004.70it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 654827.95it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 726185.81it/s]
100%|██████████| 5000/5000 [00:00<00:00, 721340.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 692861.11it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 711284.76it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 577648.26it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 212.33it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
100%|██████████| 156/156 [00:00<00:00, 1164.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18193.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 204.43it/s]100%|██████████| 156/156 [00:00<00:00, 1121.63it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17775.37it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 153.33it/s]100%|██████████| 156/156 [00:00<00:00, 853.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17881.76it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 137.62it/s] 17%|█▋        | 27/156 [00:00<00:00, 167.27it/s] 17%|█▋        | 27/156 [00:00<00:00, 179.17it/s]100%|██████████| 156/156 [00:00<00:00, 768.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 182.53it/s]100%|██████████| 156/156 [00:00<00:00, 990.37it/s]
100%|██████████| 156/156 [00:00<00:00, 926.44it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1008.76it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17947.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 17942.56it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 17417.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
100%|██████████| 156/156 [00:00<00:00, 18046.98it/s]
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 200.11it/s]100%|██████████| 156/156 [00:00<00:00, 1101.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18192.50it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 195.03it/s]100%|██████████| 156/156 [00:00<00:00, 1073.20it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17142.93it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 149.21it/s]100%|██████████| 156/156 [00:00<00:00, 830.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 11827.97it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 4150.9 words/s - loss: 0.3013
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4151.5 words/s - loss: 0.2884
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3931.6 words/s - loss: 0.3178
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3761.8 words/s - loss: 0.2875
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3801.8 words/s - loss: 0.3078
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3686.6 words/s - loss: 0.3080
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3676.5 words/s - loss: 0.2966
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3543.6 words/s - loss: 0.3002
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3449.5 words/s - loss: 0.2693
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3287.7 words/s - loss: 0.3148
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4854.6 words/s - loss: 0.2338
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 4118.9 words/s - loss: 0.2348
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 4184.3 words/s - loss: 0.2557
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3799.4 words/s - loss: 0.2228
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 4202.7 words/s - loss: 0.2281
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3647.5 words/s - loss: 0.2324
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3866.2 words/s - loss: 0.1993
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3813.1 words/s - loss: 0.2344
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3349.3 words/s - loss: 0.2008
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3434.7 words/s - loss: 0.2456
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 4045.7 words/s - loss: 0.2024
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4628.2 words/s - loss: 0.2016
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 4181.2 words/s - loss: 0.2185
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 4259.3 words/s - loss: 0.2050
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3537.6 words/s - loss: 0.2220
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3650.6 words/s - loss: 0.2044
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3799.0 words/s - loss: 0.2183
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3416.4 words/s - loss: 0.2294
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3530.0 words/s - loss: 0.2008
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3444.2 words/s - loss: 0.2122
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4011.6 words/s - loss: 0.2070
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3891.9 words/s - loss: 0.1720
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 4511.3 words/s - loss: 0.2250
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3542.7 words/s - loss: 0.1652
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 4066.7 words/s - loss: 0.1902
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4000.7 words/s - loss: 0.1967
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3782.3 words/s - loss: 0.1855
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3588.5 words/s - loss: 0.1919
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3789.1 words/s - loss: 0.1737
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3602.4 words/s - loss: 0.2051
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 4161.9 words/s - loss: 0.1904
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4655.0 words/s - loss: 0.1953
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3776.9 words/s - loss: 0.2117
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3242.5 words/s - loss: 0.2094
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3848.9 words/s - loss: 0.1984
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3734.2 words/s - loss: 0.1775
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4197.3 words/s - loss: 0.1811
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 4001.9 words/s - loss: 0.1682
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 4151.3 words/s - loss: 0.1565
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3891.0 words/s - loss: 0.2120
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3837.8 words/s - loss: 0.1427
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4049.5 words/s - loss: 0.1629
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3550.7 words/s - loss: 0.1879
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3584.7 words/s - loss: 0.1708
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3911.2 words/s - loss: 0.1812
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3293.6 words/s - loss: 0.1712
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3543.2 words/s - loss: 0.1879
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4019.1 words/s - loss: 0.1775
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 4556.7 words/s - loss: 0.1774
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3453.6 words/s - loss: 0.1772
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.490 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 4542.0 words/s - loss: 0.1666
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 4064.3 words/s - loss: 0.2142
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4016.9 words/s - loss: 0.1704
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3839.7 words/s - loss: 0.1542
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3609.6 words/s - loss: 0.1995
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3549.5 words/s - loss: 0.1726
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3622.5 words/s - loss: 0.2036
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3608.3 words/s - loss: 0.1527
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3287.5 words/s - loss: 0.1805
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3287.3 words/s - loss: 0.1568
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.536 w/ 78 queries
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 4161.4 words/s - loss: 0.1539
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 4034.5 words/s - loss: 0.1827
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3973.8 words/s - loss: 0.1912
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4098.4 words/s - loss: 0.1634
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4007.9 words/s - loss: 0.2024
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3921.5 words/s - loss: 0.1733
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3806.2 words/s - loss: 0.1640
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3592.0 words/s - loss: 0.1554
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3476.9 words/s - loss: 0.1708
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3278.8 words/s - loss: 0.1659
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 4647.4 words/s - loss: 0.1863
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3966.4 words/s - loss: 0.1896
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3979.2 words/s - loss: 0.1653
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3788.5 words/s - loss: 0.1652
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3729.7 words/s - loss: 0.1595
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3790.8 words/s - loss: 0.1683
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3600.1 words/s - loss: 0.1622
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3581.7 words/s - loss: 0.1544
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3449.8 words/s - loss: 0.1746
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3544.3 words/s - loss: 0.1455
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4734.9 words/s - loss: 0.1511
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4096.3 words/s - loss: 0.1329
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 4000.4 words/s - loss: 0.1485
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3989.0 words/s - loss: 0.1669
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3992.9 words/s - loss: 0.2040
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3621.4 words/s - loss: 0.1761
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3587.6 words/s - loss: 0.1652
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3495.8 words/s - loss: 0.1583
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3424.9 words/s - loss: 0.1817
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3244.2 words/s - loss: 0.1782
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.477 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.544 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:[0.4902253280401348, 0.4740392528653447, 0.48009001783403205, 0.4747576481467324, 0.47741825811265354]
INFO:__main__:[0.545934021097506, 0.5364823569484491, 0.5368455362452965, 0.5524929325352993, 0.5438661202137853]
INFO:__main__:0.4747576481467324
INFO:__main__:0.545934021097506
INFO:__main__:best MAP: 0.510
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='en', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 674759.33it/s]
  0%|          | 0/5000 [00:00<?, ?it/s]INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 759424.95it/s]
100%|██████████| 5000/5000 [00:00<00:00, 767540.90it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 719928.60it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 702140.08it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 672573.68it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 698934.18it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 732527.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 722483.21it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 678777.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 188.56it/s]100%|██████████| 156/156 [00:00<00:00, 1038.63it/s]
 17%|█▋        | 27/156 [00:00<00:00, 187.72it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1034.59it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
 17%|█▋        | 27/156 [00:00<00:00, 189.39it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18064.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1044.65it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17687.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 18002.79it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 192.68it/s] 17%|█▋        | 27/156 [00:00<00:00, 191.76it/s]100%|██████████| 156/156 [00:00<00:00, 1059.47it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 1057.02it/s]  0%|          | 0/156 [00:00<?, ?it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17695.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 17989.92it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
 17%|█▋        | 27/156 [00:00<00:00, 197.91it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1088.86it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17735.38it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 143.97it/s]100%|██████████| 156/156 [00:00<00:00, 801.91it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 182.65it/s]100%|██████████| 156/156 [00:00<00:00, 17275.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 1007.30it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17043.80it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 202.60it/s]100%|██████████| 156/156 [00:00<00:00, 1112.57it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17565.88it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 214.47it/s]100%|██████████| 156/156 [00:00<00:00, 1176.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 17985.47it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 4523.3 words/s - loss: 0.3146
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 4364.4 words/s - loss: 0.3011
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3995.8 words/s - loss: 0.3025
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3873.5 words/s - loss: 0.2961
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3930.3 words/s - loss: 0.2988
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3902.5 words/s - loss: 0.3264
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3764.2 words/s - loss: 0.3033
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3701.9 words/s - loss: 0.3039
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3700.3 words/s - loss: 0.2809
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3111.4 words/s - loss: 0.3257
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 4495.3 words/s - loss: 0.2179
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 4194.5 words/s - loss: 0.2304
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3967.8 words/s - loss: 0.2577
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3902.0 words/s - loss: 0.2029
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 4171.0 words/s - loss: 0.2122
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3401.2 words/s - loss: 0.2410
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3509.6 words/s - loss: 0.2253
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3408.3 words/s - loss: 0.2294
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3667.6 words/s - loss: 0.2208
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3583.7 words/s - loss: 0.2099
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3975.6 words/s - loss: 0.2178
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3982.4 words/s - loss: 0.1988
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3978.7 words/s - loss: 0.1905
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3879.0 words/s - loss: 0.2080
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3973.6 words/s - loss: 0.1839
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3691.9 words/s - loss: 0.2252
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 4074.4 words/s - loss: 0.2135
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3975.6 words/s - loss: 0.1718
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3753.6 words/s - loss: 0.1993
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3847.2 words/s - loss: 0.2045
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 4146.8 words/s - loss: 0.2072
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 4128.5 words/s - loss: 0.1528
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 4532.5 words/s - loss: 0.1880
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3734.0 words/s - loss: 0.1779
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3511.4 words/s - loss: 0.1666
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3426.0 words/s - loss: 0.1748
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3819.9 words/s - loss: 0.2005
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3866.5 words/s - loss: 0.1957
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3600.6 words/s - loss: 0.2128
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3570.5 words/s - loss: 0.2056
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 4321.5 words/s - loss: 0.1772
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 4426.5 words/s - loss: 0.1685
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3909.6 words/s - loss: 0.2152
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 4117.8 words/s - loss: 0.1775
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3483.3 words/s - loss: 0.2099
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3627.3 words/s - loss: 0.1897
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3293.9 words/s - loss: 0.1957
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3416.4 words/s - loss: 0.1707
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3473.4 words/s - loss: 0.1889
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3836.7 words/s - loss: 0.1696
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3641.9 words/s - loss: 0.1672
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3956.6 words/s - loss: 0.1823
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3767.9 words/s - loss: 0.1451
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 4174.6 words/s - loss: 0.1595
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 4288.3 words/s - loss: 0.1704
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 4054.4 words/s - loss: 0.1797
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3693.7 words/s - loss: 0.1515
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3411.2 words/s - loss: 0.1758
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3474.6 words/s - loss: 0.1572
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3944.4 words/s - loss: 0.1708
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.484 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.558 w/ 78 queries
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_enes_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4476.7 words/s - loss: 0.1763
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 4196.8 words/s - loss: 0.1587
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 4156.4 words/s - loss: 0.1649
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 4060.7 words/s - loss: 0.2181
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3835.0 words/s - loss: 0.1724
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3708.8 words/s - loss: 0.1735
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3926.4 words/s - loss: 0.1693
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3641.9 words/s - loss: 0.1918
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3763.8 words/s - loss: 0.1509
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3664.7 words/s - loss: 0.1697
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.554 w/ 78 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 4395.3 words/s - loss: 0.1761
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 4209.1 words/s - loss: 0.1884
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 4022.1 words/s - loss: 0.1504
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3914.9 words/s - loss: 0.1412
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3830.7 words/s - loss: 0.1537
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3770.1 words/s - loss: 0.1769
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3832.6 words/s - loss: 0.1402
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3617.1 words/s - loss: 0.1494
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3491.4 words/s - loss: 0.1757
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3411.6 words/s - loss: 0.1649
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.553 w/ 78 queries
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_enes_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 4557.7 words/s - loss: 0.1805
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 4399.5 words/s - loss: 0.1485
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 4188.1 words/s - loss: 0.1646
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4012.9 words/s - loss: 0.1450
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 4003.2 words/s - loss: 0.1548
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3869.8 words/s - loss: 0.1822
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3838.4 words/s - loss: 0.1499
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3571.9 words/s - loss: 0.1817
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3414.0 words/s - loss: 0.1625
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3154.7 words/s - loss: 0.1555
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.548 w/ 78 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_enes_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 4722.5 words/s - loss: 0.1374
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 4392.3 words/s - loss: 0.1620
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 4004.0 words/s - loss: 0.1591
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3973.1 words/s - loss: 0.1436
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3739.2 words/s - loss: 0.1719
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3862.3 words/s - loss: 0.1591
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3904.3 words/s - loss: 0.1397
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3808.6 words/s - loss: 0.1737
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3249.6 words/s - loss: 0.1295
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3030.0 words/s - loss: 0.1484
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:f1 set during evaluation: 28230/28224
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.480 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_enes_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f1_0_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:f2 set during evaluation: 29540/29534
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:removing file tmp/mbert_enes_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_enes_f2_0_9.txt
INFO:__main__:[0.4841561327824877, 0.4822500555543519, 0.48248518637570786, 0.4714285423764449, 0.4797735928873361]
INFO:__main__:[0.5578058926739233, 0.5537197698249329, 0.5525501420539499, 0.5475095527027275, 0.5417046071997151]
INFO:__main__:0.4841561327824877
INFO:__main__:0.5578058926739233
INFO:__main__:best MAP: 0.521

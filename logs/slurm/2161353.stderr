INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 579467.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 650804.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  9%|▉         | 17/192 [00:00<00:01, 97.41it/s]100%|██████████| 192/192 [00:00<00:00, 1047.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19883.62it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 647568.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 730485.91it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 681645.97it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 632987.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 616773.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 705565.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 674759.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 646750.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  9%|▉         | 17/192 [00:00<00:01, 115.41it/s]100%|██████████| 192/192 [00:00<00:00, 1231.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19924.45it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 137.55it/s]100%|██████████| 192/192 [00:00<00:00, 1452.21it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19991.22it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 133.54it/s]  9%|▉         | 17/192 [00:00<00:01, 139.46it/s]100%|██████████| 192/192 [00:00<00:00, 1411.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1466.83it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19742.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19303.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 136.71it/s]100%|██████████| 192/192 [00:00<00:00, 1441.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19405.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 17/192 [00:00<00:01, 140.39it/s]  9%|▉         | 17/192 [00:00<00:01, 138.12it/s]100%|██████████| 192/192 [00:00<00:00, 1482.15it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1457.35it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 20114.56it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 19917.06it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 131.60it/s]100%|██████████| 192/192 [00:00<00:00, 1387.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18715.43it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 17/192 [00:00<00:01, 133.25it/s]100%|██████████| 192/192 [00:00<00:00, 1402.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18839.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3639.1 words/s - loss: 0.3465
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3598.2 words/s - loss: 0.3549
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3540.2 words/s - loss: 0.3435
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3197.9 words/s - loss: 0.3533
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3130.6 words/s - loss: 0.3467
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3011.2 words/s - loss: 0.3782
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2908.2 words/s - loss: 0.3285
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2668.9 words/s - loss: 0.3415
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2636.6 words/s - loss: 0.3195
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2286.7 words/s - loss: 0.3412
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3735.9 words/s - loss: 0.2421
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3053.2 words/s - loss: 0.2293
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3321.5 words/s - loss: 0.2336
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2812.3 words/s - loss: 0.2178
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3004.8 words/s - loss: 0.2224
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3045.3 words/s - loss: 0.2206
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 4249.5 words/s - loss: 0.2246
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 2964.8 words/s - loss: 0.2124
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2726.3 words/s - loss: 0.1989
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2513.6 words/s - loss: 0.1848
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2692.4 words/s - loss: 0.1993
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3326.0 words/s - loss: 0.1919
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2849.4 words/s - loss: 0.2035
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2982.8 words/s - loss: 0.1886
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3116.7 words/s - loss: 0.1676
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2672.8 words/s - loss: 0.1796
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2894.0 words/s - loss: 0.1939
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2717.8 words/s - loss: 0.1614
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2700.2 words/s - loss: 0.1777
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2772.9 words/s - loss: 0.1684
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 4229.7 words/s - loss: 0.1792
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3491.8 words/s - loss: 0.1946
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3163.9 words/s - loss: 0.2009
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2880.4 words/s - loss: 0.1703
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2476.2 words/s - loss: 0.1761
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2989.0 words/s - loss: 0.1562
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2969.2 words/s - loss: 0.1297
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3553.6 words/s - loss: 0.1984
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2898.7 words/s - loss: 0.1730
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3589.7 words/s - loss: 0.1810
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3445.0 words/s - loss: 0.1545
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2945.0 words/s - loss: 0.1296
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3298.8 words/s - loss: 0.1534
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3537.7 words/s - loss: 0.1530
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3308.9 words/s - loss: 0.1569
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3464.2 words/s - loss: 0.1477
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2526.0 words/s - loss: 0.1614
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3171.7 words/s - loss: 0.1597
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2422.0 words/s - loss: 0.1683
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2854.6 words/s - loss: 0.1400
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 4040.1 words/s - loss: 0.1490
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3097.7 words/s - loss: 0.1521
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3223.4 words/s - loss: 0.1543
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3223.5 words/s - loss: 0.1472
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 2754.8 words/s - loss: 0.1556
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2933.6 words/s - loss: 0.1463
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2965.1 words/s - loss: 0.1392
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2294.8 words/s - loss: 0.1597
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3049.0 words/s - loss: 0.1272
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2868.7 words/s - loss: 0.1411
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.354 w/ 96 queries
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.342 w/ 96 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4656.3 words/s - loss: 0.1444
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 4480.7 words/s - loss: 0.1140
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3267.2 words/s - loss: 0.1524
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3096.9 words/s - loss: 0.1464
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3036.8 words/s - loss: 0.1247
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3002.0 words/s - loss: 0.1616
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2970.9 words/s - loss: 0.1283
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2910.6 words/s - loss: 0.1394
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2941.8 words/s - loss: 0.1472
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2809.8 words/s - loss: 0.1387
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.333 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.337 w/ 96 queries
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3634.8 words/s - loss: 0.1740
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3675.6 words/s - loss: 0.1231
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3593.7 words/s - loss: 0.1213
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3485.4 words/s - loss: 0.1389
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3345.3 words/s - loss: 0.1226
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3282.2 words/s - loss: 0.1449
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 2967.2 words/s - loss: 0.1201
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2906.2 words/s - loss: 0.1230
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2894.7 words/s - loss: 0.1304
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2798.4 words/s - loss: 0.1505
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.338 w/ 96 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 4490.1 words/s - loss: 0.1261
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 4297.3 words/s - loss: 0.1323
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3932.4 words/s - loss: 0.1361
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3792.1 words/s - loss: 0.1373
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3698.9 words/s - loss: 0.1626
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3621.8 words/s - loss: 0.1324
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3496.1 words/s - loss: 0.1279
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3381.3 words/s - loss: 0.1427
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3179.5 words/s - loss: 0.1321
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3151.2 words/s - loss: 0.1320
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.334 w/ 96 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.341 w/ 96 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esde_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 4171.5 words/s - loss: 0.1410
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3869.7 words/s - loss: 0.1387
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3404.7 words/s - loss: 0.1108
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3236.8 words/s - loss: 0.1096
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3130.3 words/s - loss: 0.1340
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3029.3 words/s - loss: 0.1456
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3020.3 words/s - loss: 0.1243
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2904.3 words/s - loss: 0.1083
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2865.3 words/s - loss: 0.1275
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2813.8 words/s - loss: 0.1353
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.355 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.359 w/ 96 queries
INFO:__main__:removing file tmp/mbert_esde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_9.txt
INFO:__main__:[0.3539658507529144, 0.3328562710762812, 0.3405016260004632, 0.33384631966325423, 0.3554538835719591]
INFO:__main__:[0.34150011435692385, 0.3367909505740188, 0.3382615848360005, 0.34059483826604176, 0.3591489343299443]
INFO:__main__:0.3554538835719591
INFO:__main__:0.3591489343299443
INFO:__main__:best MAP: 0.357
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 687951.71it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 708114.53it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 735223.67it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 721166.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 684203.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 657930.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 683512.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 653257.33it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 698189.57it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 688561.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 149.60it/s]100%|██████████| 192/192 [00:00<00:00, 1490.36it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 123.40it/s]100%|██████████| 192/192 [00:00<00:00, 19779.11it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1242.36it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 126.35it/s]100%|██████████| 192/192 [00:00<00:00, 19388.15it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1271.93it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 134.29it/s]100%|██████████| 192/192 [00:00<00:00, 19807.32it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1343.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19139.33it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
  9%|▉         | 18/192 [00:00<00:01, 121.55it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 136.00it/s]100%|██████████| 192/192 [00:00<00:00, 1226.61it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1360.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19638.27it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19011.01it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 125.21it/s]100%|██████████| 192/192 [00:00<00:00, 1258.69it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19067.73it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
  9%|▉         | 18/192 [00:00<00:01, 100.29it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1021.10it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 111.09it/s]100%|██████████| 192/192 [00:00<00:00, 19695.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 1122.77it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18682.87it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:02, 84.85it/s]100%|██████████| 192/192 [00:00<00:00, 867.40it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18463.98it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 4150.3 words/s - loss: 0.2341
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 4243.4 words/s - loss: 0.2327
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3797.8 words/s - loss: 0.2344
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3657.5 words/s - loss: 0.2316
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3247.2 words/s - loss: 0.2518
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3273.2 words/s - loss: 0.2152
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3212.9 words/s - loss: 0.2400
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3036.2 words/s - loss: 0.2164
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2856.9 words/s - loss: 0.2185
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2800.1 words/s - loss: 0.2488
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3590.2 words/s - loss: 0.1884
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3744.0 words/s - loss: 0.1402
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3278.7 words/s - loss: 0.1621
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3702.2 words/s - loss: 0.1502
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3210.7 words/s - loss: 0.1759
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3417.4 words/s - loss: 0.1387
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3550.9 words/s - loss: 0.1869
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2782.4 words/s - loss: 0.1380
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3071.3 words/s - loss: 0.1712
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2659.9 words/s - loss: 0.1664
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3995.6 words/s - loss: 0.1463
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 4452.4 words/s - loss: 0.1402
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2925.3 words/s - loss: 0.1469
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3005.3 words/s - loss: 0.1352
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3075.4 words/s - loss: 0.1294
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3021.8 words/s - loss: 0.1399
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3908.5 words/s - loss: 0.1355
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3222.5 words/s - loss: 0.1510
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3274.8 words/s - loss: 0.1428
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2767.9 words/s - loss: 0.1619
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3974.6 words/s - loss: 0.1453
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3368.9 words/s - loss: 0.1192
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3059.1 words/s - loss: 0.1377
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2880.5 words/s - loss: 0.1242
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 2615.2 words/s - loss: 0.1487
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3129.6 words/s - loss: 0.1124
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2793.7 words/s - loss: 0.1357
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2755.4 words/s - loss: 0.1341
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3361.4 words/s - loss: 0.1127
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2739.7 words/s - loss: 0.1608
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3126.0 words/s - loss: 0.1339
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3051.1 words/s - loss: 0.1281
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3583.5 words/s - loss: 0.1318
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3474.8 words/s - loss: 0.1194
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3608.6 words/s - loss: 0.1241
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2766.3 words/s - loss: 0.1112
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2556.6 words/s - loss: 0.1361
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3373.2 words/s - loss: 0.1146
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2812.5 words/s - loss: 0.1393
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3559.3 words/s - loss: 0.1061
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2506.2 words/s - loss: 0.1413
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3270.4 words/s - loss: 0.1400
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3851.2 words/s - loss: 0.1234
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3331.3 words/s - loss: 0.1152
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3587.6 words/s - loss: 0.1247
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2859.3 words/s - loss: 0.1205
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2653.6 words/s - loss: 0.1157
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3030.5 words/s - loss: 0.1346
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3086.1 words/s - loss: 0.1184
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3392.6 words/s - loss: 0.1529
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.408 w/ 96 queries
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.433 w/ 96 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3620.6 words/s - loss: 0.1287
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3470.7 words/s - loss: 0.1130
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3326.2 words/s - loss: 0.1370
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3305.9 words/s - loss: 0.1197
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3264.0 words/s - loss: 0.1167
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3186.7 words/s - loss: 0.1275
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 2907.9 words/s - loss: 0.0983
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2919.7 words/s - loss: 0.1244
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2925.6 words/s - loss: 0.1152
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2684.2 words/s - loss: 0.1236
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.429 w/ 96 queries
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 4005.3 words/s - loss: 0.1079
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3840.9 words/s - loss: 0.1330
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3779.7 words/s - loss: 0.1009
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3650.9 words/s - loss: 0.1314
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3245.1 words/s - loss: 0.0945
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3046.2 words/s - loss: 0.1304
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3105.8 words/s - loss: 0.0980
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3016.7 words/s - loss: 0.1224
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 2697.8 words/s - loss: 0.1156
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2161.5 words/s - loss: 0.1130
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.395 w/ 96 queries
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.426 w/ 96 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 4336.7 words/s - loss: 0.1302
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3313.9 words/s - loss: 0.1067
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3311.3 words/s - loss: 0.1115
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3240.1 words/s - loss: 0.0983
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3109.9 words/s - loss: 0.0968
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3011.3 words/s - loss: 0.1216
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2886.2 words/s - loss: 0.1092
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2757.3 words/s - loss: 0.1269
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2735.5 words/s - loss: 0.1091
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2696.9 words/s - loss: 0.0965
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.431 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.451 w/ 96 queries
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esde_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3396.9 words/s - loss: 0.1383
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3505.2 words/s - loss: 0.1057
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3476.3 words/s - loss: 0.0995
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3406.0 words/s - loss: 0.1077
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3287.5 words/s - loss: 0.1164
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3228.2 words/s - loss: 0.1064
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3200.5 words/s - loss: 0.1157
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3139.4 words/s - loss: 0.1227
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3013.1 words/s - loss: 0.0995
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2271.3 words/s - loss: 0.1084
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.415 w/ 96 queries
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.438 w/ 96 queries
INFO:__main__:removing file tmp/mbert_esde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_9.txt
INFO:__main__:[0.4075467852300158, 0.4286280636860003, 0.39468196878419715, 0.43086772886579333, 0.4150882266736366]
INFO:__main__:[0.43336157566250416, 0.4287897480075417, 0.42591775304783114, 0.4505116409460838, 0.4376474514578939]
INFO:__main__:0.43086772886579333
INFO:__main__:0.4505116409460838
INFO:__main__:best MAP: 0.441
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='de')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 686398.06it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 673091.76it/s]
  0%|          | 0/5000 [00:00<?, ?it/s]INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 704972.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 703364.64it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 685075.13it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 652911.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 701646.76it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 692380.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 712807.86it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 708449.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 131.61it/s]100%|██████████| 192/192 [00:00<00:00, 1321.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19887.55it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 127.53it/s]  9%|▉         | 18/192 [00:00<00:01, 126.59it/s]100%|██████████| 192/192 [00:00<00:00, 1282.19it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 1273.37it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19656.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19585.25it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 112.71it/s]100%|██████████| 192/192 [00:00<00:00, 1136.16it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 18486.87it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
  9%|▉         | 18/192 [00:00<00:01, 113.16it/s]  9%|▉         | 18/192 [00:00<00:01, 103.95it/s]100%|██████████| 192/192 [00:00<00:00, 1145.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 101.90it/s]  9%|▉         | 18/192 [00:00<00:01, 106.76it/s]100%|██████████| 192/192 [00:00<00:00, 1054.91it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 105.74it/s]100%|██████████| 192/192 [00:00<00:00, 1037.85it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19720.98it/s]
100%|██████████| 192/192 [00:00<00:00, 1083.48it/s]INFO:root:Number of labelled query-document pairs in [f2] set: 34553

INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]  9%|▉         | 18/192 [00:00<00:01, 93.27it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 18668.58it/s]
100%|██████████| 192/192 [00:00<00:00, 1075.13it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
  0%|          | 0/192 [00:00<?, ?it/s]100%|██████████| 192/192 [00:00<00:00, 19990.23it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 950.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 34436
100%|██████████| 192/192 [00:00<00:00, 19667.52it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
  0%|          | 0/192 [00:00<?, ?it/s]INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 192/192 [00:00<00:00, 19863.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
100%|██████████| 192/192 [00:00<00:00, 18456.36it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 34553
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 96 queries ...
INFO:__main__:f2 has 96 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3539.2 words/s - loss: 0.2311
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3563.6 words/s - loss: 0.2522
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3330.4 words/s - loss: 0.2623
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3178.4 words/s - loss: 0.2416
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3136.4 words/s - loss: 0.2377
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3028.3 words/s - loss: 0.2404
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2908.6 words/s - loss: 0.2321
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2894.9 words/s - loss: 0.2334
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2765.6 words/s - loss: 0.2141
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2734.0 words/s - loss: 0.2092
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3368.0 words/s - loss: 0.1676
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3302.2 words/s - loss: 0.1643
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3406.8 words/s - loss: 0.1670
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3862.0 words/s - loss: 0.1612
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3017.5 words/s - loss: 0.1756
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2923.2 words/s - loss: 0.1516
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2994.4 words/s - loss: 0.1311
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2480.4 words/s - loss: 0.1271
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2924.2 words/s - loss: 0.1620
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 2720.1 words/s - loss: 0.1540
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3867.4 words/s - loss: 0.1735
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3850.2 words/s - loss: 0.1459
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3894.5 words/s - loss: 0.1378
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2848.0 words/s - loss: 0.1425
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3086.7 words/s - loss: 0.1743
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 2763.0 words/s - loss: 0.1205
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3249.6 words/s - loss: 0.1375
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3235.5 words/s - loss: 0.1276
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 2964.9 words/s - loss: 0.1283
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2972.5 words/s - loss: 0.1313
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2990.4 words/s - loss: 0.1192
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3129.4 words/s - loss: 0.0941
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 4033.1 words/s - loss: 0.1355
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3832.9 words/s - loss: 0.1137
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3455.3 words/s - loss: 0.1512
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3668.1 words/s - loss: 0.1441
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2723.8 words/s - loss: 0.1365
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3534.6 words/s - loss: 0.1345
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3874.0 words/s - loss: 0.1395
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2544.2 words/s - loss: 0.1512
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 4082.6 words/s - loss: 0.1237
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3518.1 words/s - loss: 0.1079
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3581.7 words/s - loss: 0.1064
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2810.5 words/s - loss: 0.1226
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2728.4 words/s - loss: 0.1276
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2527.4 words/s - loss: 0.1193
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2952.1 words/s - loss: 0.1358
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3179.8 words/s - loss: 0.1173
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2879.3 words/s - loss: 0.1228
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2578.4 words/s - loss: 0.1495
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3847.2 words/s - loss: 0.1216
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3579.2 words/s - loss: 0.1195
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3061.2 words/s - loss: 0.1094
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3034.8 words/s - loss: 0.1074
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2594.1 words/s - loss: 0.1202
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3067.7 words/s - loss: 0.1238
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 2837.6 words/s - loss: 0.1127
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3421.5 words/s - loss: 0.1329
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2680.3 words/s - loss: 0.1191
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3331.0 words/s - loss: 0.1117
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.400 w/ 96 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.434 w/ 96 queries
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esde_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3962.7 words/s - loss: 0.1341
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3333.3 words/s - loss: 0.1136
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3362.8 words/s - loss: 0.1351
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3113.1 words/s - loss: 0.1075
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3053.6 words/s - loss: 0.1090
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3010.5 words/s - loss: 0.1126
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2993.1 words/s - loss: 0.1063
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2897.9 words/s - loss: 0.1105
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2659.1 words/s - loss: 0.1283
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2649.2 words/s - loss: 0.1180
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.424 w/ 96 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.441 w/ 96 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esde_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3484.0 words/s - loss: 0.1051
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3337.8 words/s - loss: 0.1142
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3286.1 words/s - loss: 0.1238
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3265.3 words/s - loss: 0.1121
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3222.7 words/s - loss: 0.1193
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2934.1 words/s - loss: 0.1188
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2985.3 words/s - loss: 0.0821
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2799.5 words/s - loss: 0.0882
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2733.9 words/s - loss: 0.0880
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2739.0 words/s - loss: 0.1114
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.419 w/ 96 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.446 w/ 96 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esde_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 4258.5 words/s - loss: 0.1138
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3438.8 words/s - loss: 0.0924
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3233.0 words/s - loss: 0.1139
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3031.1 words/s - loss: 0.1117
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2981.2 words/s - loss: 0.1110
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2873.7 words/s - loss: 0.1094
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2811.2 words/s - loss: 0.0904
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2756.3 words/s - loss: 0.1037
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2769.7 words/s - loss: 0.1071
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2610.4 words/s - loss: 0.1153
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.430 w/ 96 queries
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.445 w/ 96 queries
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esde_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esde_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3804.4 words/s - loss: 0.1113
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3732.8 words/s - loss: 0.1081
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3100.2 words/s - loss: 0.0959
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3055.0 words/s - loss: 0.0897
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 2935.3 words/s - loss: 0.0939
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2767.1 words/s - loss: 0.1169
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2793.6 words/s - loss: 0.1193
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2782.4 words/s - loss: 0.1013
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2778.9 words/s - loss: 0.1072
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2502.2 words/s - loss: 0.1276
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:f1 set during evaluation: 34440/34436
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.420 w/ 96 queries
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esde_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f1_3_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:f2 set during evaluation: 34560/34553
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.448 w/ 96 queries
INFO:__main__:removing file tmp/mbert_esde_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esde_f2_4_9.txt
INFO:__main__:[0.4003575756187437, 0.4238957102116883, 0.4193698803594204, 0.4302982733912728, 0.4196193009939259]
INFO:__main__:[0.43401311909650037, 0.4408101958917287, 0.4463122139766509, 0.4447233122606556, 0.44805989826224074]
INFO:__main__:0.4196193009939259
INFO:__main__:0.4447233122606556
INFO:__main__:best MAP: 0.432

INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 632053.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 662084.29it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 617281.45it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 668713.37it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 726689.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 404644.68it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 620679.53it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 672897.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 662858.59it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 610080.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 133.80it/s]100%|██████████| 176/176 [00:00<00:00, 1060.56it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16138.99it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:01, 130.02it/s]100%|██████████| 176/176 [00:00<00:00, 1031.58it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 120.99it/s] 12%|█▏        | 21/176 [00:00<00:00, 160.29it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:00, 170.15it/s] 12%|█▏        | 21/176 [00:00<00:00, 166.05it/s]100%|██████████| 176/176 [00:00<00:00, 16281.37it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 962.02it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1257.87it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1330.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1297.96it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15195.82it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 16427.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16420.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 15904.97it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:00, 156.19it/s]100%|██████████| 176/176 [00:00<00:00, 1220.99it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15012.56it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:00, 161.33it/s]100%|██████████| 176/176 [00:00<00:00, 1260.20it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 12%|█▏        | 21/176 [00:00<00:01, 147.97it/s]100%|██████████| 176/176 [00:00<00:00, 15250.12it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1164.57it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16074.68it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 12%|█▏        | 21/176 [00:00<00:01, 135.95it/s]100%|██████████| 176/176 [00:00<00:00, 1076.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16235.18it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3644.8 words/s - loss: 0.4313
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3433.3 words/s - loss: 0.4026
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3455.9 words/s - loss: 0.4193
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3377.2 words/s - loss: 0.4284
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3364.6 words/s - loss: 0.3985
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3294.3 words/s - loss: 0.4342
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3349.0 words/s - loss: 0.3923
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3259.7 words/s - loss: 0.4360
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3222.1 words/s - loss: 0.4150
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2915.8 words/s - loss: 0.3961
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3556.5 words/s - loss: 0.2653
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3712.1 words/s - loss: 0.2617
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3588.9 words/s - loss: 0.2457
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3472.7 words/s - loss: 0.2725
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3369.0 words/s - loss: 0.2427
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3311.2 words/s - loss: 0.2588
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3336.5 words/s - loss: 0.2371
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3351.0 words/s - loss: 0.2463
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3227.1 words/s - loss: 0.2309
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3573.0 words/s - loss: 0.2443
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 4075.8 words/s - loss: 0.2025
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3888.3 words/s - loss: 0.2106
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3899.9 words/s - loss: 0.2254
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3532.7 words/s - loss: 0.2260
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3558.9 words/s - loss: 0.2175
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3731.1 words/s - loss: 0.2450
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3253.5 words/s - loss: 0.2021
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3501.0 words/s - loss: 0.2365
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3344.8 words/s - loss: 0.1970
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3606.7 words/s - loss: 0.1792
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3491.1 words/s - loss: 0.1998
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3397.9 words/s - loss: 0.2052
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3643.2 words/s - loss: 0.2145
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3556.4 words/s - loss: 0.2186
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3421.7 words/s - loss: 0.1910
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3627.3 words/s - loss: 0.1740
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3223.9 words/s - loss: 0.1933
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3220.8 words/s - loss: 0.1915
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3323.6 words/s - loss: 0.2105
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3074.5 words/s - loss: 0.2163
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3373.6 words/s - loss: 0.2024
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3622.2 words/s - loss: 0.1860
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3484.6 words/s - loss: 0.1810
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3073.1 words/s - loss: 0.1761
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3557.2 words/s - loss: 0.1967
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3759.4 words/s - loss: 0.1905
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3635.2 words/s - loss: 0.1867
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3570.7 words/s - loss: 0.1839
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3607.0 words/s - loss: 0.2151
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3105.6 words/s - loss: 0.1889
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3567.1 words/s - loss: 0.1854
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3303.3 words/s - loss: 0.1825
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3334.8 words/s - loss: 0.1831
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3419.2 words/s - loss: 0.1581
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3314.3 words/s - loss: 0.1745
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3527.1 words/s - loss: 0.2060
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3544.9 words/s - loss: 0.2018
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3429.8 words/s - loss: 0.1641
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3354.4 words/s - loss: 0.1829
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3154.6 words/s - loss: 0.2009
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.315 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 4079.9 words/s - loss: 0.1843
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3753.4 words/s - loss: 0.1689
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3540.5 words/s - loss: 0.1657
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3535.3 words/s - loss: 0.1627
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3503.4 words/s - loss: 0.1502
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3461.3 words/s - loss: 0.1712
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3388.6 words/s - loss: 0.1772
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3225.9 words/s - loss: 0.1689
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3210.6 words/s - loss: 0.1849
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3208.4 words/s - loss: 0.1683
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.311 w/ 88 queries
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.258 w/ 88 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_deen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3900.4 words/s - loss: 0.1767
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3809.9 words/s - loss: 0.1793
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3420.8 words/s - loss: 0.1577
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3465.3 words/s - loss: 0.1479
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3396.4 words/s - loss: 0.2032
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3305.2 words/s - loss: 0.1590
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3312.8 words/s - loss: 0.1645
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3186.8 words/s - loss: 0.1414
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3139.0 words/s - loss: 0.1670
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3066.2 words/s - loss: 0.1639
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.309 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.271 w/ 88 queries
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_deen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3867.8 words/s - loss: 0.1347
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3807.2 words/s - loss: 0.1513
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3745.3 words/s - loss: 0.1487
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3694.0 words/s - loss: 0.1523
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3513.3 words/s - loss: 0.1543
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3446.1 words/s - loss: 0.1581
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3428.6 words/s - loss: 0.1679
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3318.4 words/s - loss: 0.1665
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3265.6 words/s - loss: 0.1717
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3110.2 words/s - loss: 0.1732
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.322 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.269 w/ 88 queries
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3880.0 words/s - loss: 0.1578
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3724.5 words/s - loss: 0.1450
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3514.6 words/s - loss: 0.1546
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3549.6 words/s - loss: 0.1363
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3547.0 words/s - loss: 0.1668
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3596.2 words/s - loss: 0.1450
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3520.2 words/s - loss: 0.1608
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3386.2 words/s - loss: 0.1591
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3214.5 words/s - loss: 0.1570
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2983.0 words/s - loss: 0.1642
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.299 w/ 88 queries
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_2_9.txt
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.259 w/ 88 queries
INFO:__main__:removing file tmp/mbert_deen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_9.txt
INFO:__main__:[0.31541739462709556, 0.31110131672279534, 0.3094290977839236, 0.3223698008358485, 0.299230607169121]
INFO:__main__:[0.2576829121278958, 0.2584052549016073, 0.2709473543809731, 0.2687892047160736, 0.2590174459132933]
INFO:__main__:0.3094290977839236
INFO:__main__:0.2687892047160736
INFO:__main__:best MAP: 0.289
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 677243.43it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 520475.52it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 656755.61it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 630002.40it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 682000.65it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 683244.93it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 646371.40it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 655831.38it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 688900.86it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 660728.42it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 156.63it/s]100%|██████████| 176/176 [00:00<00:00, 1129.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 120.04it/s] 13%|█▎        | 23/176 [00:00<00:01, 114.86it/s]100%|██████████| 176/176 [00:00<00:00, 16324.58it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 877.33it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 144.42it/s] 13%|█▎        | 23/176 [00:00<00:01, 118.23it/s] 13%|█▎        | 23/176 [00:00<00:01, 138.23it/s]100%|██████████| 176/176 [00:00<00:00, 842.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1047.46it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16197.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 866.03it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1006.11it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16340.84it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 169.95it/s]100%|██████████| 176/176 [00:00<00:00, 16174.00it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
100%|██████████| 176/176 [00:00<00:00, 16662.85it/s]INFO:__main__:f2 has 88 queries ...

INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 16540.02it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1221.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15892.99it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 170.15it/s]100%|██████████| 176/176 [00:00<00:00, 1221.00it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16273.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
 13%|█▎        | 23/176 [00:00<00:01, 140.25it/s]INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1018.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 132.19it/s]100%|██████████| 176/176 [00:00<00:00, 962.79it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16311.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 15855.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3783.3 words/s - loss: 0.3234
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3661.7 words/s - loss: 0.3027
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3705.8 words/s - loss: 0.2627
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3619.4 words/s - loss: 0.2834
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3569.0 words/s - loss: 0.2789
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3477.8 words/s - loss: 0.2731
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3425.6 words/s - loss: 0.3269
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3379.7 words/s - loss: 0.3022
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3307.3 words/s - loss: 0.2920
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3294.3 words/s - loss: 0.2892
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3862.9 words/s - loss: 0.2067
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3638.9 words/s - loss: 0.1820
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3885.9 words/s - loss: 0.2047
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3504.9 words/s - loss: 0.2048
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3333.8 words/s - loss: 0.1901
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3334.5 words/s - loss: 0.2101
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3311.8 words/s - loss: 0.2011
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3280.4 words/s - loss: 0.2117
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3296.3 words/s - loss: 0.2053
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3293.7 words/s - loss: 0.2056
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3633.3 words/s - loss: 0.1771
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3575.4 words/s - loss: 0.1958
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3477.6 words/s - loss: 0.1748
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3202.9 words/s - loss: 0.1915
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3275.9 words/s - loss: 0.1760
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3204.1 words/s - loss: 0.1942
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3571.6 words/s - loss: 0.1864
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3144.1 words/s - loss: 0.1452
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3183.5 words/s - loss: 0.1543
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3232.5 words/s - loss: 0.1713
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3926.3 words/s - loss: 0.1628
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3404.2 words/s - loss: 0.1538
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3527.8 words/s - loss: 0.1694
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3209.3 words/s - loss: 0.1831
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3795.1 words/s - loss: 0.1593
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3280.1 words/s - loss: 0.1838
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3215.7 words/s - loss: 0.1559
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3370.9 words/s - loss: 0.1734
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3546.8 words/s - loss: 0.1536
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3086.5 words/s - loss: 0.1702
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3397.3 words/s - loss: 0.1529
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3325.9 words/s - loss: 0.1454
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3595.5 words/s - loss: 0.1495
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 3307.9 words/s - loss: 0.1803
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3848.3 words/s - loss: 0.1444
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3495.8 words/s - loss: 0.1506
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3839.5 words/s - loss: 0.1633
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3456.9 words/s - loss: 0.1520
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3307.5 words/s - loss: 0.1677
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2947.5 words/s - loss: 0.1337
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3578.6 words/s - loss: 0.1981
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3638.7 words/s - loss: 0.1433
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3547.9 words/s - loss: 0.1301
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3659.9 words/s - loss: 0.1794
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3343.0 words/s - loss: 0.1769
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3480.1 words/s - loss: 0.1646
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3458.1 words/s - loss: 0.1710
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3066.8 words/s - loss: 0.1222
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3385.5 words/s - loss: 0.1552
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3555.3 words/s - loss: 0.1512
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.371 w/ 88 queries
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.353 w/ 88 queries
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3941.6 words/s - loss: 0.1576
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3721.6 words/s - loss: 0.1520
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3727.8 words/s - loss: 0.1535
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3657.7 words/s - loss: 0.1804
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3602.2 words/s - loss: 0.1633
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3509.1 words/s - loss: 0.1480
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3507.9 words/s - loss: 0.1463
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3490.2 words/s - loss: 0.1384
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3257.9 words/s - loss: 0.1292
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3298.8 words/s - loss: 0.1175
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.370 w/ 88 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.351 w/ 88 queries
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_deen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3955.0 words/s - loss: 0.1091
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3829.8 words/s - loss: 0.1586
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3678.8 words/s - loss: 0.1238
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3543.1 words/s - loss: 0.1316
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3527.4 words/s - loss: 0.1474
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3556.5 words/s - loss: 0.1419
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3379.7 words/s - loss: 0.1295
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3318.8 words/s - loss: 0.1296
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3298.6 words/s - loss: 0.1579
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2844.3 words/s - loss: 0.1569
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.367 w/ 88 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_deen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3680.3 words/s - loss: 0.1286
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3575.9 words/s - loss: 0.1351
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3629.2 words/s - loss: 0.1526
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3491.3 words/s - loss: 0.1209
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3555.7 words/s - loss: 0.1332
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3464.7 words/s - loss: 0.1336
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3377.9 words/s - loss: 0.1544
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3390.0 words/s - loss: 0.1423
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3390.8 words/s - loss: 0.1252
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3230.9 words/s - loss: 0.1546
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.364 w/ 88 queries
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.355 w/ 88 queries
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3903.4 words/s - loss: 0.1503
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3642.6 words/s - loss: 0.1356
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3520.5 words/s - loss: 0.1408
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 3514.3 words/s - loss: 0.1551
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3412.9 words/s - loss: 0.1080
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3456.1 words/s - loss: 0.1752
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3256.6 words/s - loss: 0.1426
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3229.5 words/s - loss: 0.1276
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3205.5 words/s - loss: 0.1273
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3138.9 words/s - loss: 0.1328
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.375 w/ 88 queries
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.346 w/ 88 queries
INFO:__main__:removing file tmp/mbert_deen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_9.txt
INFO:__main__:[0.37119627487598145, 0.37002700610527856, 0.3794631729249504, 0.363931491040608, 0.37521627986274914]
INFO:__main__:[0.35330612678712414, 0.3514634855344459, 0.3666334823288951, 0.35519857114248404, 0.3464041505079276]
INFO:__main__:0.3794631729249504
INFO:__main__:0.3666334823288951
INFO:__main__:best MAP: 0.373
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=4, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='en')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 657661.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 668393.68it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 722657.48it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 700381.39it/s]
100%|██████████| 5000/5000 [00:00<00:00, 685164.66it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 652322.62it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 684248.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 695457.47it/s]100%|██████████| 5000/5000 [00:00<00:00, 689716.50it/s]

INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 606586.64it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 180.00it/s]100%|██████████| 176/176 [00:00<00:00, 1287.59it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16366.56it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:00, 171.01it/s]100%|██████████| 176/176 [00:00<00:00, 1228.27it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 169.52it/s]100%|██████████| 176/176 [00:00<00:00, 1220.38it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16616.71it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16511.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:01, 130.08it/s]100%|██████████| 176/176 [00:00<00:00, 948.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 16590.20it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 115.68it/s] 13%|█▎        | 23/176 [00:00<00:01, 147.12it/s] 13%|█▎        | 23/176 [00:00<00:01, 146.82it/s] 13%|█▎        | 23/176 [00:00<00:01, 149.40it/s]100%|██████████| 176/176 [00:00<00:00, 847.82it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
100%|██████████| 176/176 [00:00<00:00, 1066.07it/s]
  0%|          | 0/176 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 35910
100%|██████████| 176/176 [00:00<00:00, 1062.94it/s]
  0%|          | 0/176 [00:00<?, ?it/s]INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 1081.64it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s] 13%|█▎        | 23/176 [00:00<00:00, 156.02it/s]100%|██████████| 176/176 [00:00<00:00, 16142.87it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 16040.10it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
100%|██████████| 176/176 [00:00<00:00, 15711.01it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 16327.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 176/176 [00:00<00:00, 1125.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15614.64it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
 13%|█▎        | 23/176 [00:00<00:01, 151.34it/s]100%|██████████| 176/176 [00:00<00:00, 1093.50it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 35910
  0%|          | 0/176 [00:00<?, ?it/s]100%|██████████| 176/176 [00:00<00:00, 15989.72it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 38456
INFO:__main__:f1 has 88 queries ...
INFO:__main__:f2 has 88 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 3941.6 words/s - loss: 0.2921
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 3866.7 words/s - loss: 0.2925
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 3723.9 words/s - loss: 0.3115
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 3479.6 words/s - loss: 0.3160
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 3437.8 words/s - loss: 0.2920
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 3357.2 words/s - loss: 0.2779
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 3321.1 words/s - loss: 0.3195
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 3284.4 words/s - loss: 0.2908
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 3100.3 words/s - loss: 0.3106
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 3037.0 words/s - loss: 0.2807
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3658.3 words/s - loss: 0.1950
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 3477.1 words/s - loss: 0.1801
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 3677.7 words/s - loss: 0.2055
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 3717.1 words/s - loss: 0.2011
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 3474.4 words/s - loss: 0.2155
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 3560.6 words/s - loss: 0.1891
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3274.4 words/s - loss: 0.1821
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3088.6 words/s - loss: 0.1835
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 3307.0 words/s - loss: 0.2200
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 3169.2 words/s - loss: 0.2009
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 3625.7 words/s - loss: 0.1757
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 3547.7 words/s - loss: 0.1993
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3396.0 words/s - loss: 0.1494
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 3664.7 words/s - loss: 0.1744
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 3149.2 words/s - loss: 0.1738
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 3741.7 words/s - loss: 0.2013
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3380.3 words/s - loss: 0.1772
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 3496.2 words/s - loss: 0.1725
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 3249.1 words/s - loss: 0.1579
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 3205.8 words/s - loss: 0.1863
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 3720.9 words/s - loss: 0.1950
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 3187.2 words/s - loss: 0.1411
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 3864.8 words/s - loss: 0.1649
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 3370.1 words/s - loss: 0.1797
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 3511.1 words/s - loss: 0.1373
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 3651.7 words/s - loss: 0.1592
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 3593.3 words/s - loss: 0.1725
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 3697.8 words/s - loss: 0.1632
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 3187.8 words/s - loss: 0.1499
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3539.7 words/s - loss: 0.1500
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 3486.7 words/s - loss: 0.1856
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 3336.5 words/s - loss: 0.1499
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 3789.2 words/s - loss: 0.1526
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 3306.6 words/s - loss: 0.1496
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 3270.8 words/s - loss: 0.1309
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 3362.5 words/s - loss: 0.1971
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 3971.5 words/s - loss: 0.1884
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2989.7 words/s - loss: 0.1566
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 3596.2 words/s - loss: 0.1582
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 3257.3 words/s - loss: 0.1639
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3526.0 words/s - loss: 0.1545
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 3790.5 words/s - loss: 0.1739
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 3311.3 words/s - loss: 0.1448
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3078.7 words/s - loss: 0.1673
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 3388.4 words/s - loss: 0.1278
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 3379.9 words/s - loss: 0.1572
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 3455.6 words/s - loss: 0.1412
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 3259.3 words/s - loss: 0.1513
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 3591.0 words/s - loss: 0.1623
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 3305.0 words/s - loss: 0.1706
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.376 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.359 w/ 88 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 3716.7 words/s - loss: 0.1486
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 3660.0 words/s - loss: 0.1609
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 3654.5 words/s - loss: 0.1483
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 3726.3 words/s - loss: 0.1346
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3607.8 words/s - loss: 0.1506
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 3553.8 words/s - loss: 0.1277
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 3506.4 words/s - loss: 0.1432
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 3286.8 words/s - loss: 0.1201
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 3248.1 words/s - loss: 0.1299
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3089.6 words/s - loss: 0.1220
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.386 w/ 88 queries
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.358 w/ 88 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_deen_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 3892.5 words/s - loss: 0.1406
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 3818.1 words/s - loss: 0.1247
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 3666.6 words/s - loss: 0.1240
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 3616.2 words/s - loss: 0.1238
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 3540.4 words/s - loss: 0.1248
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 3388.4 words/s - loss: 0.1337
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 3290.3 words/s - loss: 0.1432
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3266.1 words/s - loss: 0.1486
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 3176.5 words/s - loss: 0.1367
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3068.5 words/s - loss: 0.1332
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.383 w/ 88 queries
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.354 w/ 88 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_deen_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 3704.7 words/s - loss: 0.1167
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 3587.5 words/s - loss: 0.1385
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 3514.7 words/s - loss: 0.1411
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 3520.7 words/s - loss: 0.1220
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 3492.8 words/s - loss: 0.1417
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 3508.2 words/s - loss: 0.1394
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 3353.9 words/s - loss: 0.1108
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 3330.1 words/s - loss: 0.1283
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3359.0 words/s - loss: 0.1161
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 3052.0 words/s - loss: 0.1503
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.366 w/ 88 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.347 w/ 88 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_deen_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 3987.1 words/s - loss: 0.1483
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 3620.5 words/s - loss: 0.1391
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 3602.4 words/s - loss: 0.1037
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 3532.2 words/s - loss: 0.0857
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 3375.6 words/s - loss: 0.1292
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3451.5 words/s - loss: 0.1220
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 3358.4 words/s - loss: 0.1297
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 3237.3 words/s - loss: 0.1325
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3262.2 words/s - loss: 0.1398
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2982.0 words/s - loss: 0.1307
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.379 w/ 88 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_deen_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f1_8_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:f2 set during evaluation: 38460/38456
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.362 w/ 88 queries
INFO:__main__:removing file tmp/mbert_deen_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_5_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_deen_f2_3_9.txt
INFO:__main__:[0.3763210830661027, 0.3859928361762281, 0.3825560192985568, 0.36588377654473453, 0.378959895131694]
INFO:__main__:[0.3588426634514022, 0.35787670027670776, 0.3543977272690565, 0.34654897431097137, 0.36151189386769494]
INFO:__main__:0.378959895131694
INFO:__main__:0.35787670027670776
INFO:__main__:best MAP: 0.368

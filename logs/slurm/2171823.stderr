INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 766867.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 803506.51it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
 14%|█▎        | 25/185 [00:00<00:00, 170.01it/s]100%|██████████| 185/185 [00:00<00:00, 1207.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25910.65it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 369.70it/s]100%|██████████| 185/185 [00:00<00:00, 1748.70it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25851.95it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 876955.76it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 883048.55it/s]
100%|██████████| 5000/5000 [00:00<00:00, 872976.73it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 850944.21it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 831906.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 810618.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 216.64it/s]100%|██████████| 185/185 [00:00<00:00, 1524.72it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25918.44it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 14%|█▎        | 25/185 [00:00<00:00, 223.31it/s] 14%|█▎        | 25/185 [00:00<00:00, 219.06it/s]100%|██████████| 185/185 [00:00<00:00, 1561.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1537.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22505.55it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 24124.68it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 177.91it/s]100%|██████████| 185/185 [00:00<00:00, 1263.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25684.23it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 164.07it/s]100%|██████████| 185/185 [00:00<00:00, 1169.18it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25415.03it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 14%|█▎        | 25/185 [00:00<00:00, 163.13it/s]100%|██████████| 185/185 [00:00<00:00, 1159.80it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25290.77it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3605.8 words/s - loss: 0.4464
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 3496.7 words/s - loss: 0.4528
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3202.6 words/s - loss: 0.4539
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 3199.5 words/s - loss: 0.4381
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3072.6 words/s - loss: 0.4620
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3032.3 words/s - loss: 0.4705
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 2834.6 words/s - loss: 0.4447
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 2641.7 words/s - loss: 0.4562
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3332.9 words/s - loss: 0.3142
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3420.7 words/s - loss: 0.3120
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 2861.5 words/s - loss: 0.3546
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 2756.6 words/s - loss: 0.3432
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 2802.2 words/s - loss: 0.3160
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 2822.3 words/s - loss: 0.3349
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 2677.5 words/s - loss: 0.3281
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 2696.1 words/s - loss: 0.3022
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 3027.6 words/s - loss: 0.2957
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3072.6 words/s - loss: 0.2817
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 2985.0 words/s - loss: 0.2919
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 3295.7 words/s - loss: 0.2833
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3352.3 words/s - loss: 0.3045
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3710.6 words/s - loss: 0.3145
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3380.1 words/s - loss: 0.2955
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 2628.7 words/s - loss: 0.2787
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3411.2 words/s - loss: 0.2570
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 3028.7 words/s - loss: 0.3107
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3290.8 words/s - loss: 0.2565
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3449.2 words/s - loss: 0.2781
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 2714.2 words/s - loss: 0.2611
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 3059.6 words/s - loss: 0.2673
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3321.5 words/s - loss: 0.2849
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 2504.2 words/s - loss: 0.2636
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3733.1 words/s - loss: 0.2841
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3744.4 words/s - loss: 0.2602
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 3516.1 words/s - loss: 0.2636
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3067.6 words/s - loss: 0.2706
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3015.2 words/s - loss: 0.2354
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 2701.5 words/s - loss: 0.2623
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 2989.7 words/s - loss: 0.2451
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 2939.8 words/s - loss: 0.2850
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 2803.5 words/s - loss: 0.2537
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 3282.4 words/s - loss: 0.2580
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3363.2 words/s - loss: 0.2329
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3288.5 words/s - loss: 0.2373
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3567.4 words/s - loss: 0.2529
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 3177.4 words/s - loss: 0.2233
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 2886.6 words/s - loss: 0.2293
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 2655.0 words/s - loss: 0.2628
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.324 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.261 w/ 92 queries
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3504.4 words/s - loss: 0.2738
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3367.1 words/s - loss: 0.2439
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3297.0 words/s - loss: 0.2406
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3160.6 words/s - loss: 0.2515
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3085.0 words/s - loss: 0.2465
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 2979.6 words/s - loss: 0.2428
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3068.1 words/s - loss: 0.2532
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 2997.0 words/s - loss: 0.2171
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.296 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.262 w/ 92 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3552.7 words/s - loss: 0.2243
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3376.9 words/s - loss: 0.2395
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 3260.0 words/s - loss: 0.2292
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3190.9 words/s - loss: 0.2310
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3207.7 words/s - loss: 0.2452
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 2974.9 words/s - loss: 0.2277
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 2703.7 words/s - loss: 0.2531
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 2712.5 words/s - loss: 0.2502
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.315 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.273 w/ 92 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3359.2 words/s - loss: 0.2540
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3299.8 words/s - loss: 0.2173
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3165.7 words/s - loss: 0.2567
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 2965.0 words/s - loss: 0.2028
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 2800.6 words/s - loss: 0.2049
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 2931.6 words/s - loss: 0.2188
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 2772.6 words/s - loss: 0.2341
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 2781.7 words/s - loss: 0.2192
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.320 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.285 w/ 92 queries
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3517.8 words/s - loss: 0.2297
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3305.9 words/s - loss: 0.2624
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3169.2 words/s - loss: 0.2199
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3114.9 words/s - loss: 0.2316
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 3046.8 words/s - loss: 0.2202
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 2937.8 words/s - loss: 0.2190
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 2952.6 words/s - loss: 0.2246
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 2908.4 words/s - loss: 0.2193
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.337 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.305 w/ 92 queries
INFO:__main__:removing file tmp/mbert_esfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_9.txt
INFO:__main__:[0.32406170201030354, 0.295666981428536, 0.3153458464282099, 0.32005349649336934, 0.3369560544143281]
INFO:__main__:[0.2612539415303264, 0.26219631773647034, 0.27279943769417264, 0.2853027268631792, 0.30518125843548466]
INFO:__main__:0.3369560544143281
INFO:__main__:0.30518125843548466
INFO:__main__:best MAP: 0.321
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 809024.00it/s]
100%|██████████| 5000/5000 [00:00<00:00, 801970.17it/s]
100%|██████████| 5000/5000 [00:00<00:00, 843890.39it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 810336.94it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 869466.00it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 856784.74it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 876589.20it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 638033.41it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 286.48it/s]100%|██████████| 185/185 [00:00<00:00, 1369.60it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25310.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 313.39it/s] 20%|██        | 37/185 [00:00<00:00, 313.34it/s]100%|██████████| 185/185 [00:00<00:00, 1493.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
100%|██████████| 185/185 [00:00<00:00, 1491.94it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25460.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 25080.69it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 183.62it/s]100%|██████████| 185/185 [00:00<00:00, 891.17it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 177.25it/s] 20%|██        | 37/185 [00:00<00:00, 189.23it/s]100%|██████████| 185/185 [00:00<00:00, 862.30it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 330.67it/s]100%|██████████| 185/185 [00:00<00:00, 24691.22it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 918.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 1567.95it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25084.74it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 25251.27it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 24788.24it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 287.02it/s]100%|██████████| 185/185 [00:00<00:00, 1373.63it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25691.03it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3479.9 words/s - loss: 0.3552
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3293.9 words/s - loss: 0.3659
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3079.0 words/s - loss: 0.3642
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 3089.2 words/s - loss: 0.3489
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3022.1 words/s - loss: 0.3758
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 2962.7 words/s - loss: 0.3604
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 2803.1 words/s - loss: 0.3290
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 2740.9 words/s - loss: 0.3554
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3421.8 words/s - loss: 0.2722
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 3497.1 words/s - loss: 0.2585
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 2907.5 words/s - loss: 0.2446
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3089.4 words/s - loss: 0.2653
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3222.2 words/s - loss: 0.2631
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 2665.2 words/s - loss: 0.2933
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 2658.2 words/s - loss: 0.2950
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 2611.6 words/s - loss: 0.2290
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 2925.4 words/s - loss: 0.2540
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 3022.0 words/s - loss: 0.2572
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 3293.9 words/s - loss: 0.2593
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 2548.6 words/s - loss: 0.2273
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 2798.1 words/s - loss: 0.2415
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 2794.9 words/s - loss: 0.2588
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 3250.2 words/s - loss: 0.2492
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 2583.7 words/s - loss: 0.2425
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3546.5 words/s - loss: 0.2167
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 2808.0 words/s - loss: 0.2626
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 3035.4 words/s - loss: 0.2429
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 2720.5 words/s - loss: 0.2403
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 2694.4 words/s - loss: 0.2080
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 2927.7 words/s - loss: 0.2507
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 2852.9 words/s - loss: 0.2330
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 2730.3 words/s - loss: 0.2346
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 3196.9 words/s - loss: 0.2294
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3730.9 words/s - loss: 0.2234
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 2844.6 words/s - loss: 0.2137
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 2850.3 words/s - loss: 0.2200
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3492.9 words/s - loss: 0.2363
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 3194.2 words/s - loss: 0.2110
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 3003.3 words/s - loss: 0.2056
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 2747.9 words/s - loss: 0.2188
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 2872.7 words/s - loss: 0.2158
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3199.3 words/s - loss: 0.1944
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3352.8 words/s - loss: 0.2201
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 2808.7 words/s - loss: 0.2343
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 3044.8 words/s - loss: 0.1972
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 3076.3 words/s - loss: 0.2020
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 2902.5 words/s - loss: 0.2063
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 2872.6 words/s - loss: 0.2283
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.384 w/ 92 queries
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3339.4 words/s - loss: 0.2203
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3126.8 words/s - loss: 0.1927
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 3101.5 words/s - loss: 0.1909
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3082.0 words/s - loss: 0.2081
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3002.6 words/s - loss: 0.2409
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3032.3 words/s - loss: 0.1906
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 2968.0 words/s - loss: 0.2213
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 2893.6 words/s - loss: 0.2166
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.424 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 3284.2 words/s - loss: 0.2110
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3230.4 words/s - loss: 0.2320
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 3224.4 words/s - loss: 0.2122
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3086.7 words/s - loss: 0.2052
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 2896.1 words/s - loss: 0.2008
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 2838.4 words/s - loss: 0.2200
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 2808.2 words/s - loss: 0.1671
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 2497.2 words/s - loss: 0.2065
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3439.4 words/s - loss: 0.2297
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3159.6 words/s - loss: 0.1792
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3135.7 words/s - loss: 0.2110
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3062.7 words/s - loss: 0.2021
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3012.0 words/s - loss: 0.2091
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 2929.9 words/s - loss: 0.2144
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 2796.7 words/s - loss: 0.1937
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 2727.7 words/s - loss: 0.1841
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.428 w/ 93 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.371 w/ 92 queries
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3552.7 words/s - loss: 0.1947
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 3153.9 words/s - loss: 0.1891
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 3124.5 words/s - loss: 0.1940
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3124.7 words/s - loss: 0.1880
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 3007.3 words/s - loss: 0.1720
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 2961.7 words/s - loss: 0.1841
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 2758.6 words/s - loss: 0.2219
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 2797.6 words/s - loss: 0.2116
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.432 w/ 93 queries
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.379 w/ 92 queries
INFO:__main__:removing file tmp/mbert_esfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_9.txt
INFO:__main__:[0.4292514741751813, 0.4241812145372177, 0.4440085437797246, 0.42808992124045325, 0.4316857729332166]
INFO:__main__:[0.38401632181258505, 0.37110062477332034, 0.37480727337375125, 0.3706056883796855, 0.37892179672464305]
INFO:__main__:0.4292514741751813
INFO:__main__:0.37480727337375125
INFO:__main__:best MAP: 0.402
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 863843.14it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 815821.99it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 797972.68it/s]
100%|██████████| 5000/5000 [00:00<00:00, 795913.32it/s]
100%|██████████| 5000/5000 [00:00<00:00, 781615.30it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 871561.80it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 806038.90it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 806131.85it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 280.94it/s]100%|██████████| 185/185 [00:00<00:00, 1341.42it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24935.61it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
 20%|██        | 37/185 [00:00<00:00, 253.24it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1214.51it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 297.19it/s]100%|██████████| 185/185 [00:00<00:00, 23441.07it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 1419.46it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24940.42it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 293.15it/s]100%|██████████| 185/185 [00:00<00:00, 1400.12it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 169.97it/s] 20%|██        | 37/185 [00:00<00:00, 169.01it/s]100%|██████████| 185/185 [00:00<00:00, 25238.13it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
 20%|██        | 37/185 [00:00<00:00, 168.31it/s]INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 827.78it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 822.71it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 819.28it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24659.05it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
100%|██████████| 185/185 [00:00<00:00, 24520.34it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 185/185 [00:00<00:00, 24629.30it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 181.62it/s]100%|██████████| 185/185 [00:00<00:00, 883.39it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 25607.94it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 3425.1 words/s - loss: 0.3984
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3326.8 words/s - loss: 0.3441
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 3297.3 words/s - loss: 0.3828
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 3143.1 words/s - loss: 0.3502
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 3131.8 words/s - loss: 0.3531
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 3041.3 words/s - loss: 0.3498
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 2822.5 words/s - loss: 0.3658
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 2850.7 words/s - loss: 0.3500
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 3392.3 words/s - loss: 0.2475
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 3413.7 words/s - loss: 0.2579
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 2966.0 words/s - loss: 0.2715
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 2935.0 words/s - loss: 0.2735
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 3368.9 words/s - loss: 0.2421
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3007.3 words/s - loss: 0.2657
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 2971.0 words/s - loss: 0.2700
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 2681.1 words/s - loss: 0.2493
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 3200.0 words/s - loss: 0.2377
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 2835.0 words/s - loss: 0.2142
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 2649.5 words/s - loss: 0.2331
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 2899.4 words/s - loss: 0.2440
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 2630.9 words/s - loss: 0.2504
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 2625.7 words/s - loss: 0.2333
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 2873.1 words/s - loss: 0.2430
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 2742.2 words/s - loss: 0.2461
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 3319.7 words/s - loss: 0.2196
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 3993.5 words/s - loss: 0.2108
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 2916.0 words/s - loss: 0.2568
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 3043.3 words/s - loss: 0.2459
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 3114.9 words/s - loss: 0.2334
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 2684.7 words/s - loss: 0.2058
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 2862.3 words/s - loss: 0.2235
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 2693.4 words/s - loss: 0.2407
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3632.9 words/s - loss: 0.2199
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 3359.6 words/s - loss: 0.2260
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 3064.3 words/s - loss: 0.2329
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 2629.5 words/s - loss: 0.2306
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 2846.2 words/s - loss: 0.1929
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 2752.6 words/s - loss: 0.2087
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 2854.6 words/s - loss: 0.2048
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 3273.7 words/s - loss: 0.2238
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 2431.0 words/s - loss: 0.2016
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 3168.7 words/s - loss: 0.1973
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 2979.5 words/s - loss: 0.2226
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 3317.9 words/s - loss: 0.2265
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 2806.6 words/s - loss: 0.2165
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 3334.6 words/s - loss: 0.2167
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 2939.0 words/s - loss: 0.2084
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3096.8 words/s - loss: 0.2060
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.438 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 3551.6 words/s - loss: 0.2076
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3447.7 words/s - loss: 0.2303
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 3448.2 words/s - loss: 0.2126
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 3282.7 words/s - loss: 0.2256
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 3088.9 words/s - loss: 0.2053
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 3046.7 words/s - loss: 0.1872
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 2984.6 words/s - loss: 0.1952
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 2899.1 words/s - loss: 0.2215
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.445 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.379 w/ 92 queries
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3330.0 words/s - loss: 0.2006
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3253.0 words/s - loss: 0.2127
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 3030.6 words/s - loss: 0.2233
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 2896.2 words/s - loss: 0.1952
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 2882.4 words/s - loss: 0.1728
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 2754.6 words/s - loss: 0.2157
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 2710.3 words/s - loss: 0.2051
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 2584.7 words/s - loss: 0.2182
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.458 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.387 w/ 92 queries
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 3365.1 words/s - loss: 0.2078
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3242.9 words/s - loss: 0.1782
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3149.9 words/s - loss: 0.2065
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 2869.4 words/s - loss: 0.1978
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 2671.0 words/s - loss: 0.1771
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 2714.8 words/s - loss: 0.2163
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 2638.4 words/s - loss: 0.2020
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 2485.7 words/s - loss: 0.2205
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.441 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.378 w/ 92 queries
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 3123.5 words/s - loss: 0.1934
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3188.6 words/s - loss: 0.1887
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 3105.4 words/s - loss: 0.1855
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 3085.4 words/s - loss: 0.1947
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 2884.2 words/s - loss: 0.1957
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 2946.1 words/s - loss: 0.1877
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 2805.2 words/s - loss: 0.2342
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 2565.3 words/s - loss: 0.1691
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:f1 set during evaluation: 26368/26363
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.444 w/ 93 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.380 w/ 92 queries
INFO:__main__:removing file tmp/mbert_esfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_9.txt
INFO:__main__:[0.4382875733854724, 0.44452247141381923, 0.45835925041446696, 0.44107082095158967, 0.44386226164201575]
INFO:__main__:[0.37858534339743916, 0.379330875945561, 0.38694796804845133, 0.37828012603307676, 0.38041057879150786]
INFO:__main__:0.45835925041446696
INFO:__main__:0.38694796804845133
INFO:__main__:best MAP: 0.423

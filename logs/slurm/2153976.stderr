INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=9, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=8, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:__main__:Namespace(apex_level='O2', batch_size=16, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=4, num_neg=1, projector_lr=2e-05, seed=611, source_lang='es', target_lang='fr')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 677221.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 551678.86it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 651512.01it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 576774.48it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 561621.81it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
 20%|██        | 37/185 [00:00<00:00, 205.18it/s]100%|██████████| 185/185 [00:00<00:00, 986.85it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 20535.83it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 201.06it/s]100%|██████████| 185/185 [00:00<00:00, 964.90it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
100%|██████████| 185/185 [00:00<00:00, 20011.51it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 250.46it/s]100%|██████████| 185/185 [00:00<00:00, 1198.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22501.63it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 277.52it/s]100%|██████████| 185/185 [00:00<00:00, 1324.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 24056.62it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 284.02it/s]100%|██████████| 185/185 [00:00<00:00, 1354.68it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23927.54it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 760030.44it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 625996.84it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 758299.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 659606.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 724855.52it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/185 [00:00<?, ?it/s] 20%|██        | 37/185 [00:00<00:00, 241.67it/s]100%|██████████| 185/185 [00:00<00:00, 1159.55it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23582.12it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 247.99it/s]100%|██████████| 185/185 [00:00<00:00, 1188.01it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23298.19it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 215.57it/s]100%|██████████| 185/185 [00:00<00:00, 1038.06it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 23618.01it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 300.28it/s]100%|██████████| 185/185 [00:00<00:00, 1424.97it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22430.08it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
 20%|██        | 37/185 [00:00<00:00, 256.32it/s]100%|██████████| 185/185 [00:00<00:00, 1225.48it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 26363
  0%|          | 0/185 [00:00<?, ?it/s]100%|██████████| 185/185 [00:00<00:00, 22408.71it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 27153
INFO:__main__:f1 has 93 queries ...
INFO:__main__:f2 has 92 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 8-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[8]: training epoch 0 ...
INFO:__main__:process[9]: training epoch 0 ...
INFO:__main__:process[9] - epoch 0 - train iter 500 - 2990.4 words/s - loss: 0.4072
INFO:__main__:process[9]: training epoch 1 ...
INFO:__main__:process[8] - epoch 0 - train iter 500 - 2995.1 words/s - loss: 0.3694
INFO:__main__:process[8]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 500 - 2834.0 words/s - loss: 0.3920
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 500 - 2881.9 words/s - loss: 0.3792
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 500 - 2735.3 words/s - loss: 0.3779
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 500 - 2774.4 words/s - loss: 0.3529
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 500 - 2692.9 words/s - loss: 0.3774
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 500 - 2598.1 words/s - loss: 0.3832
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 500 - 2322.2 words/s - loss: 0.3546
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 500 - 2153.5 words/s - loss: 0.3815
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[0] - epoch 1 - train iter 500 - 3202.1 words/s - loss: 0.2863
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 500 - 3341.2 words/s - loss: 0.2512
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[8] - epoch 1 - train iter 500 - 3008.2 words/s - loss: 0.2606
INFO:__main__:process[8]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 500 - 2959.9 words/s - loss: 0.2575
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[9] - epoch 1 - train iter 500 - 2697.1 words/s - loss: 0.2634
INFO:__main__:process[9]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 500 - 2608.7 words/s - loss: 0.2793
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 500 - 2607.6 words/s - loss: 0.2665
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 500 - 2845.8 words/s - loss: 0.2698
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 500 - 2351.9 words/s - loss: 0.2716
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 500 - 2972.1 words/s - loss: 0.2515
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 2 - train iter 500 - 3283.6 words/s - loss: 0.2414
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 500 - 2898.0 words/s - loss: 0.2681
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 500 - 2778.5 words/s - loss: 0.2441
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[8] - epoch 2 - train iter 500 - 2492.3 words/s - loss: 0.2332
INFO:__main__:process[8]: training epoch 3 ...
INFO:__main__:process[9] - epoch 2 - train iter 500 - 2431.0 words/s - loss: 0.2285
INFO:__main__:process[9]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 500 - 3026.5 words/s - loss: 0.1982
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 500 - 2671.0 words/s - loss: 0.2106
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 500 - 2478.7 words/s - loss: 0.2370
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 500 - 2635.7 words/s - loss: 0.2602
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 500 - 2520.2 words/s - loss: 0.2170
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[7] - epoch 3 - train iter 500 - 2884.7 words/s - loss: 0.2256
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 500 - 2714.6 words/s - loss: 0.2140
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 500 - 2643.9 words/s - loss: 0.2289
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[8] - epoch 3 - train iter 500 - 2738.3 words/s - loss: 0.2411
INFO:__main__:process[8]: training epoch 4 ...
INFO:__main__:process[9] - epoch 3 - train iter 500 - 2763.9 words/s - loss: 0.2542
INFO:__main__:process[9]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 500 - 3063.9 words/s - loss: 0.2421
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 500 - 2862.0 words/s - loss: 0.1989
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 500 - 2919.8 words/s - loss: 0.2309
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 500 - 2699.2 words/s - loss: 0.2223
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 500 - 2267.4 words/s - loss: 0.2647
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[0] - epoch 4 - train iter 500 - 2854.8 words/s - loss: 0.2185
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 500 - 2671.7 words/s - loss: 0.2229
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 500 - 2434.7 words/s - loss: 0.2200
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 500 - 2856.2 words/s - loss: 0.1976
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[8] - epoch 4 - train iter 500 - 2493.6 words/s - loss: 0.2320
INFO:__main__:process[8]: training epoch 5 ...
INFO:__main__:process[4] - epoch 4 - train iter 500 - 2788.8 words/s - loss: 0.1836
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[9] - epoch 4 - train iter 500 - 2635.7 words/s - loss: 0.2130
INFO:__main__:process[9]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 500 - 2398.2 words/s - loss: 0.1826
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 500 - 2744.2 words/s - loss: 0.2336
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 500 - 2738.5 words/s - loss: 0.2354
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 5 - train iter 500 - 3332.6 words/s - loss: 0.1944
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 500 - 2906.0 words/s - loss: 0.2393
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[8] - epoch 5 - train iter 500 - 3306.1 words/s - loss: 0.1808
INFO:__main__:process[8]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 500 - 2567.3 words/s - loss: 0.2263
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 500 - 2725.5 words/s - loss: 0.2155
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[9] - epoch 5 - train iter 500 - 2840.7 words/s - loss: 0.1979
INFO:__main__:process[9]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 500 - 2447.8 words/s - loss: 0.2108
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 500 - 2760.0 words/s - loss: 0.2018
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 500 - 2508.1 words/s - loss: 0.2183
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 500 - 2884.1 words/s - loss: 0.1974
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.447 w/ 93 queries
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[9]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[8]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 5 - mAP: 0.388 w/ 92 queries
INFO:__main__:process[8]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[9]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_5.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[7] - epoch 6 - train iter 500 - 3286.7 words/s - loss: 0.1866
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[8] - epoch 6 - train iter 500 - 3112.5 words/s - loss: 0.2094
INFO:__main__:process[8]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 500 - 2943.9 words/s - loss: 0.2337
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[5] - epoch 6 - train iter 500 - 2788.4 words/s - loss: 0.2190
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 500 - 2696.0 words/s - loss: 0.1862
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 500 - 2672.2 words/s - loss: 0.1999
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 500 - 2600.1 words/s - loss: 0.2244
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 500 - 2573.8 words/s - loss: 0.1935
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[9] - epoch 6 - train iter 500 - 2453.9 words/s - loss: 0.2068
INFO:__main__:process[9]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 500 - 2342.3 words/s - loss: 0.1940
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.453 w/ 93 queries
INFO:__main__:process[8]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[9]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.394 w/ 92 queries
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[8]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[9]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 500 - 3590.1 words/s - loss: 0.1928
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[9] - epoch 7 - train iter 500 - 3214.0 words/s - loss: 0.2336
INFO:__main__:process[9]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 500 - 2983.1 words/s - loss: 0.2023
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 500 - 2845.7 words/s - loss: 0.2119
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[8] - epoch 7 - train iter 500 - 2854.9 words/s - loss: 0.1984
INFO:__main__:process[8]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 500 - 2824.4 words/s - loss: 0.1901
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 500 - 2790.7 words/s - loss: 0.2139
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 500 - 2672.1 words/s - loss: 0.1952
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 500 - 2344.0 words/s - loss: 0.1889
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 500 - 2285.3 words/s - loss: 0.1600
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[9] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.454 w/ 93 queries
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[8]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[9]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.391 w/ 92 queries
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[8]: training epoch 8 ...
INFO:__main__:process[9]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_7.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[0] - epoch 8 - train iter 500 - 3036.5 words/s - loss: 0.1905
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 500 - 2907.2 words/s - loss: 0.1948
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 500 - 2929.1 words/s - loss: 0.1912
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[8] - epoch 8 - train iter 500 - 2772.2 words/s - loss: 0.2067
INFO:__main__:process[8]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 500 - 2749.7 words/s - loss: 0.2075
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 500 - 2706.2 words/s - loss: 0.2032
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 500 - 2490.3 words/s - loss: 0.2166
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 500 - 2407.6 words/s - loss: 0.1957
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 500 - 2280.1 words/s - loss: 0.1888
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[9] - epoch 8 - train iter 500 - 2214.8 words/s - loss: 0.1856
INFO:__main__:process[9]: evaluating epoch 8 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.429 w/ 93 queries
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[8]: evaluating epoch 8 on f2 ...
INFO:__main__:process[9]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[9] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[8] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.375 w/ 92 queries
INFO:__main__:process[8]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[9]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_esfr_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[4] - epoch 9 - train iter 500 - 3348.3 words/s - loss: 0.2083
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[8] - epoch 9 - train iter 500 - 3427.6 words/s - loss: 0.2123
INFO:__main__:process[8]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 500 - 2916.3 words/s - loss: 0.2035
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 500 - 2947.9 words/s - loss: 0.2178
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 500 - 2862.9 words/s - loss: 0.1996
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 500 - 2669.4 words/s - loss: 0.1777
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[9] - epoch 9 - train iter 500 - 2685.7 words/s - loss: 0.1815
INFO:__main__:process[9]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 500 - 2715.1 words/s - loss: 0.1599
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 500 - 2651.7 words/s - loss: 0.2000
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 500 - 2600.1 words/s - loss: 0.1725
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[9] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:f1 set during evaluation: 26370/26363
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[8] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.442 w/ 93 queries
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[8]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[9]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_esfr_f1_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[9] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:f2 set during evaluation: 27160/27153
INFO:__main__:process[8] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.389 w/ 92 queries
INFO:__main__:removing file tmp/mbert_esfr_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_8_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_9_9.txt
INFO:__main__:removing file tmp/mbert_esfr_f2_5_9.txt
INFO:__main__:[0.44650209692966675, 0.45344359401165596, 0.4544479544404535, 0.4289775901174073, 0.4416674900096404]
INFO:__main__:[0.38783603100655223, 0.39386957366091796, 0.3914937761968991, 0.374575801224262, 0.38942579869245764]
INFO:__main__:0.45344359401165596
INFO:__main__:0.3914937761968991
INFO:__main__:best MAP: 0.423

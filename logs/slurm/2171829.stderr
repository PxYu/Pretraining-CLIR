INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='bert-base-multilingual-uncased', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /mnt/home/puxuan/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /mnt/home/puxuan/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /mnt/home/puxuan/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 874724.50it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 796003.95it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 832831.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 858714.27it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
 15%|█▍        | 23/156 [00:00<00:00, 204.48it/s]100%|██████████| 156/156 [00:00<00:00, 1308.92it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19612.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 841182.46it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 817188.95it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 844128.16it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForXLRetrieval: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForXLRetrieval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForXLRetrieval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForXLRetrieval were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['mlm_cls.predictions.bias', 'mlm_cls.predictions.transform.dense.weight', 'mlm_cls.predictions.transform.dense.bias', 'mlm_cls.predictions.transform.LayerNorm.weight', 'mlm_cls.predictions.transform.LayerNorm.bias', 'mlm_cls.predictions.decoder.weight', 'mlm_cls.predictions.decoder.bias', 'seqcls_classifier.weight', 'seqcls_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 189.54it/s]100%|██████████| 156/156 [00:00<00:00, 1220.05it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20130.18it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 736075.25it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 150.91it/s]100%|██████████| 156/156 [00:00<00:00, 981.53it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20335.39it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 187.78it/s]100%|██████████| 156/156 [00:00<00:00, 1207.96it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20002.79it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 150.23it/s]100%|██████████| 156/156 [00:00<00:00, 978.24it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20637.48it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 146.41it/s]100%|██████████| 156/156 [00:00<00:00, 952.75it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19990.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 15%|█▍        | 23/156 [00:00<00:00, 184.23it/s]100%|██████████| 156/156 [00:00<00:00, 1186.62it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 20846.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 15%|█▍        | 23/156 [00:00<00:00, 206.37it/s]100%|██████████| 156/156 [00:00<00:00, 1327.24it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 22084.23it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 5085.5 words/s - loss: 0.5327
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4911.9 words/s - loss: 0.5237
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4573.0 words/s - loss: 0.5326
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4558.2 words/s - loss: 0.5333
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4565.0 words/s - loss: 0.5369
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4517.3 words/s - loss: 0.5157
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4302.8 words/s - loss: 0.5393
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 3715.9 words/s - loss: 0.5190
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4719.5 words/s - loss: 0.3256
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4578.8 words/s - loss: 0.3098
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4787.9 words/s - loss: 0.3090
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4646.9 words/s - loss: 0.2996
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4161.0 words/s - loss: 0.3001
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4110.7 words/s - loss: 0.3118
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 4111.9 words/s - loss: 0.3102
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4091.4 words/s - loss: 0.3239
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4864.8 words/s - loss: 0.2726
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4768.8 words/s - loss: 0.2770
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4972.6 words/s - loss: 0.2804
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4323.1 words/s - loss: 0.2667
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4484.0 words/s - loss: 0.2680
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 4015.8 words/s - loss: 0.2694
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4183.4 words/s - loss: 0.2507
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4054.6 words/s - loss: 0.2640
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4279.8 words/s - loss: 0.2479
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4776.2 words/s - loss: 0.2547
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 5144.6 words/s - loss: 0.2736
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3880.8 words/s - loss: 0.2586
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4412.2 words/s - loss: 0.2196
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4521.0 words/s - loss: 0.2171
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4221.0 words/s - loss: 0.2483
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4022.8 words/s - loss: 0.2318
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 5427.2 words/s - loss: 0.2274
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 4351.7 words/s - loss: 0.2510
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4218.5 words/s - loss: 0.2441
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4109.7 words/s - loss: 0.2279
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4044.0 words/s - loss: 0.2454
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4278.8 words/s - loss: 0.2427
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4430.1 words/s - loss: 0.2161
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4881.2 words/s - loss: 0.2290
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 3933.9 words/s - loss: 0.2473
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4603.1 words/s - loss: 0.2269
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 4706.5 words/s - loss: 0.2429
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4414.6 words/s - loss: 0.2093
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 4156.3 words/s - loss: 0.2275
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4881.2 words/s - loss: 0.2098
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4225.6 words/s - loss: 0.2091
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 4698.1 words/s - loss: 0.2415
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.390 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_dees_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 5604.4 words/s - loss: 0.2106
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4791.8 words/s - loss: 0.1979
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4599.6 words/s - loss: 0.2222
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 4569.5 words/s - loss: 0.2218
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 4604.8 words/s - loss: 0.2092
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4426.4 words/s - loss: 0.2224
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4104.3 words/s - loss: 0.2225
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4082.3 words/s - loss: 0.2051
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.427 w/ 78 queries
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_dees_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 5120.1 words/s - loss: 0.2125
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4620.2 words/s - loss: 0.2196
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 4313.7 words/s - loss: 0.2219
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 4087.7 words/s - loss: 0.2260
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4157.3 words/s - loss: 0.1838
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 4204.0 words/s - loss: 0.1743
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4003.7 words/s - loss: 0.1893
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3885.4 words/s - loss: 0.2020
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.375 w/ 78 queries
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.436 w/ 78 queries
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_dees_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 5203.7 words/s - loss: 0.2154
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4835.7 words/s - loss: 0.2358
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4641.6 words/s - loss: 0.1874
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4500.1 words/s - loss: 0.1755
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4277.8 words/s - loss: 0.1869
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4160.7 words/s - loss: 0.1897
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 4229.1 words/s - loss: 0.1861
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3752.9 words/s - loss: 0.1772
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.359 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.431 w/ 78 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_dees_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4795.2 words/s - loss: 0.2043
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4665.8 words/s - loss: 0.1877
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4551.0 words/s - loss: 0.1604
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 4530.1 words/s - loss: 0.2281
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4423.4 words/s - loss: 0.2046
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4419.2 words/s - loss: 0.1659
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4415.5 words/s - loss: 0.1780
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4398.1 words/s - loss: 0.2189
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.350 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.422 w/ 78 queries
INFO:__main__:removing file tmp/mbert_dees_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_9.txt
INFO:__main__:[0.3895515824708976, 0.3749833155418146, 0.37493382123797986, 0.3588953194950399, 0.34970634667548883]
INFO:__main__:[0.4313798252369635, 0.4265112359450201, 0.43595118491333124, 0.4311669380194506, 0.4224568899394319]
INFO:__main__:0.37493382123797986
INFO:__main__:0.4313798252369635
INFO:__main__:best MAP: 0.403
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-4.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 873704.12it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 806565.90it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 844230.10it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 906053.75it/s]
100%|██████████| 5000/5000 [00:00<00:00, 884053.62it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 809898.82it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 687387.98it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 826561.56it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 207.27it/s] 17%|█▋        | 27/156 [00:00<00:00, 160.65it/s]100%|██████████| 156/156 [00:00<00:00, 1142.38it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 894.88it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19170.59it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 19793.43it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 209.83it/s] 17%|█▋        | 27/156 [00:00<00:00, 206.16it/s]100%|██████████| 156/156 [00:00<00:00, 1156.80it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1137.73it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 163.23it/s]100%|██████████| 156/156 [00:00<00:00, 19250.12it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 19504.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 907.26it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 19232.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 131.25it/s] 17%|█▋        | 27/156 [00:00<00:00, 129.99it/s]100%|██████████| 156/156 [00:00<00:00, 736.54it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 728.45it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19556.20it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 18992.52it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 220.00it/s]100%|██████████| 156/156 [00:00<00:00, 1210.89it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19724.81it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4836.6 words/s - loss: 0.3295
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4745.5 words/s - loss: 0.3326
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4515.1 words/s - loss: 0.3592
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4396.5 words/s - loss: 0.3840
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4231.2 words/s - loss: 0.3243
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4174.7 words/s - loss: 0.3582
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 4050.8 words/s - loss: 0.3400
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 3740.2 words/s - loss: 0.3437
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4496.7 words/s - loss: 0.2684
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 4510.2 words/s - loss: 0.2636
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4323.6 words/s - loss: 0.2444
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4583.7 words/s - loss: 0.2520
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4445.7 words/s - loss: 0.2510
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 4072.3 words/s - loss: 0.2488
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 4184.7 words/s - loss: 0.2362
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 4012.3 words/s - loss: 0.2589
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4553.9 words/s - loss: 0.2239
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4791.7 words/s - loss: 0.2408
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4444.8 words/s - loss: 0.2280
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4425.7 words/s - loss: 0.2108
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 3944.4 words/s - loss: 0.2436
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4171.3 words/s - loss: 0.2229
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4322.7 words/s - loss: 0.2286
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 5141.3 words/s - loss: 0.2322
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4906.8 words/s - loss: 0.2083
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4857.1 words/s - loss: 0.2496
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4972.3 words/s - loss: 0.1977
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4366.8 words/s - loss: 0.2184
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4689.5 words/s - loss: 0.2003
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4606.4 words/s - loss: 0.2143
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4276.6 words/s - loss: 0.2171
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 3382.1 words/s - loss: 0.2137
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 5008.4 words/s - loss: 0.1881
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4747.1 words/s - loss: 0.2066
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3853.6 words/s - loss: 0.2159
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 4389.6 words/s - loss: 0.1978
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4273.1 words/s - loss: 0.1899
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4319.1 words/s - loss: 0.1782
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4399.1 words/s - loss: 0.1892
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4072.9 words/s - loss: 0.1886
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4656.0 words/s - loss: 0.2279
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4110.4 words/s - loss: 0.2019
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 4955.0 words/s - loss: 0.1731
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4680.0 words/s - loss: 0.2091
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 3605.8 words/s - loss: 0.1857
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4420.5 words/s - loss: 0.2034
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 3978.6 words/s - loss: 0.1914
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 4371.8 words/s - loss: 0.1843
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.549 w/ 78 queries
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_dees_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 5048.7 words/s - loss: 0.1829
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4629.4 words/s - loss: 0.1811
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 4701.8 words/s - loss: 0.1858
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 4745.6 words/s - loss: 0.1737
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4200.7 words/s - loss: 0.1789
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4159.7 words/s - loss: 0.1799
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 4211.5 words/s - loss: 0.1906
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4027.7 words/s - loss: 0.1807
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.473 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.540 w/ 78 queries
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_dees_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 5206.6 words/s - loss: 0.1839
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 5081.0 words/s - loss: 0.1692
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4144.9 words/s - loss: 0.1816
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 4112.6 words/s - loss: 0.1865
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 4157.1 words/s - loss: 0.1725
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 3856.5 words/s - loss: 0.1756
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 3898.2 words/s - loss: 0.1610
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 3548.7 words/s - loss: 0.1863
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.482 w/ 78 queries
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.546 w/ 78 queries
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_dees_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 4821.9 words/s - loss: 0.1639
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 4817.9 words/s - loss: 0.1883
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 4500.3 words/s - loss: 0.1726
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 4447.9 words/s - loss: 0.1640
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 4199.2 words/s - loss: 0.1551
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4256.2 words/s - loss: 0.1583
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 3922.5 words/s - loss: 0.1565
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 3860.9 words/s - loss: 0.1855
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.479 w/ 78 queries
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_dees_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4905.0 words/s - loss: 0.1689
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4784.9 words/s - loss: 0.1944
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4696.2 words/s - loss: 0.1664
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4549.2 words/s - loss: 0.1977
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4358.9 words/s - loss: 0.1772
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4164.6 words/s - loss: 0.1396
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 3917.9 words/s - loss: 0.1806
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 3838.1 words/s - loss: 0.1440
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.474 w/ 78 queries
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.542 w/ 78 queries
INFO:__main__:removing file tmp/mbert_dees_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_9.txt
INFO:__main__:[0.47915353461921245, 0.47255836644373883, 0.4821136939630381, 0.47896871385016454, 0.47390056449336926]
INFO:__main__:[0.5494227746625387, 0.539514117797557, 0.5460241776805735, 0.5452091934916393, 0.5415861722574141]
INFO:__main__:0.47915353461921245
INFO:__main__:0.5460241776805735
INFO:__main__:best MAP: 0.513
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=7, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=3, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=2, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=6, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=5, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=1, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=0, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:__main__:Namespace(apex_level='O2', batch_size=8, cased=False, dataset='mix', debug=False, encoder_lr=2e-05, eval_step=1, full_doc_length=True, local_rank=4, model_path='/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9', model_type='mbert', num_epochs=10, num_ft_encoders=3, num_neg=1, projector_lr=2e-05, seed=611, source_lang='de', target_lang='es')
INFO:transformers.tokenization_utils_base:Model name '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:Didn't find file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer.json. We won't load it.
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/vocab.txt
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/special_tokens_map.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/tokenizer_config.json
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.tokenization_utils_base:loading file None
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.configuration_utils:loading configuration file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForXLRetrieval"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:loading weights file /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForXLRetrieval.

INFO:transformers.modeling_utils:All the weights of BertForXLRetrieval were initialized from the model checkpoint at /mnt/scratch/puxuan/short_qlm-rr/2147416/huggingface-9.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForXLRetrieval for predictions without further training.
INFO:__main__:Evaluating every 1 epochs ...
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 833095.78it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 857275.07it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 796155.04it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 888209.73it/s]
100%|██████████| 5000/5000 [00:00<00:00, 882639.73it/s]
100%|██████████| 5000/5000 [00:00<00:00, 882602.58it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:root:Number of positive query-document pairs in [train] set: 5000
100%|██████████| 5000/5000 [00:00<00:00, 816361.87it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 793624.22it/s]
INFO:root:Number of positive query-document pairs in [train] set: 5000
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 237.66it/s]100%|██████████| 156/156 [00:00<00:00, 1301.29it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19481.09it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 206.76it/s] 17%|█▋        | 27/156 [00:00<00:00, 203.76it/s] 17%|█▋        | 27/156 [00:00<00:00, 203.53it/s] 17%|█▋        | 27/156 [00:00<00:00, 199.90it/s]100%|██████████| 156/156 [00:00<00:00, 1139.31it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]INFO:__main__:reading data from /mnt/home/puxuan/wiki-clir/uncased and /mnt/home/puxuan/CLIR-project/Evaluation_data/process-clef/uncased
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1122.90it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 1121.44it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 1101.01it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s] 17%|█▋        | 27/156 [00:00<00:00, 159.25it/s]100%|██████████| 156/156 [00:00<00:00, 18950.72it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
100%|██████████| 156/156 [00:00<00:00, 19410.59it/s]
INFO:__main__:Data reading done ...
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
100%|██████████| 156/156 [00:00<00:00, 887.90it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
100%|██████████| 156/156 [00:00<00:00, 18974.90it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19051.70it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
100%|██████████| 156/156 [00:00<00:00, 19601.31it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 141.96it/s]100%|██████████| 156/156 [00:00<00:00, 792.49it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19159.36it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
 17%|█▋        | 27/156 [00:00<00:00, 224.73it/s]100%|██████████| 156/156 [00:00<00:00, 1228.07it/s]
INFO:root:Number of labelled query-document pairs in [f1] set: 28224
  0%|          | 0/156 [00:00<?, ?it/s]100%|██████████| 156/156 [00:00<00:00, 19691.57it/s]
INFO:root:Number of labelled query-document pairs in [f2] set: 29534
INFO:__main__:f1 has 78 queries ...
INFO:__main__:f2 has 78 queries ...
INFO:__main__:Data reading done ...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:adding 9-th encoder to optimizer...
INFO:__main__:adding 10-th encoder to optimizer...
INFO:__main__:adding 11-th encoder to optimizer...
INFO:__main__:process[0]: training epoch 0 ...
INFO:__main__:process[5]: training epoch 0 ...
INFO:__main__:process[6]: training epoch 0 ...
INFO:__main__:process[2]: training epoch 0 ...
INFO:__main__:process[1]: training epoch 0 ...
INFO:__main__:process[3]: training epoch 0 ...
INFO:__main__:process[4]: training epoch 0 ...
INFO:__main__:process[7]: training epoch 0 ...
INFO:__main__:process[3] - epoch 0 - train iter 625 - 4986.4 words/s - loss: 0.3408
INFO:__main__:process[3]: training epoch 1 ...
INFO:__main__:process[4] - epoch 0 - train iter 625 - 4720.2 words/s - loss: 0.3618
INFO:__main__:process[4]: training epoch 1 ...
INFO:__main__:process[0] - epoch 0 - train iter 625 - 4697.8 words/s - loss: 0.3347
INFO:__main__:process[0]: training epoch 1 ...
INFO:__main__:process[5] - epoch 0 - train iter 625 - 4343.9 words/s - loss: 0.3549
INFO:__main__:process[5]: training epoch 1 ...
INFO:__main__:process[1] - epoch 0 - train iter 625 - 4401.4 words/s - loss: 0.3531
INFO:__main__:process[1]: training epoch 1 ...
INFO:__main__:process[2] - epoch 0 - train iter 625 - 4363.5 words/s - loss: 0.3425
INFO:__main__:process[2]: training epoch 1 ...
INFO:__main__:process[6] - epoch 0 - train iter 625 - 4389.3 words/s - loss: 0.3506
INFO:__main__:process[6]: training epoch 1 ...
INFO:__main__:process[7] - epoch 0 - train iter 625 - 4056.9 words/s - loss: 0.3527
INFO:__main__:process[7]: training epoch 1 ...
INFO:__main__:process[3] - epoch 1 - train iter 625 - 4496.4 words/s - loss: 0.2552
INFO:__main__:process[3]: training epoch 2 ...
INFO:__main__:process[0] - epoch 1 - train iter 625 - 5030.7 words/s - loss: 0.2603
INFO:__main__:process[0]: training epoch 2 ...
INFO:__main__:process[5] - epoch 1 - train iter 625 - 4544.3 words/s - loss: 0.2394
INFO:__main__:process[5]: training epoch 2 ...
INFO:__main__:process[4] - epoch 1 - train iter 625 - 4141.6 words/s - loss: 0.2545
INFO:__main__:process[4]: training epoch 2 ...
INFO:__main__:process[7] - epoch 1 - train iter 625 - 4350.8 words/s - loss: 0.2551
INFO:__main__:process[7]: training epoch 2 ...
INFO:__main__:process[2] - epoch 1 - train iter 625 - 3944.5 words/s - loss: 0.2734
INFO:__main__:process[2]: training epoch 2 ...
INFO:__main__:process[1] - epoch 1 - train iter 625 - 3922.2 words/s - loss: 0.2244
INFO:__main__:process[1]: training epoch 2 ...
INFO:__main__:process[6] - epoch 1 - train iter 625 - 3762.2 words/s - loss: 0.2465
INFO:__main__:process[6]: training epoch 2 ...
INFO:__main__:process[4] - epoch 2 - train iter 625 - 4940.0 words/s - loss: 0.2134
INFO:__main__:process[4]: training epoch 3 ...
INFO:__main__:process[3] - epoch 2 - train iter 625 - 4180.3 words/s - loss: 0.2265
INFO:__main__:process[3]: training epoch 3 ...
INFO:__main__:process[0] - epoch 2 - train iter 625 - 4031.1 words/s - loss: 0.2197
INFO:__main__:process[0]: training epoch 3 ...
INFO:__main__:process[5] - epoch 2 - train iter 625 - 4330.3 words/s - loss: 0.2325
INFO:__main__:process[5]: training epoch 3 ...
INFO:__main__:process[2] - epoch 2 - train iter 625 - 4771.4 words/s - loss: 0.2125
INFO:__main__:process[2]: training epoch 3 ...
INFO:__main__:process[7] - epoch 2 - train iter 625 - 4385.4 words/s - loss: 0.1993
INFO:__main__:process[7]: training epoch 3 ...
INFO:__main__:process[1] - epoch 2 - train iter 625 - 4319.0 words/s - loss: 0.2316
INFO:__main__:process[1]: training epoch 3 ...
INFO:__main__:process[6] - epoch 2 - train iter 625 - 4187.9 words/s - loss: 0.2175
INFO:__main__:process[6]: training epoch 3 ...
INFO:__main__:process[4] - epoch 3 - train iter 625 - 4461.7 words/s - loss: 0.2061
INFO:__main__:process[4]: training epoch 4 ...
INFO:__main__:process[3] - epoch 3 - train iter 625 - 4411.8 words/s - loss: 0.2215
INFO:__main__:process[3]: training epoch 4 ...
INFO:__main__:process[0] - epoch 3 - train iter 625 - 4629.0 words/s - loss: 0.2275
INFO:__main__:process[0]: training epoch 4 ...
INFO:__main__:process[5] - epoch 3 - train iter 625 - 4525.5 words/s - loss: 0.1871
INFO:__main__:process[5]: training epoch 4 ...
INFO:__main__:process[7] - epoch 3 - train iter 625 - 4706.1 words/s - loss: 0.1909
INFO:__main__:process[7]: training epoch 4 ...
INFO:__main__:process[2] - epoch 3 - train iter 625 - 4221.7 words/s - loss: 0.2008
INFO:__main__:process[2]: training epoch 4 ...
INFO:__main__:process[1] - epoch 3 - train iter 625 - 4088.0 words/s - loss: 0.2276
INFO:__main__:process[1]: training epoch 4 ...
INFO:__main__:process[6] - epoch 3 - train iter 625 - 4284.5 words/s - loss: 0.2084
INFO:__main__:process[6]: training epoch 4 ...
INFO:__main__:process[4] - epoch 4 - train iter 625 - 4791.0 words/s - loss: 0.1776
INFO:__main__:process[4]: training epoch 5 ...
INFO:__main__:process[0] - epoch 4 - train iter 625 - 4627.2 words/s - loss: 0.1738
INFO:__main__:process[0]: training epoch 5 ...
INFO:__main__:process[3] - epoch 4 - train iter 625 - 3981.8 words/s - loss: 0.2195
INFO:__main__:process[3]: training epoch 5 ...
INFO:__main__:process[5] - epoch 4 - train iter 625 - 4129.8 words/s - loss: 0.2258
INFO:__main__:process[5]: training epoch 5 ...
INFO:__main__:process[7] - epoch 4 - train iter 625 - 4345.4 words/s - loss: 0.1700
INFO:__main__:process[7]: training epoch 5 ...
INFO:__main__:process[2] - epoch 4 - train iter 625 - 4291.0 words/s - loss: 0.1948
INFO:__main__:process[2]: training epoch 5 ...
INFO:__main__:process[1] - epoch 4 - train iter 625 - 4269.3 words/s - loss: 0.1901
INFO:__main__:process[1]: training epoch 5 ...
INFO:__main__:process[6] - epoch 4 - train iter 625 - 4144.0 words/s - loss: 0.1959
INFO:__main__:process[6]: training epoch 5 ...
INFO:__main__:process[4] - epoch 5 - train iter 625 - 4603.4 words/s - loss: 0.1834
INFO:__main__:process[4]: evaluating epoch 5 on f1 ...
INFO:__main__:process[0] - epoch 5 - train iter 625 - 4959.2 words/s - loss: 0.1718
INFO:__main__:process[0]: evaluating epoch 5 on f1 ...
INFO:__main__:process[7] - epoch 5 - train iter 625 - 5208.8 words/s - loss: 0.1789
INFO:__main__:process[7]: evaluating epoch 5 on f1 ...
INFO:__main__:process[5] - epoch 5 - train iter 625 - 4511.2 words/s - loss: 0.1956
INFO:__main__:process[5]: evaluating epoch 5 on f1 ...
INFO:__main__:process[2] - epoch 5 - train iter 625 - 4968.2 words/s - loss: 0.1955
INFO:__main__:process[2]: evaluating epoch 5 on f1 ...
INFO:__main__:process[3] - epoch 5 - train iter 625 - 4087.0 words/s - loss: 0.1923
INFO:__main__:process[3]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - epoch 5 - train iter 625 - 5215.7 words/s - loss: 0.1888
INFO:__main__:process[6]: evaluating epoch 5 on f1 ...
INFO:__main__:process[1] - epoch 5 - train iter 625 - 4700.5 words/s - loss: 0.1968
INFO:__main__:process[1]: evaluating epoch 5 on f1 ...
INFO:__main__:process[6] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 5 - mAP: 0.475 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 5 on f2 ...
INFO:__main__:process[2]: evaluating epoch 5 on f2 ...
INFO:__main__:process[3]: evaluating epoch 5 on f2 ...
INFO:__main__:process[5]: evaluating epoch 5 on f2 ...
INFO:__main__:process[1]: evaluating epoch 5 on f2 ...
INFO:__main__:process[7]: evaluating epoch 5 on f2 ...
INFO:__main__:process[4]: evaluating epoch 5 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_7_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_5.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_5.txt
INFO:__main__:process[0]: evaluating epoch 5 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[5] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 5 - mAP: 0.537 w/ 78 queries
INFO:__main__:process[2]: training epoch 6 ...
INFO:__main__:process[6]: training epoch 6 ...
INFO:__main__:process[3]: training epoch 6 ...
INFO:__main__:process[5]: training epoch 6 ...
INFO:__main__:process[4]: training epoch 6 ...
INFO:__main__:process[1]: training epoch 6 ...
INFO:__main__:process[7]: training epoch 6 ...
INFO:__main__:removing file tmp/mbert_dees_f2_6_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_5.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_5.txt
INFO:__main__:process[0]: training epoch 6 ...
INFO:__main__:process[5] - epoch 6 - train iter 625 - 4978.1 words/s - loss: 0.1749
INFO:__main__:process[5]: evaluating epoch 6 on f1 ...
INFO:__main__:process[3] - epoch 6 - train iter 625 - 4726.9 words/s - loss: 0.1820
INFO:__main__:process[3]: evaluating epoch 6 on f1 ...
INFO:__main__:process[7] - epoch 6 - train iter 625 - 4339.5 words/s - loss: 0.1881
INFO:__main__:process[7]: evaluating epoch 6 on f1 ...
INFO:__main__:process[2] - epoch 6 - train iter 625 - 4400.6 words/s - loss: 0.1856
INFO:__main__:process[2]: evaluating epoch 6 on f1 ...
INFO:__main__:process[0] - epoch 6 - train iter 625 - 4478.0 words/s - loss: 0.1830
INFO:__main__:process[0]: evaluating epoch 6 on f1 ...
INFO:__main__:process[4] - epoch 6 - train iter 625 - 4349.1 words/s - loss: 0.1600
INFO:__main__:process[4]: evaluating epoch 6 on f1 ...
INFO:__main__:process[1] - epoch 6 - train iter 625 - 3612.2 words/s - loss: 0.1879
INFO:__main__:process[1]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - epoch 6 - train iter 625 - 3632.2 words/s - loss: 0.1895
INFO:__main__:process[6]: evaluating epoch 6 on f1 ...
INFO:__main__:process[6] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 6 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 6 on f2 ...
INFO:__main__:process[2]: evaluating epoch 6 on f2 ...
INFO:__main__:process[5]: evaluating epoch 6 on f2 ...
INFO:__main__:process[1]: evaluating epoch 6 on f2 ...
INFO:__main__:process[3]: evaluating epoch 6 on f2 ...
INFO:__main__:process[4]: evaluating epoch 6 on f2 ...
INFO:__main__:process[7]: evaluating epoch 6 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_3_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_6.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_6.txt
INFO:__main__:process[0]: evaluating epoch 6 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 6 - mAP: 0.552 w/ 78 queries
INFO:__main__:process[5]: training epoch 7 ...
INFO:__main__:process[6]: training epoch 7 ...
INFO:__main__:process[2]: training epoch 7 ...
INFO:__main__:process[3]: training epoch 7 ...
INFO:__main__:process[4]: training epoch 7 ...
INFO:__main__:process[7]: training epoch 7 ...
INFO:__main__:process[1]: training epoch 7 ...
INFO:__main__:removing file tmp/mbert_dees_f2_5_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_6.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_6.txt
INFO:__main__:process[0]: training epoch 7 ...
INFO:__main__:process[7] - epoch 7 - train iter 625 - 5494.7 words/s - loss: 0.1541
INFO:__main__:process[7]: evaluating epoch 7 on f1 ...
INFO:__main__:process[3] - epoch 7 - train iter 625 - 4947.0 words/s - loss: 0.1871
INFO:__main__:process[3]: evaluating epoch 7 on f1 ...
INFO:__main__:process[5] - epoch 7 - train iter 625 - 4585.2 words/s - loss: 0.1825
INFO:__main__:process[5]: evaluating epoch 7 on f1 ...
INFO:__main__:process[2] - epoch 7 - train iter 625 - 4539.7 words/s - loss: 0.1973
INFO:__main__:process[2]: evaluating epoch 7 on f1 ...
INFO:__main__:process[4] - epoch 7 - train iter 625 - 4589.6 words/s - loss: 0.1637
INFO:__main__:process[4]: evaluating epoch 7 on f1 ...
INFO:__main__:process[1] - epoch 7 - train iter 625 - 4341.8 words/s - loss: 0.1970
INFO:__main__:process[1]: evaluating epoch 7 on f1 ...
INFO:__main__:process[0] - epoch 7 - train iter 625 - 4096.1 words/s - loss: 0.1743
INFO:__main__:process[0]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - epoch 7 - train iter 625 - 4238.1 words/s - loss: 0.1639
INFO:__main__:process[6]: evaluating epoch 7 on f1 ...
INFO:__main__:process[6] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[0] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 7 - mAP: 0.471 w/ 78 queries
INFO:__main__:process[6]: evaluating epoch 7 on f2 ...
INFO:__main__:process[5]: evaluating epoch 7 on f2 ...
INFO:__main__:process[2]: evaluating epoch 7 on f2 ...
INFO:__main__:process[7]: evaluating epoch 7 on f2 ...
INFO:__main__:process[4]: evaluating epoch 7 on f2 ...
INFO:__main__:process[3]: evaluating epoch 7 on f2 ...
INFO:__main__:process[1]: evaluating epoch 7 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_3_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_7.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_7.txt
INFO:__main__:process[0]: evaluating epoch 7 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 7 - mAP: 0.545 w/ 78 queries
INFO:__main__:process[6]: training epoch 8 ...
INFO:__main__:process[2]: training epoch 8 ...
INFO:__main__:process[5]: training epoch 8 ...
INFO:__main__:process[3]: training epoch 8 ...
INFO:__main__:process[7]: training epoch 8 ...
INFO:__main__:process[4]: training epoch 8 ...
INFO:__main__:process[1]: training epoch 8 ...
INFO:__main__:removing file tmp/mbert_dees_f2_7_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_7.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_7.txt
INFO:__main__:process[0]: training epoch 8 ...
INFO:__main__:process[1] - epoch 8 - train iter 625 - 4427.0 words/s - loss: 0.1803
INFO:__main__:process[1]: evaluating epoch 8 on f1 ...
INFO:__main__:process[6] - epoch 8 - train iter 625 - 4360.5 words/s - loss: 0.1696
INFO:__main__:process[6]: evaluating epoch 8 on f1 ...
INFO:__main__:process[5] - epoch 8 - train iter 625 - 4219.9 words/s - loss: 0.1602
INFO:__main__:process[5]: evaluating epoch 8 on f1 ...
INFO:__main__:process[3] - epoch 8 - train iter 625 - 3948.1 words/s - loss: 0.1631
INFO:__main__:process[3]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - epoch 8 - train iter 625 - 3906.3 words/s - loss: 0.1607
INFO:__main__:process[0]: evaluating epoch 8 on f1 ...
INFO:__main__:process[4] - epoch 8 - train iter 625 - 4030.0 words/s - loss: 0.1657
INFO:__main__:process[4]: evaluating epoch 8 on f1 ...
INFO:__main__:process[2] - epoch 8 - train iter 625 - 3930.5 words/s - loss: 0.1547
INFO:__main__:process[2]: evaluating epoch 8 on f1 ...
INFO:__main__:process[7] - epoch 8 - train iter 625 - 3692.0 words/s - loss: 0.1556
INFO:__main__:process[7]: evaluating epoch 8 on f1 ...
INFO:__main__:process[0] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 8 - mAP: 0.447 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 8 on f2 ...
INFO:__main__:process[6]: evaluating epoch 8 on f2 ...
INFO:__main__:process[4]: evaluating epoch 8 on f2 ...
INFO:__main__:process[2]: evaluating epoch 8 on f2 ...
INFO:__main__:process[7]: evaluating epoch 8 on f2 ...
INFO:__main__:process[3]: evaluating epoch 8 on f2 ...
INFO:__main__:process[1]: evaluating epoch 8 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_0_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_1_8.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_8.txt
INFO:__main__:process[0]: evaluating epoch 8 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[2] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[0] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 8 - mAP: 0.535 w/ 78 queries
INFO:__main__:process[6]: training epoch 9 ...
INFO:__main__:process[2]: training epoch 9 ...
INFO:__main__:process[5]: training epoch 9 ...
INFO:__main__:process[7]: training epoch 9 ...
INFO:__main__:process[4]: training epoch 9 ...
INFO:__main__:process[3]: training epoch 9 ...
INFO:__main__:process[1]: training epoch 9 ...
INFO:__main__:removing file tmp/mbert_dees_f2_7_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_2_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_8.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_8.txt
INFO:__main__:process[0]: training epoch 9 ...
INFO:__main__:process[0] - epoch 9 - train iter 625 - 5036.0 words/s - loss: 0.1773
INFO:__main__:process[0]: evaluating epoch 9 on f1 ...
INFO:__main__:process[2] - epoch 9 - train iter 625 - 4756.1 words/s - loss: 0.1773
INFO:__main__:process[2]: evaluating epoch 9 on f1 ...
INFO:__main__:process[7] - epoch 9 - train iter 625 - 4624.0 words/s - loss: 0.1580
INFO:__main__:process[7]: evaluating epoch 9 on f1 ...
INFO:__main__:process[6] - epoch 9 - train iter 625 - 4599.5 words/s - loss: 0.1446
INFO:__main__:process[6]: evaluating epoch 9 on f1 ...
INFO:__main__:process[4] - epoch 9 - train iter 625 - 4352.8 words/s - loss: 0.1410
INFO:__main__:process[4]: evaluating epoch 9 on f1 ...
INFO:__main__:process[1] - epoch 9 - train iter 625 - 4486.2 words/s - loss: 0.1506
INFO:__main__:process[1]: evaluating epoch 9 on f1 ...
INFO:__main__:process[3] - epoch 9 - train iter 625 - 4316.6 words/s - loss: 0.1775
INFO:__main__:process[3]: evaluating epoch 9 on f1 ...
INFO:__main__:process[5] - epoch 9 - train iter 625 - 4127.0 words/s - loss: 0.1706
INFO:__main__:process[5]: evaluating epoch 9 on f1 ...
INFO:__main__:process[0] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[5] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[2] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[6] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[3] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[4] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[1] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[7] - f1 - epoch 9 - mAP: 0.448 w/ 78 queries
INFO:__main__:process[5]: evaluating epoch 9 on f2 ...
INFO:__main__:process[3]: evaluating epoch 9 on f2 ...
INFO:__main__:process[6]: evaluating epoch 9 on f2 ...
INFO:__main__:process[4]: evaluating epoch 9 on f2 ...
INFO:__main__:process[7]: evaluating epoch 9 on f2 ...
INFO:__main__:process[2]: evaluating epoch 9 on f2 ...
INFO:__main__:process[1]: evaluating epoch 9 on f2 ...
INFO:__main__:removing file tmp/mbert_dees_f1_1_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_2_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_3_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_0_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_4_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_6_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_7_9.txt
INFO:__main__:removing file tmp/mbert_dees_f1_5_9.txt
INFO:__main__:process[0]: evaluating epoch 9 on f2 ...
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:f2 set during evaluation: 29536/29534
INFO:__main__:process[0] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:process[5] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:process[6] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:process[2] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:process[3] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:process[7] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:process[4] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:process[1] - f2 - epoch 9 - mAP: 0.529 w/ 78 queries
INFO:__main__:removing file tmp/mbert_dees_f2_2_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_1_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_6_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_7_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_3_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_4_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_0_9.txt
INFO:__main__:removing file tmp/mbert_dees_f2_5_9.txt
INFO:__main__:[0.47482243465790536, 0.4709974921873726, 0.4710281094300791, 0.44737595718653966, 0.4475448452841436]
INFO:__main__:[0.5371570631502819, 0.5516177162483026, 0.5449459343992957, 0.5347691472831818, 0.5291910141946138]
INFO:__main__:0.4709974921873726
INFO:__main__:0.5371570631502819
INFO:__main__:best MAP: 0.504
